[{"authors":null,"categories":null,"content":"Cong Peng is a PhD student in the Department of Computer Science at Blekinge Institute of Technology (BTH). He has master’s degrees in Computer Science from BTH, and Software Engineering from Zhejiang University of Technology in China. His research interests are Semantic Web, Machine Learning, eHealth and Web services. He has been involved in the works of teaching assistant, bachelor thesis supervision and review.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pcx.linkedinfo.co/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Cong Peng is a PhD student in the Department of Computer Science at Blekinge Institute of Technology (BTH). He has master’s degrees in Computer Science from BTH, and Software Engineering from Zhejiang University of Technology in China. His research interests are Semantic Web, Machine Learning, eHealth and Web services. He has been involved in the works of teaching assistant, bachelor thesis supervision and review.","tags":null,"title":"PENG, Cong","type":"authors"},{"authors":[],"categories":[],"content":"  This code snippet is to predict topic tags based on the text of an article. Each article could have 1 or more tags (usually have at least 1 tag), and the tags are not mutually exclusive. So this is a multi-label classification problem. It\u0026#39;s different from multi-class classification, the classes in multi-class classification are mutually exclusive, i.e., each item belongs to 1 and only 1 class.  In this snippet, we will use OneVsRestClassifier (the One-Vs-the-Rest) in scikit-learn to process the multi-label classification. The article data will be retrieved from LinkedInfo.co via Web API. The methods in this snippet should give credits to Working With Text Data - scikit-learn and this post. Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB Preprocessing data and explore the method   dataset.df_tags fetches the data set from LinkedInfo.co. It calls Web API of LinkedInfo.co to retrieve the article list, and then download and extract the full text of each article based on an article\u0026#39;s url. The tags of each article are encoded using MultiLabelBinarizer in scikit-learn. The implementation of the code could be found in dataset.py. We\u0026#39;ve set the parameter of content_length_threshold to 100 to screen out the articles with less than 100 for the description or full text. import dataset ds = dataset.df_tags(content_length_threshold=100)   The dataset contains 3353 articles by the time retrieved the data. The dataset re returned as an object with the following attribute:     ds.data: pandas.DataFrame with cols of title, description, fulltext    ds.target: encoding of tagsID    ds.target_names: tagsID    ds.target_decoded: the list of lists contains tagsID for each info    \u0026gt;\u0026gt; ds.data.head()      description fulltext title   0 Both HTTP 1.x and HTTP/2 rely on lower level c… [Stressgrid]()\\n\\n__\\n\\n[]( \u0026#34;home\u0026#34;)\\n\\n * [… Achieving 100k connections per second with Elixir   1 At Phusion we run a simple multithreaded HTTP … [![Hongli Lai](images/avatar-b64f1ad5.png)](… What causes Ruby memory bloat?   2 Have you ever wanted to contribute to a projec… [ ![Real Python](/static/real-python-logo.ab1a… Managing Multiple Python Versions With pyenv   3 安卓在版本Pie中第一次引入了ART优化配置文件，这个新特性利用发送到Play Cloud的… 安卓在版本Pie中第一次引入了[ART优化配置文件](https://youtu.be/Yi... ART云配置文件，提高安卓应用的性能   4 I work at Red Hat on GCC, the GNU Compiler Col… [ ![Red Hat\\nLogo](https://developers.redhat.c... Usability improvements in GCC 9    \u0026gt;\u0026gt; ds.target[:5] array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]])  \u0026gt;\u0026gt; ds.target_names[:5] array([\u0026#39;academia\u0026#39;, \u0026#39;access-control\u0026#39;, \u0026#39;activemq\u0026#39;, \u0026#39;aes\u0026#39;, \u0026#39;agile\u0026#39;], dtype=object)  \u0026gt;\u0026gt; ds.target_decoded[:5] [[\u0026#39;concurrency\u0026#39;, \u0026#39;elixir\u0026#39;], [\u0026#39;ruby\u0026#39;], [\u0026#39;python\u0026#39;, \u0026#39;virtualenv\u0026#39;], [\u0026#39;android\u0026#39;], [\u0026#39;gcc\u0026#39;]]   The following snippet is the actual process of getting the above dataset, by reading from file. import json import pandas as pd from sklearn.preprocessing import MultiLabelBinarizer infos_file = \u0026#39;data/infos/infos_0_3353_fulltext.json\u0026#39; with open(infos_file, \u0026#39;r\u0026#39;) as f: infos = json.load(f) content_length_threshold = 100 data_lst = [] tags_lst = [] for info in infos[\u0026#39;content\u0026#39;]: if len(info[\u0026#39;fulltext\u0026#39;]) \u0026lt; content_length_threshold: continue if len(info[\u0026#39;description\u0026#39;]) \u0026lt; content_length_threshold: continue data_lst.append({\u0026#39;title\u0026#39;: info[\u0026#39;title\u0026#39;], \u0026#39;description\u0026#39;: info[\u0026#39;description\u0026#39;], \u0026#39;fulltext\u0026#39;: info[\u0026#39;fulltext\u0026#39;]}) tags_lst.append([tag[\u0026#39;tagID\u0026#39;] for tag in info[\u0026#39;tags\u0026#39;]]) df_data = pd.DataFrame(data_lst) df_tags = pd.DataFrame(tags_lst) # fit and transform the binarizer mlb = MultiLabelBinarizer() Y = mlb.fit_transform(tags_lst) Y.shape  (3221, 560)   Now we\u0026#39;ve transformed the target (tags) but we cannot directly perform the algorithms on the text data, so we have to process and transform them into vectors. In order to do this, we will use TfidfVectorizer to preprocess, tokenize, filter stop words and transform the text data. The TfidfVectorizer implements the tf-idf (Term Frequency-Inverse Deocument Frequency) to reflect how important a word is to to a docuemnt in a collection of documents. from sklearn.feature_extraction.text import TfidfVectorizer # Use the default parameters for now, use_idf=True in default vectorizer = TfidfVectorizer() # Use the short descriptions for now for faster processing X = vectorizer.fit_transform(df_data.description) X.shape  (3221, 35506)   As mentioned in the beginning, this is a multi-label classification problem, we will use OneVsRestClassifier to tackle our problem. And firstly we will use the SVM (Support Vector Machines) with linear kernel, implemented as LinearSVC in scikit-learn, to do the classification. from sklearn.multiclass import OneVsRestClassifier from sklearn.svm import LinearSVC from sklearn.model_selection import train_test_split # Use default parameters, and train and test with small set of samples. clf = OneVsRestClassifier(LinearSVC()) from sklearn.utils import resample X_sample, Y_sample = resample( X, Y, n_samples=1000, replace=False, random_state=7) # X_sample_test, Y_sample_test = resample( # X, Y, n_samples=10, replace=False, random_state=1) X_sample_train, X_sample_test, Y_sample_train, Y_sample_test = train_test_split( X_sample, Y_sample, test_size=0.01, random_state=42) clf.fit(X_sample, Y_sample) Y_sample_pred = clf.predict(X_sample_test) # Inverse transform the vectors back to tags pred_transformed = mlb.inverse_transform(Y_sample_pred) test_transformed = mlb.inverse_transform(Y_sample_test) for (t, p) in zip(test_transformed, pred_transformed): print(f\u0026#39;tags: {t} predicted as: {p}\u0026#39;)  tags: ('javascript',) predicted as: ('javascript',) tags: ('erasure-code', 'storage') predicted as: () tags: ('mysql', 'network') predicted as: () tags: ('token',) predicted as: () tags: ('flask', 'python', 'web') predicted as: () tags: ('refactoring',) predicted as: () tags: ('emacs',) predicted as: () tags: ('async', 'javascript', 'promises') predicted as: ('async', 'javascript') tags: ('neural-networks',) predicted as: () tags: ('kubernetes',) predicted as: ('kubernetes',)   Though not very satisfied, this classifier predicted right a few tags. Next we\u0026#39;ll try to search for the best parameters for the classifier and train with fulltext of articles. Search for best model parameters for SVM with linear kernel   For the estimators TfidfVectorizer and LinearSVC, they both have many parameters could be tuned for better performance. We\u0026#39;ll the GridSearchCV to search for the best parameters with the help of Pipeline. from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split, GridSearchCV # Split the dataset into training and test set, and use fulltext of articles: X_train, X_test, Y_train, Y_test = train_test_split( df_data.fulltext, Y, test_size=0.5, random_state=42) # Build vectorizer classifier pipeline clf = Pipeline([ (\u0026#39;vect\u0026#39;, TfidfVectorizer()), (\u0026#39;clf\u0026#39;, OneVsRestClassifier(LinearSVC())), ]) # Grid search parameters, I minimized the parameter set based on previous # experience to accelerate the processing speed. # And the combination of penalty=\u0026#39;l1\u0026#39; and loss=\u0026#39;squared_hinge\u0026#39; are not supported when dual=True parameters = { \u0026#39;vect__ngram_range\u0026#39;: [(1, 2), (1, 3), (1, 4)], \u0026#39;vect__max_df\u0026#39;: [1, 0.9, 0.8, 0.7], \u0026#39;vect__min_df\u0026#39;: [1, 0.9, 0.8, 0.7, 0], \u0026#39;vect__use_idf\u0026#39;: [True, False], \u0026#39;clf__estimator__penalty\u0026#39;: [\u0026#39;l1\u0026#39;, \u0026#39;l2\u0026#39;], \u0026#39;clf__estimator__C\u0026#39;: [1, 10, 100, 1000], \u0026#39;clf__estimator__dual\u0026#39;: [False], } gs_clf = GridSearchCV(clf, parameters, cv=5, n_jobs=-1) gs_clf.fit(X_train, Y_train)  import datetime from sklearn import metrics # Predict the outcome on the testing set in a variable named y_predicted Y_predicted = gs_clf.predict(X_test) print(metrics.classification_report(Y_test, Y_predicted)) print(gs_clf.best_params_) print(gs_clf.best_score_) # Export some of the result cols cols = [ \u0026#39;mean_test_score\u0026#39;, \u0026#39;mean_fit_time\u0026#39;, \u0026#39;param_vect__ngram_range\u0026#39;, ] df_result = pd.DataFrame(gs_clf.cv_results_) df_result = df_result.sort_values(by=\u0026#39;rank_test_score\u0026#39;) df_result = df_result[cols] timestamp = datetime.now().strftime(\u0026#39;%Y-%m-%d_%H-%M-%S\u0026#39;) df_result.to_html( f\u0026#39;data/results/gridcv_results_{timestamp}_linearSVC.html\u0026#39;)   Here we attach the top-5 performed classifiers with selected parameters.   rank_test_score mean_test_score mean_fit_time param_vect__max_df param_vect__ngram_range param_vect__use_idf param_clf__estimator__penalty param_clf__estimator__C     64 1 0.140811 96.127405 0.8 (1, 4) True l1 10   70 2 0.140215 103.252332 0.7 (1, 4) True l1 10   58 2 0.140215 98.990952 0.9 (1, 4) True l1 10   154 2 0.140215 1690.433151 0.9 (1, 4) True l1 1000   68 5 0.139618 70.778621 0.7 (1, 3) True l1 10    Training and testing with the best parameters   Based on the grid search results, we found the following parameters combined with the default parameters have the best performance. Now let\u0026#39;s see how it will perform. X_train, X_test, Y_train, Y_test = train_test_split( df_data, Y, test_size=0.2, random_state=42) clf = Pipeline([ (\u0026#39;vect\u0026#39;, TfidfVectorizer(use_idf=True, max_df=0.8, ngram_range=[1, 4])), (\u0026#39;clf\u0026#39;, OneVsRestClassifier(LinearSVC(penalty=\u0026#39;l1\u0026#39;, C=10, dual=False))), ]) clf.fit(X_train.fulltext, Y_train) Y_pred = clf.predict(X_test.fulltext) # Inverse transform the vectors back to tags pred_transformed = mlb.inverse_transform(Y_pred) test_transformed = mlb.inverse_transform(Y_test) for (title, t, p) in zip(X_test.title, test_transformed, pred_transformed): print(f\u0026#39;Article title: {title} \\n\u0026#39; f\u0026#39;Manual tags: {t} \\n\u0026#39; f\u0026#39;predicted as: {p}\\n\u0026#39;)   Here below is a fraction of the list that shows the manually input tags and the predicted tags. We can see that usually the more frequently appeared and more popular tags have better change to be correctly predicted. Personally, I would say the prediction is satisfied to me comparing when I tag the articles manually. However, there\u0026#39;s much room for improvement. Article title: Will PWAs Replace Native Mobile Apps? Manual tags: (\u0026#39;pwa\u0026#39;,) predicted as: (\u0026#39;pwa\u0026#39;,) Article title: 基于Consul的分布式信号量实现 Manual tags: (\u0026#39;consul\u0026#39;, \u0026#39;distributed-system\u0026#39;) predicted as: (\u0026#39;microservices\u0026#39;, \u0026#39;multithreading\u0026#39;) Article title: commit 和 branch 理解深入 Manual tags: (\u0026#39;git\u0026#39;,) predicted as: (\u0026#39;git\u0026#39;,) Article title: Existential types in Scala Manual tags: (\u0026#39;scala\u0026#39;,) predicted as: (\u0026#39;scala\u0026#39;,) Article title: Calling back into Python from llvmlite-JITed code Manual tags: (\u0026#39;jit\u0026#39;, \u0026#39;python\u0026#39;) predicted as: (\u0026#39;compiler\u0026#39;, \u0026#39;python\u0026#39;) Article title: Writing a Simple Linux Kernel Module Manual tags: (\u0026#39;kernel\u0026#39;, \u0026#39;linux\u0026#39;) predicted as: (\u0026#39;linux\u0026#39;,) Article title: Semantic segmentation with OpenCV and deep learning Manual tags: (\u0026#39;deep-learning\u0026#39;, \u0026#39;opencv\u0026#39;) predicted as: (\u0026#39;deep-learning\u0026#39;, \u0026#39;image-classification\u0026#39;, \u0026#39;opencv\u0026#39;) Article title: Transducers: Efficient Data Processing Pipelines in JavaScript Manual tags: (\u0026#39;javascript\u0026#39;,) predicted as: (\u0026#39;javascript\u0026#39;,) Article title: C++之stl::string写时拷贝导致的问题 Manual tags: (\u0026#39;cpp\u0026#39;,) predicted as: (\u0026#39;functional-programming\u0026#39;,) Article title: WebSocket 浅析 Manual tags: (\u0026#39;websocket\u0026#39;,) predicted as: (\u0026#39;websocket\u0026#39;,) Article title: You shouldn’t name your variables after their types for the same reason you wouldn’t name your pets “dog” or “cat” Manual tags: (\u0026#39;golang\u0026#39;,) predicted as: (\u0026#39;golang\u0026#39;,) Article title: Introduction to Data Visualization using Python Manual tags: (\u0026#39;data-visualization\u0026#39;, \u0026#39;python\u0026#39;) predicted as: (\u0026#39;data-visualization\u0026#39;, \u0026#39;matplotlib\u0026#39;, \u0026#39;python\u0026#39;) Article title: How JavaScript works: A comparison with WebAssembly + why in certain cases it’s better to use it over JavaScript Manual tags: (\u0026#39;javascript\u0026#39;, \u0026#39;webassembly\u0026#39;) predicted as: (\u0026#39;javascript\u0026#39;, \u0026#39;webassembly\u0026#39;) Article title: Parsing logs 230x faster with Rust Manual tags: (\u0026#39;log\u0026#39;, \u0026#39;rust\u0026#39;) predicted as: (\u0026#39;rust\u0026#39;,) Article title: Troubleshooting Memory Issues in Java Applications Manual tags: (\u0026#39;java\u0026#39;, \u0026#39;memory\u0026#39;) predicted as: (\u0026#39;java\u0026#39;,) Article title: How to use Docker for Node.js development Manual tags: (\u0026#39;docker\u0026#39;, \u0026#39;node.js\u0026#39;) predicted as: (\u0026#39;docker\u0026#39;,)  A glance at the different evaluation metrics   Now let\u0026#39;s have a look at the evaluation metrics on the prediction performance. Evaluating multi-label classification is very different from evaluating binary classification. There\u0026#39;re quite many different evaluation methods for different situations in the model evaluation part of scikit-learn\u0026#39;s documentation. We will take a look at the ones that suit this problem.  We can start with the accuracy_score function in metrics module. As mentioned in scikit-learn documentation, in multi-label classification, a subset accuracy is 1.0 when the entire set of predicted labels for a sample matches strictly with the true label set. The equation is simple like this: $$\\operatorname{accuracy}(y, \\hat{y})=\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {minples }}-1} 1\\left(\\hat{y}_{i}=y_{i}\\right)$$ from sklearn import metrics import matplotlib.pyplot as plt metrics.accuracy_score(Y_test, Y_pred)  0.26356589147286824   The score is somehow low. But we should be noted that for this problem, an inexact match of the labels is acceptable in many cases, e.g., an article talks about the golang\u0026#39;s interface is predicted with an only label golang while it was manually labeled with golang and interface. So to my opinion, this accuracy_score is not a good evaluation metric for this problem.  Now let\u0026#39;s see the classification_report that presents averaged precision, recall and f1-score. print(metrics.classification_report(Y_test, Y_pred))       precision recall f1-score support   micro avg 0.74 0.42 0.54 1186   macro avg 0.17 0.13 0.14 1186   weighted avg 0.60 0.42 0.48 1186     Let\u0026#39;s look at the micro row. Why? Let me quote scikit-learn\u0026#39;s documentation:   \u0026#34;micro\u0026#34; gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.   Here we\u0026#39;re more interested in the average precision, which is 0.74. As we mentioned, for this problem and for me, it\u0026#39;s more important to not predict a label that should be negative to an article. Some of the labels for an article, e.g., the label interface for the just mentioned article, are less important. So I\u0026#39;m OK for having a low score of recall, which measure how good the model predicts all the labels as the manually labeled.  However, there\u0026#39;s much room for improvement. Another problem to be though about is, the training samples are not equally labeled, i.e., for the same example all the articles talking about golang\u0026#39;s interface, some of them labeled with golang + interface while some of them labeled only golang. ","date":1568201803,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568374603,"objectID":"ac7a73f355de3ce2c7a7929c89ab4b51","permalink":"https://pcx.linkedinfo.co/post/texttagspred/","publishdate":"2019-09-11T13:36:43+02:00","relpermalink":"/post/texttagspred/","section":"post","summary":"This code snippet is to predict topic tags based on the text of an article. Each article could have 1 or more tags (usually have at least 1 tag), and the tags are not mutually exclusive. So this is a multi-label classification problem. It\u0026#39;s different from multi-class classification, the classes in multi-class classification are mutually exclusive, i.e., each item belongs to 1 and only 1 class.  In this snippet, we will use OneVsRestClassifier (the One-Vs-the-Rest) in scikit-learn to process the multi-label classification.","tags":["Machine Learning","Multi-label classification","Text classification","LinkedInfo.co"],"title":"Multi-label classification to predict topic tags of technical articles from LinkedInfo.co","type":"post"},{"authors":["Cong Peng","Prashant Goswami"],"categories":null,"content":"","date":1554069600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554069600,"objectID":"60d09f5ebc761287248b9f096f0928f1","permalink":"https://pcx.linkedinfo.co/publication/pcx-2019-a/","publishdate":"2019-04-01T00:00:00+02:00","relpermalink":"/publication/pcx-2019-a/","section":"publication","summary":"The development of electronic health records, wearable devices, health applications and Internet of Things (IoT)-empowered smart homes is promoting various applications. It also makes health self-management much more feasible, which can partially mitigate one of the challenges that the current healthcare system is facing. Effective and convenient self-management of health requires the collaborative use of health data and home environment data from different services, devices, and even open data on the Web. Although health data interoperability standards including HL7 Fast Healthcare Interoperability Resources (FHIR) and IoT ontology including Semantic Sensor Network (SSN) have been developed and promoted, it is impossible for all the different categories of services to adopt the same standard in the near future. This study presents a method that applies Semantic Web technologies to integrate the health data and home environment data from heterogeneously built services and devices. We propose a Web Ontology Language (OWL)-based integration ontology that models health data from HL7 FHIR standard implemented services, normal Web services and Web of Things (WoT) services and Linked Data together with home environment data from formal ontology-described WoT services. It works on the resource integration layer of the layered integration architecture. An example use case with a prototype implementation shows that the proposed method successfully integrates the health data and home environment data into a resource graph. The integrated data are annotated with semantics and ontological links, which make them machine-understandable and cross-system reusable.","tags":["FHIR","REST","Semantic Web","Web service","WoT","eHealth","health data integration","ontology","smart homes"],"title":"Meaningful Integration of Data from Heterogeneous Health Services and Home Environment Based on Ontology","type":"publication"},{"authors":null,"categories":null,"content":"The Web should be an open web. All the informations published on the Web are meant to be shared, share through links by search engines, rss, social networks, etc. This site is yet another method that tries to link all the informations (but starts with only technical articles on LinkedInfo) and share them.\nThe original idea of this side project is to utilize Semantic Web technologies and Machine learning to link the informations. Noble ambition shall start from basic, it needs to be improved little by little.\n","date":1548584274,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548584274,"objectID":"6268ed86f6986d5dbdacded5a30b3972","permalink":"https://pcx.linkedinfo.co/project/linkedinfo/","publishdate":"2019-01-27T11:17:54+01:00","relpermalink":"/project/linkedinfo/","section":"project","summary":"The Web should be an open web, all the informations published on the Web are meant to be shared. Linkedinfo.co is a Web service uses Semantic Web technologies and Machine Learning to link and share technical articles on the Web.","tags":["Semantic Web","Web technologies","Machine Learning"],"title":"LinkedInfo.co","type":"project"},{"authors":["Cong Peng"],"categories":null,"content":"","date":1546297200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546297200,"objectID":"8ca333643ce6917c513fec55c6c10a0c","permalink":"https://pcx.linkedinfo.co/publication/peng-2019/","publishdate":"2019-01-01T00:00:00+01:00","relpermalink":"/publication/peng-2019/","section":"publication","summary":"Group work-based learning is encouraged in higher education on account of both ped-agogical benefits and industrial employers's requirements. However, although a plenty of studies have been performed, there are still various factors that will affect students' group work-based learning in practice. It is important for the teachers to understand which factors are influenceable and what can be done to influence. This paper performs a literature review to identify the factors that has been investigated and reported in journal articles. Fifteen journal articles were found relevant and fifteen factors were identified, which could be influenced by instructors directly or indirectly. However, more evidence is needed to support the conclusion of some studies since they were performed only in one single course. Therefore, more studies are required on this topic to investigate the factors in different subject areas.","tags":["Group work","TBL","collaborative learning","teamwork"],"title":"What Can Teachers Do to Make the Group Work Learning Effective - a Literature Review","type":"publication"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1538344800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538344800,"objectID":"9706bb40671e2c103cc24202b650b5b7","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-d/","publishdate":"2018-10-01T00:00:00+02:00","relpermalink":"/publication/pcx-2018-d/","section":"publication","summary":"Effective and convenient self-management of health requires collaborative utilization of health data from different services provided by healthcare providers, consumer-facing products and even open data on the Web. Although health data interoperability standards include Fast Healthcare Interoperability Resources (FHIR) have been developed and promoted, it is impossible for all the different categories of services to adopt in the near future. The objective of this study aims to apply Semantic Web technologies to integrate the health data from heterogeneously built services. We present an Web Ontology Language (OWL)-based ontology that models together health data from FHIR standard implemented services, normal Web services and Linked Data. It works on the resource integration layer of the presented layered integration architecture. An example use case that demonstrates how this method integrates the health data into a linked semantic health resource graph with the proposed ontology is presented.","tags":["FHIR","Health data integration","REST","Semantic Web","Web service","eHealth","ontology"],"title":"An Ontological Approach to Integrate Health Resources from Different Categories of Services","type":"publication"},{"authors":[],"categories":[],"content":"","date":1515554369,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516245569,"objectID":"a0b16b66c87372eb421e9e35ae610b5b","permalink":"https://pcx.linkedinfo.co/post/web-notes/","publishdate":"2018-01-10T05:19:29+02:00","relpermalink":"/post/web-notes/","section":"post","summary":"Typically, the term “Web services” is used to label the older, XML-based interfaces on top of HTTP. The term “Web APIs” is more fashionable for interfaces that use HTTP and JSON. ","tags":["Web","Semantic Web"],"title":"Web Notes","type":"post"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"2979f299debfb142891b565c7962abe8","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-c/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-c/","section":"publication","summary":"The vast amount of Web services brings the problem of discovering desired services for composition and orchestration. The syntactic service matching methods based on the classical set theory have a difficulty to capture the imprecise information. Therefore, an approximate service matching approach based on fuzzy control is explored in this paper. A service description matching model to the OpenAPI specification, which is the most widely used standard for describing the defacto REST Web services, is proposed to realize the fuzzy service matching with the fuzzy inference method developed by Mamdani and Assilian. An evaluation shows that the fuzzy service matching approach performed slightly better than the weighted approach in the setting context.","tags":null,"title":"Fuzzy Matching of OpenAPI Described REST Services","type":"publication"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"07ecb044ca58895dc86215b3dd2d5afb","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-b/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-b/","section":"publication","summary":"Various health Web services host a huge amount of health data about patients. The heterogeneity of the services hinders the collaborative utilization of these health data, which can provide a valuable support for the self-management of chronic diseases. The combination of REST Web services and Semantic Web technologies has proven to be a viable approach to address the problem. This paper proposes a method to add semantic annotations to the REST Web services. The service descriptions and the resource representations with semantic annotations can be transformed into a resource graph. It integrates health data from different services, and can link to the health-domain ontologies and Linked Open Health Data to support health management and imaginative applications. The feasibility of our method is demonstrated by realizing with OpenAPI service description and JSON-LD representation in an example use case.","tags":null,"title":"Linking Health Web Services as Resource Graph by Semantic REST Resource Tagging","type":"publication"},{"authors":["Cong Peng","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"bd953f4e0ce85f1fbf6d4b9aa345c53d","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-a/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-a/","section":"publication","summary":"The utilization of Web services is becoming a human labor consuming work as the rapid growth of Web. The semantic annotated service description can support more automatic ways on tasks such as service discovery, invocation and composition. But the adoption of existed Semantic Web Services solutions is hindering by their overly complexity and high expertise demand. In this paper we propose a highly lightweight and non-intrusive method to enrich the REST Web service resources with semantic annotations to support a more autonomous Web service utilization and generic client service interaction. It is achieved by turning the service description into a semantic resource graph represented in RDF, with the added tag based semantic annotation and a small vocabulary. The method is implemented with the popular OpenAPI service description format, and illustrated by a simple use case example.","tags":["OpenAPI","REST","Semantic Annotation","Semantic Web Services","Service Discovery","Web Service Description"],"title":"Using Tag based Semantic Annotation to Empower Client and REST Service Interaction","type":"publication"},{"authors":null,"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"139f78476b81d743eb7410727891ddd5","permalink":"https://pcx.linkedinfo.co/post-old/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/post-old/","section":"","summary":"Hello!","tags":null,"title":"Posts","type":"widget_page"},{"authors":["Cong Peng"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"bf2e6fb9722806fbbc16597bcfe691f8","permalink":"https://pcx.linkedinfo.co/publication/peng-2017/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/peng-2017/","section":"publication","summary":"","tags":["Record keeping","research ethics"],"title":"Good Record Keeping for Conducting Research Ethically Correct","type":"publication"},{"authors":["Cong Peng","Guohua Bai"],"categories":null,"content":"","date":1451602800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451602800,"objectID":"31e3a3b71b5ab915c6d140c2e759a6b2","permalink":"https://pcx.linkedinfo.co/publication/pcx-2016/","publishdate":"2016-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2016/","section":"publication","summary":"Health data sharing can benefit patients to self-manage the challenging chronic diseases out of hospital. The patient controlled electronic Personal Health Record (PHR), as a tool manages comprehensive health data, is absolutely a good entry point to share health data with multiple parties for mutual benefits in the long-term. However, sharing health data from PHR remains challenges. The sharing of health data has to be considered holistically together with the key issues such as privacy, compatibility, evolvement and so on. A PHR system should be flexible to aggregate health data of a patient from various sources to make it comprehensive and up-to-date, should be flexible to share different categories and levels of health data for various utilizations and should be flexible to embed emerging access control mechanisms to ensure privacy and security under different sceneries. Therefore, the flexibility of system architecture on the integration of existed and future diversifications is crucial for PHR's practical long-term usability. This paper discussed the necessity and some possible solution, based on the reviewed literatures and the experience from a previous study, of flexible PHR system architecture on the mentioned aspects.","tags":null,"title":"Flexible System Architecture of PHR to Support Sharing Health Data for Chronic Disease Self-Management","type":"publication"},{"authors":["Y. Hu","C. Peng","G. Bai"],"categories":null,"content":"","date":1420066800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420066800,"objectID":"4de5be7c4b3d9f4859435c10012402f6","permalink":"https://pcx.linkedinfo.co/publication/huyanpcx-2015/","publishdate":"2015-01-01T00:00:00+01:00","relpermalink":"/publication/huyanpcx-2015/","section":"publication","summary":"Nowadays, patient self-management is encouraged in home-based healthcare, especially for chronic disease care. Sharing health information could improve the quality of patient self-management. In this paper, we introduce cloud computing as a potential technology to provide a more sustainable long-term solution compared with other technologies. A hybrid cloud is identified as a suitable way to enable patients to share health information for promoting the treatment of chronic diseases. And then a prototype on the case of type 2 diabetes is implemented to prove the feasibility of the proposed solution.","tags":["Chronic disease","Hybrid cloud","cloud computing"],"title":"Sharing health data through hybrid cloud for self-management","type":"publication"},{"authors":[],"categories":[],"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"112a179f289faa12977d845909944856","permalink":"https://pcx.linkedinfo.co/post/swift-brief-intro/","publishdate":"2014-07-01T00:00:00Z","relpermalink":"/post/swift-brief-intro/","section":"post","summary":"Highly Available, distributed, eventually consistent object store","tags":["OpenStack Swift"],"title":"OpenStack Swift Brief Intro","type":"post"}]