[{"authors":null,"categories":null,"content":"Cong Peng is a PhD student in the Department of Computer Science at Blekinge Institute of Technology (BTH). He has master’s degrees in Computer Science from BTH, and Software Engineering from Zhejiang University of Technology in China. His research interests are Semantic Web, Machine Learning, eHealth and Web services. He has been involved in the works of teaching assistant, bachelor thesis supervision and review.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pcx.linkedinfo.co/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Cong Peng is a PhD student in the Department of Computer Science at Blekinge Institute of Technology (BTH). He has master’s degrees in Computer Science from BTH, and Software Engineering from Zhejiang University of Technology in China. His research interests are Semantic Web, Machine Learning, eHealth and Web services. He has been involved in the works of teaching assistant, bachelor thesis supervision and review.","tags":null,"title":"PENG, Cong","type":"authors"},{"authors":[],"categories":[],"content":" Introduction This is a follow up post of Multi-label classification to predict topic tags of technical articles from LinkedInfo.co. We will continute the same task by using BERT.\nFirstly we\u0026rsquo;ll just use the embeddings from BERT, and then feed them to the same classification model used in the last post, SVM with linear kenel. The reason of keep using SVM is that the size of the dataset is quite small.\nExperiments Classify by using BERT-Mini and SVM with Linear Kernel Due to the limited computation capacity, we\u0026rsquo;ll use a smaller BERT model - BERT-Mini. The first experiment we\u0026rsquo;ll try to train on only the titles of the articles.\nNow we firstly load the dataset. And then load the pretrained BERT tokenizer and model. Note that we only load the article samples that are in English since the BERT-Mini model here were pretrained in English.\nimport os from collections import Counter import numpy as np import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC, LinearSVC from sklearn.multiclass import OneVsRestClassifier from sklearn.feature_extraction.text import TfidfVectorizer from sklearn import metrics import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader, RandomSampler, SequentialSampler import nltk import plotly.express as px from transformers import (BertPreTrainedModel, AutoTokenizer, AutoModel, BertForSequenceClassification, AdamW, BertModel, BertTokenizer, BertConfig, get_linear_schedule_with_warmup) import dataset from mltb.bert import bert_tokenize, bert_transform, get_tokenizer_model, download_once_pretrained_transformers from mltb.experiment import multilearn_iterative_train_test_split from mltb.metrics import classification_report_avg nltk.download('punkt') RAND_STATE = 20200122  ds = dataset.ds_info_tags(from_batch_cache='info', lan='en', concate_title=True, filter_tags_threshold=0, partial_len=3000)  c = Counter([tag for tags in ds.target_decoded for tag in tags]) dfc = pd.DataFrame.from_dict(c, orient='index', columns=['count']).sort_values(by='count', ascending=False)[:100] fig_Y = px.bar(dfc, x=dfc.index, y='count', text='count', labels={'count': 'Number of infos', 'x': 'Tags'}) fig_Y.update_traces(texttemplate='%{text}')      if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'};   window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"d7db1ccb-6f97-4d35-a5b0-eda705ab1bdb\")) { Plotly.newPlot( 'd7db1ccb-6f97-4d35-a5b0-eda705ab1bdb', [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Tags=%{x}\nNumber of infos=%{text}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"text\": [425.0, 272.0, 181.0, 155.0, 129.0, 89.0, 80.0, 77.0, 67.0, 60.0, 53.0, 50.0, 48.0, 47.0, 42.0, 39.0, 39.0, 39.0, 35.0, 35.0, 33.0, 32.0, 32.0, 32.0, 31.0, 31.0, 31.0, 31.0, 30.0, 29.0, 28.0, 27.0, 27.0, 26.0, 26.0, 25.0, 25.0, 25.0, 22.0, 22.0, 21.0, 20.0, 19.0, 19.0, 19.0, 18.0, 18.0, 17.0, 17.0, 17.0, 16.0, 16.0, 16.0, 16.0, 15.0, 15.0, 15.0, 15.0, 14.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 10.0, 10.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0], \"textposition\": \"auto\", \"texttemplate\": \"%{text}\", \"type\": \"bar\", \"x\": [\"python\", \"golang\", \"web\", \"javascript\", \"machine-learning\", \"microservices\", \"deep-learning\", \"neural-networks\", \"api\", \"data-science\", \"java\", \"node.js\", \"testing\", \"concurrency\", \"vue.js\", \"db\", \"system-architecture\", \"react\", \"compiler\", \"docker\", \"http\", \"kubernetes\", \"rust\", \"git\", \"django\", \"restful\", \"cpp\", \"data-visualization\", \"nlp\", \"oop\", \"kafka\", \"angular\", \"graphql\", \"linux\", \"programming\", \"css\", \"frontend\", \"security\", \"interpreter\", \"functional-programming\", \"distributed-system\", \"chatbot\", \"c\", \"r\", \"ai\", \"mysql\", \"http2\", \"debug\", \"tensorflow\", \"asyncio\", \"tdd\", \"pandas\", \"error-handling\", \"memory\", \"performance\", \"algorithm\", \"semantic-web\", \"emacs\", \"auth\", \"grpc\", \"computer-vision\", \"pascal\", \"statistics\", \"redis\", \"big-data\", \"numpy\", \"postgres\", \"refactoring\", \"mongodb\", \"programmer-development\", \"data-structure\", \"pwa\", \"finance\", \"graphdb\", \"asynchronous\", \"json\", \"cloud-computing\", \"web-scraping\", \"html\", \"spark\", \"blockchain\", \"webpack\", \"vim\", \"dotnet\", \"jwt\", \"sql\", \"stock\", \"csharp\", \"log\", \"lxc\", \"kotlin\", \"storage\", \"multithreading\", \"scala\", \"interface\", \"spring\", \"jit\", \"flask\", \"design-pattern\", \"websocket\"], \"xaxis\": \"x\", \"y\": [425, 272, 181, 155, 129, 89, 80, 77, 67, 60, 53, 50, 48, 47, 42, 39, 39, 39, 35, 35, 33, 32, 32, 32, 31, 31, 31, 31, 30, 29, 28, 27, 27, 26, 26, 25, 25, 25, 22, 22, 21, 20, 19, 19, 19, 18, 18, 17, 17, 17, 16, 16, 16, 16, 15, 15, 15, 15, 14, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8], \"yaxis\": \"y\"}], {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Tags\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Number of infos\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('d7db1ccb-6f97-4d35-a5b0-eda705ab1bdb'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) };     dfc_tail = pd.DataFrame.from_dict(c, orient='index', columns=['count']).sort_values(by='count', ascending=False)[-200:] fig_Y = px.bar(dfc_tail, x=dfc_tail.index, y='count', text='count', labels={'count': 'Number of infos', 'x': 'Tags'}) fig_Y.update_traces(texttemplate='%{text}')      if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'};   window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"93a29e39-9a0c-49f1-9585-5d6ba50f38f5\")) { Plotly.newPlot( '93a29e39-9a0c-49f1-9585-5d6ba50f38f5', [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Tags=%{x}\nNumber of infos=%{text}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"text\": [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], \"textposition\": \"auto\", \"texttemplate\": \"%{text}\", \"type\": \"bar\", \"x\": [\"fusion.js\", \"load-balance\", \"ethic\", \"web-services\", \"aws\", \"filesystem\", \"iot\", \"emacs-lisp\", \"github\", \"crawler\", \"event-driven\", \"pytorch\", \"pdf\", \"lambda\", \"spacemacs\", \"pdb\", \"namespaces\", \"mvc\", \"unicode\", \"sdk\", \"makefile\", \"webkit\", \"cdn\", \"object-detection\", \"documentation\", \"project-management\", \"cloud\", \"classification\", \"jvm\", \"circuit-breaker\", \"integration-test\", \"amqp\", \"anomaly-detection\", \"dom\", \"cloud-storage\", \"ddd\", \"vue-router\", \"activemq\", \"excel\", \"linked-list\", \"linear-programming\", \"electron\", \"kernel\", \"lock-free\", \"data-mining\", \"virtual-machine\", \"d3\", \"neo4j\", \"text-editor\", \"unity\", \"baas\", \"information-theory\", \"messaging\", \"crdt\", \"os\", \"webgl\", \"anti-pattern\", \"hadoop\", \"development\", \"orm\", \"voltdb\", \"linear-model\", \"consul\", \"ansible\", \"pathfinding\", \"linked-data\", \"ajax\", \"rendering-techniques\", \"random-forests\", \"photography\", \"dns\", \"code-review\", \"rsa\", \"bloom-filter\", \"deployment\", \"forward+\", \"token\", \"gcc\", \"oracle\", \"cap\", \"severless\", \"wpf\", \"graphics\", \"cors\", \"locale\", \"ipv6\", \"apache\", \"iots\", \"bytecode\", \"cookie\", \"bot\", \"time-series-analysis\", \"assembly\", \"arm\", \"multiprocessing\", \"cms\", \"cassandra\", \"ggplot\", \"mesos\", \"automl\", \"auditing\", \"service-architecture\", \"goroutine\", \"text-generator\", \"option\", \"travis-ci\", \"gateway\", \"productivity\", \"angularjs\", \"pyspark\", \"data-pipeline\", \"raspberry-pi\", \"service-mesh\", \"memcached\", \"quantum-communication\", \"soa\", \"ssh\", \"open-data\", \"command-line-interface\", \"yaml\", \"server\", \"mac\", \"monitoring\", \"translation\", \"posix\", \"data-analytics\", \"hdfs\", \"recurrent-neural-networks\", \"word-embedding\", \"ludwig\", \"image-recognition\", \"maven\", \"wordpress\", \"elm\", \"monad\", \"plotly\", \"math\", \"protobuf\", \"face-recognition\", \"paxos\", \"academia\", \"speech-recognition\", \"feature-engineering\", \"babel\", \"tachyon\", \"postgresql\", \"relational-database\", \"data-workflow\", \"monkey-patch\", \"udp\", \"ehealth\", \"optimization\", \"webrtc\", \"proxy\", \"cross-validation\", \"hypothesis-test\", \"ivy\", \"markov-chains\", \"text-recognition\", \"latex\", \"openstack\", \"sqlite\", \"computer-system\", \"eslint\", \"obect-detection\", \"bokeh\", \"data-cleaning\", \"color-topics\", \"firebase\", \"doom-emacs\", \"speech--recognition\", \"decentralized-web\", \"tkinter\", \"ops\", \"epoll\", \"index\", \"lua\", \"dgraph\", \"desktop\", \"reliability\", \"laravel\", \"mypy\", \"access-control\", \"virtualization\", \"kvm\", \"qemu\", \"csrf\", \"quantum-computing\", \"nodejs\", \"random-forest\", \"quantum-theory\", \"socket\", \"redux\", \"ide\", \"postgre\", \"text-classifier\", \"etl\", \"cloud-native\", \"agile\", \"sparql\"], \"xaxis\": \"x\", \"y\": [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"yaxis\": \"y\"}], {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Tags\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Number of infos\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('93a29e39-9a0c-49f1-9585-5d6ba50f38f5'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) };     After we loaded the data, we checked how frequently are the tags being tagged to the articles. Here we only visualized the top-100 tags (you can select area of the figure to zoomin), we can see that there\u0026rsquo;s a big imbalancement of popularity among tags. We can try to mitigate this imbalancement by using different methods like sampling methods and augmentation. But now we\u0026rsquo;ll just pretend we don\u0026rsquo;t know that and leave this aside.\nNow let\u0026rsquo;s load the BERT tokenizer and model.\nPRETRAINED_BERT_WEIGHTS = download_once_pretrained_transformers( \u0026quot;google/bert_uncased_L-4_H-256_A-4\u0026quot;) tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BERT_WEIGHTS) model = AutoModel.from_pretrained(PRETRAINED_BERT_WEIGHTS)  Now we encode all the titles by the BERT-Mini model. We\u0026rsquo;ll use only the 1st output vector from the model as it\u0026rsquo;s used for classification task.\ncol_text = 'title' max_length = ds.data[col_text].apply(lambda x: len(nltk.word_tokenize(x))).max() encoded = ds.data[col_text].apply( (lambda x: tokenizer.encode_plus(x, add_special_tokens=True, pad_to_max_length=True, return_attention_mask=True, max_length=max_length, return_tensors='pt'))) input_ids = torch.cat(tuple(encoded.apply(lambda x:x['input_ids']))) attention_mask = torch.cat(tuple(encoded.apply(lambda x:x['attention_mask']))) features = [] with torch.no_grad(): last_hidden_states = model(input_ids, attention_mask=attention_mask) features = last_hidden_states[0][:, 0, :].numpy()  As the features are changed from Tf-idf transformed to BERT transformed, so we\u0026rsquo;ll re-search for the hyper-parameters for the LinearSVC to use.\nThe scorer we used in grid search is f-0.5 score since we want to weight higher precision over recall.\ntrain_features, test_features, train_labels, test_labels = train_test_split( features, ds.target, test_size=0.3, random_state=RAND_STATE) clf = OneVsRestClassifier(LinearSVC()) C_OPTIONS = [0.01, 0.1, 0.5, 1, 10] parameters = { 'estimator__penalty': ['l1', 'l2'], 'estimator__dual': [True, False], 'estimator__C': C_OPTIONS, } micro_f05_sco = metrics.make_scorer( metrics.fbeta_score, beta=0.5, average='micro') gs_clf = GridSearchCV(clf, parameters, scoring=micro_f05_sco, cv=3, n_jobs=-1) gs_clf.fit(train_features, train_labels) print(gs_clf.best_params_) print(gs_clf.best_score_) Y_predicted = gs_clf.predict(test_features) report = metrics.classification_report( test_labels, Y_predicted, output_dict=True, zero_division=0) df_report = pd.DataFrame(report).transpose() cols_avg = ['micro avg', 'macro avg', 'weighted avg', 'samples avg'] df_report.loc[cols_avg,]  {'estimator__C': 0.1, 'estimator__dual': True, 'estimator__penalty': 'l2'} 0.5793483937857783   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    precision recall f1-score support     micro avg 0.892857 0.242326 0.381194 1238.0   macro avg 0.173746 0.092542 0.111124 1238.0   weighted avg 0.608618 0.242326 0.324186 1238.0   samples avg 0.404088 0.274188 0.312305 1238.0     Though it\u0026rsquo;s not comparable, the result metrics are no better than the Tf-idf one when we use only the English samples with their titles here. The micro average precision is higher, the other averages of precision are about the same. The recalls got much lower.\nNow let\u0026rsquo;s combine the titles and short descriptions to see if there\u0026rsquo;s any improvment.\ncol_text = 'description' max_length = ds.data[col_text].apply(lambda x: len(nltk.word_tokenize(x))).max() encoded = ds.data[col_text].apply( (lambda x: tokenizer.encode_plus(x, add_special_tokens=True, pad_to_max_length=True, return_attention_mask=True, max_length=max_length, return_tensors='pt'))) input_ids = torch.cat(tuple(encoded.apply(lambda x:x['input_ids']))) attention_mask = torch.cat(tuple(encoded.apply(lambda x:x['attention_mask']))) features = [] with torch.no_grad(): last_hidden_states = model(input_ids, attention_mask=attention_mask) features = last_hidden_states[0][:, 0, :].numpy()  train_features, test_features, train_labels, test_labels = train_test_split( features, ds.target, test_size=0.3, random_state=RAND_STATE)  clf = OneVsRestClassifier(LinearSVC()) C_OPTIONS = [0.1, 1] parameters = { 'estimator__penalty': ['l2'], 'estimator__dual': [True], 'estimator__C': C_OPTIONS, } micro_f05_sco = metrics.make_scorer( metrics.fbeta_score, beta=0.5, average='micro') gs_clf = GridSearchCV(clf, parameters, scoring=micro_f05_sco, cv=3, n_jobs=-1) gs_clf.fit(train_features, train_labels) print(gs_clf.best_params_) print(gs_clf.best_score_) Y_predicted = gs_clf.predict(test_features) classification_report_avg(test_labels, Y_predicted)  {'estimator__C': 1, 'estimator__dual': True, 'estimator__penalty': 'l2'} 0.4954311860243222 precision recall f1-score support micro avg 0.684015 0.297254 0.414414 1238.0 macro avg 0.178030 0.109793 0.127622 1238.0 weighted avg 0.522266 0.297254 0.362237 1238.0 samples avg 0.401599 0.314649 0.337884 1238.0  There is no improvement, the precision averages even got a little bit worse. Let\u0026rsquo;s try to explore further.\nIterative stratified multilabel data sampling It would be a good idea to perform stratified sampling for spliting training and test sets since there\u0026rsquo;s a big imbalancement in the dataset for the labels. The problem is that the size of dataset is very small, which causes it that using normal stratified sampling method would fail since it\u0026rsquo;s likely that some labels may not appear in both training and testing sets. That\u0026rsquo;s why we have to use iterative stratified multilabel sampling. The explanation of this method can refer to document of scikit-multilearn.\nIn the code below we have wrapped the split method for brevity.\nCOL_TEXT = 'description' train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split( ds.data, ds.target, test_size=0.3, cols=ds.data.columns) batch_size = 128 model_name = \u0026quot;google/bert_uncased_L-4_H-256_A-4\u0026quot; train_features, test_features = bert_transform( train_features, test_features, COL_TEXT, model_name, batch_size) clf = OneVsRestClassifier(LinearSVC()) C_OPTIONS = [0.1, 1] parameters = { 'estimator__penalty': ['l2'], 'estimator__dual': [True], 'estimator__C': C_OPTIONS, } micro_f05_sco = metrics.make_scorer( metrics.fbeta_score, beta=0.5, average='micro') gs_clf = GridSearchCV(clf, parameters, scoring=micro_f05_sco, cv=3, n_jobs=-1) gs_clf.fit(train_features, train_labels) print(gs_clf.best_params_) print(gs_clf.best_score_) Y_predicted = gs_clf.predict(test_features) print(classification_report_avg(test_labels, Y_predicted))  {'estimator__C': 0.1, 'estimator__dual': True, 'estimator__penalty': 'l2'} 0.3292528001922235 precision recall f1-score support micro avg 0.674086 0.356003 0.465934 1191.0 macro avg 0.230836 0.162106 0.181784 1191.0 weighted avg 0.551619 0.356003 0.420731 1191.0 samples avg 0.460420 0.377735 0.392599 1191.0  There seems no improvement. But the cross validation F-0.5 score is lower than the testing score. It might be a sign that it\u0026rsquo;s under-fitting.\nTraining set augmentation As the dataset is quite small, now we\u0026rsquo;ll try to augment the trainig set to see if there\u0026rsquo;s any improvement.\nHere we set the augmentation level to 2, which means the dataset are concatenated by 2 times of the samples. And the added samples\u0026rsquo; content will be randomly chopped out as 9\u0026frasl;10 of its original content. Of course, both the actions only apply to the training set. The 30% test set is kept aside.\nCOL_TEXT = 'description' train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split( ds.data, ds.target, test_size=0.3, cols=ds.data.columns) train_features, train_labels = dataset.augmented_samples( train_features, train_labels, level=2, crop_ratio=0.1) batch_size = 128 model_name = \u0026quot;google/bert_uncased_L-4_H-256_A-4\u0026quot; train_features, test_features = bert_transform( train_features, test_features, COL_TEXT, model_name, batch_size) clf = OneVsRestClassifier(LinearSVC()) C_OPTIONS = [0.1, 1] parameters = { 'estimator__penalty': ['l2'], 'estimator__dual': [True], 'estimator__C': C_OPTIONS, } micro_f05_sco = metrics.make_scorer( metrics.fbeta_score, beta=0.5, average='micro') gs_clf = GridSearchCV(clf, parameters, scoring=micro_f05_sco, cv=3, n_jobs=-1) gs_clf.fit(train_features, train_labels) print(gs_clf.best_params_) print(gs_clf.best_score_) Y_predicted = gs_clf.predict(test_features) classification_report_avg(test_labels, Y_predicted)  {'estimator__C': 0.1, 'estimator__dual': True, 'estimator__penalty': 'l2'} 0.9249583214520737 precision recall f1-score support micro avg 0.616296 0.348409 0.445158 1194.0 macro avg 0.224752 0.162945 0.180873 1194.0 weighted avg 0.520024 0.348409 0.406509 1194.0 samples avg 0.442572 0.373784 0.384738 1194.0  We can see that there\u0026rsquo;s still no improvement. It seems that we should change direction.\nFilter rare tags If you remember that the first time we loaded the data we visualized the appearence frequency of the tags. It showed that most of the tags appeared only very few times, over 200 tags appeared only once or twice. This is quite a big problem for the model to classify for these tags.\nNow let\u0026rsquo;s try to filter out the least appeared tags. Let\u0026rsquo;s start from a big number of 20, i.e., tags appeared in less than 20 articles will be removed.\ncol_text = 'description' ds_param = dict(from_batch_cache='info', lan='en', concate_title=True, filter_tags_threshold=20) ds = dataset.ds_info_tags(**ds_param) c = Counter([tag for tags in ds.target_decoded for tag in tags]) dfc = pd.DataFrame.from_dict(c, orient='index', columns=['count']).sort_values(by='count', ascending=False)[:100] fig_Y = px.bar(dfc, x=dfc.index, y='count', text='count', labels={'count': 'Number of infos', 'x': 'Tags'}) fig_Y.update_traces(texttemplate='%{text}')      if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'};   window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"1c689a44-bec8-4749-bd6f-2a3cc74c5cd9\")) { Plotly.newPlot( '1c689a44-bec8-4749-bd6f-2a3cc74c5cd9', [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Tags=%{x}\nNumber of infos=%{text}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"text\": [425.0, 272.0, 181.0, 155.0, 129.0, 89.0, 80.0, 77.0, 67.0, 60.0, 53.0, 50.0, 48.0, 47.0, 42.0, 39.0, 39.0, 39.0, 35.0, 35.0, 33.0, 32.0, 32.0, 32.0, 31.0, 31.0, 31.0, 31.0, 30.0, 29.0, 28.0, 27.0, 27.0, 26.0, 26.0, 25.0, 25.0, 25.0, 22.0, 22.0, 21.0], \"textposition\": \"auto\", \"texttemplate\": \"%{text}\", \"type\": \"bar\", \"x\": [\"python\", \"golang\", \"web\", \"javascript\", \"machine-learning\", \"microservices\", \"deep-learning\", \"neural-networks\", \"api\", \"data-science\", \"java\", \"node.js\", \"testing\", \"concurrency\", \"vue.js\", \"system-architecture\", \"react\", \"db\", \"compiler\", \"docker\", \"http\", \"git\", \"kubernetes\", \"rust\", \"restful\", \"data-visualization\", \"cpp\", \"django\", \"nlp\", \"oop\", \"kafka\", \"graphql\", \"angular\", \"programming\", \"linux\", \"css\", \"frontend\", \"security\", \"functional-programming\", \"interpreter\", \"distributed-system\"], \"xaxis\": \"x\", \"y\": [425, 272, 181, 155, 129, 89, 80, 77, 67, 60, 53, 50, 48, 47, 42, 39, 39, 39, 35, 35, 33, 32, 32, 32, 31, 31, 31, 31, 30, 29, 28, 27, 27, 26, 26, 25, 25, 25, 22, 22, 21], \"yaxis\": \"y\"}], {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Tags\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Number of infos\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('1c689a44-bec8-4749-bd6f-2a3cc74c5cd9'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) };     test_size = 0.3 train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split( ds.data, ds.target, test_size=test_size, cols=ds.data.columns) train_features, train_labels = dataset.augmented_samples( train_features, train_labels, level=2, crop_ratio=0.1) batch_size = 128 model_name = \u0026quot;google/bert_uncased_L-4_H-256_A-4\u0026quot; train_features, test_features = bert_transform( train_features, test_features, col_text, model_name, batch_size) clf = OneVsRestClassifier(LinearSVC()) C_OPTIONS = [0.1, 1] parameters = { 'estimator__penalty': ['l2'], 'estimator__dual': [True], 'estimator__C': C_OPTIONS, } micro_f05_sco = metrics.make_scorer( metrics.fbeta_score, beta=0.5, average='micro') gs_clf = GridSearchCV(clf, parameters, scoring=micro_f05_sco, cv=3, n_jobs=-1) gs_clf.fit(train_features, train_labels) print(f'Best params in CV: {gs_clf.best_params_}') print(f'Best score in CV: {gs_clf.best_score_}') Y_predicted = gs_clf.predict(test_features) classification_report_avg(test_labels, Y_predicted)  Best params in CV: {'estimator__C': 0.1, 'estimator__dual': True, 'estimator__penalty': 'l2'} Best score in CV: 0.8943719982878996 precision recall f1-score support micro avg 0.593583 0.435294 0.502262 765.0 macro avg 0.523965 0.361293 0.416650 765.0 weighted avg 0.586632 0.435294 0.490803 765.0 samples avg 0.458254 0.472063 0.444127 765.0  The filtering of tags made the averages of recall higher, but made the precision lower. The macro average goes up as there\u0026rsquo;re much fewer tags.\nFine-tuning BERT model The next step is to see if we can make some progress by fine-tuning the BERT-Mini model. As for a comparable result, the fine-tuning training will be using the same dataset that filtered of tags appear at least in 20 infos. The final classifier model will also be the same of SVM with Linear kernel feeded by the embeddings from the fine-tuned BERT-Mini.\nThe processing of fine-tuning refers much to Chris McCormick\u0026rsquo;s post.\ncol_text = 'description' ds_param = dict(from_batch_cache='info', lan='en', concate_title=True, filter_tags_threshold=20) ds = dataset.ds_info_tags(**ds_param) test_size = 0.3 train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split( ds.data, ds.target, test_size=test_size, cols=ds.data.columns) train_features, train_labels = dataset.augmented_samples( train_features, train_labels, level=2, crop_ratio=0.1)  The BertForSequenceMultiLabelClassification class defined below is basically a copy of the BertForSequenceClassification class in huggingface\u0026rsquo;s Transformers, only with a small change of adding sigmoid the logits from classification and adding labels = torch.max(labels, 1)[1] in forward for supporting multilabel.\nclass BertForSequenceMultiLabelClassification(BertPreTrainedModel): def __init__(self, config): super(BertForSequenceMultiLabelClassification, self).__init__(config) self.num_labels = config.num_labels self.bert = BertModel(config) self.dropout = nn.Dropout(config.hidden_dropout_prob) self.classifier = nn.Linear(config.hidden_size, self.config.num_labels) self.init_weights() def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None): outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds) pooled_output = outputs[1] pooled_output = self.dropout(pooled_output) logits = self.classifier(pooled_output) logtis = torch.sigmoid(logits) # add hidden states and attention if they are here outputs = (logits,) + outputs[2:] if labels is not None: if self.num_labels == 1: # We are doing regression loss_fct = nn.MSELoss() loss = loss_fct(logits.view(-1), labels.view(-1)) else: loss_fct = nn.CrossEntropyLoss() labels = torch.max(labels, 1)[1] loss = loss_fct( logits.view(-1, self.num_labels), labels.view(-1)) outputs = (loss,) + outputs return outputs # (loss), logits, (hidden_states), (attentions)  DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' n_classes = train_labels.shape[1] batch_size: int = 16 epochs: int = 4 model_name = download_once_pretrained_transformers( \u0026quot;google/bert_uncased_L-4_H-256_A-4\u0026quot;) model = BertForSequenceMultiLabelClassification.from_pretrained( model_name, num_labels=n_classes, output_attentions=False, output_hidden_states=False, ) model.to(DEVICE) # Prepare optimizer and schedule (linear warmup and decay) no_decay = [\u0026quot;bias\u0026quot;, \u0026quot;LayerNorm.weight\u0026quot;] optimizer_grouped_parameters = [ {\u0026quot;params\u0026quot;: [p for n, p in model.named_parameters() if not any( nd in n for nd in no_decay)], \u0026quot;weight_decay\u0026quot;: 0.1, }, {\u0026quot;params\u0026quot;: [p for n, p in model.named_parameters() if any( nd in n for nd in no_decay)], \u0026quot;weight_decay\u0026quot;: 0.0}, ] optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8 ) tokenizer, model_notuse = get_tokenizer_model(model_name) input_ids, attention_mask = bert_tokenize( tokenizer, train_features, col_text=col_text) input_ids_test, attention_mask_test = bert_tokenize( tokenizer, test_features, col_text=col_text) train_set = torch.utils.data.TensorDataset( input_ids, attention_mask, torch.Tensor(train_labels)) test_set = torch.utils.data.TensorDataset( input_ids_test, attention_mask_test, torch.Tensor(test_labels)) train_loader = torch.utils.data.DataLoader( train_set, batch_size=batch_size, sampler=RandomSampler(train_set)) test_loader = torch.utils.data.DataLoader( test_set, sampler=SequentialSampler(test_set), batch_size=batch_size) total_steps = len(train_loader) * epochs scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, # Default value in run_glue.py num_training_steps=total_steps)  training_stats = [] def best_prec_score(true_labels, predictions): fbeta = 0 thr_bst = 0 for thr in range(0, 6): Y_predicted = (predictions \u0026gt; (thr * 0.1)) f = metrics.average_precision_score( true_labels, Y_predicted, average='micro') if f \u0026gt; fbeta: fbeta = f thr_bst = thr * 0.1 return fbeta, thr def train(): model.train() total_train_loss = 0 for step, (input_ids, masks, labels) in enumerate(train_loader): input_ids, masks, labels = input_ids.to( DEVICE), masks.to(DEVICE), labels.to(DEVICE) model.zero_grad() loss, logits = model(input_ids, token_type_ids=None, attention_mask=masks, labels=labels) total_train_loss += loss.item() loss.backward() # Clip the norm of the gradients to 1.0. # This is to help prevent the \u0026quot;exploding gradients\u0026quot; problem. torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) optimizer.step() scheduler.step() avg_train_loss = total_train_loss / len(train_loader) print(\u0026quot;Train loss: {0:.2f}\u0026quot;.format(avg_train_loss)) def val(): model.eval() val_loss = 0 y_pred, y_true = [], [] # Evaluate data for one epoch for (input_ids, masks, labels) in test_loader: input_ids, masks, labels = input_ids.to( DEVICE), masks.to(DEVICE), labels.to(DEVICE) with torch.no_grad(): (loss, logits) = model(input_ids, token_type_ids=None, attention_mask=masks, labels=labels) val_loss += loss.item() logits = logits.detach().cpu().numpy() label_ids = labels.to('cpu').numpy() y_pred += logits.tolist() y_true += label_ids.tolist() bes_val_prec, bes_val_prec_thr = best_prec_score( np.array(y_true), np.array(y_pred)) y_predicted = (np.array(y_pred) \u0026gt; 0.5) avg_val_loss = val_loss / len(test_loader) print(\u0026quot;Val loss: {0:.2f}\u0026quot;.format(avg_val_loss)) print(\u0026quot;best prec: {0:.4f}, thr: {1}\u0026quot;.format( bes_val_prec, bes_val_prec_thr)) print(classification_report_avg(y_true, y_predicted)) for ep in range(epochs): print(f'-------------- Epoch: {ep+1}/{epochs} --------------') train() val() print('-------------- Completed --------------')  -------------- Epoch: 1/4 -------------- Train loss: 2.94 Val loss: 2.35 best prec: 0.1540, thr: 5 precision recall f1-score support micro avg 0.233079 0.599476 0.335654 764.0 macro avg 0.185025 0.294674 0.196651 764.0 weighted avg 0.225475 0.599476 0.292065 764.0 samples avg 0.252645 0.634959 0.342227 764.0 -------------- Epoch: 2/4 -------------- Train loss: 2.14 Val loss: 1.92 best prec: 0.1848, thr: 5 precision recall f1-score support micro avg 0.255676 0.678010 0.371326 764.0 macro avg 0.381630 0.448064 0.303961 764.0 weighted avg 0.328057 0.678010 0.355185 764.0 samples avg 0.273901 0.735660 0.379705 764.0 -------------- Epoch: 3/4 -------------- Train loss: 1.78 Val loss: 1.74 best prec: 0.1881, thr: 5 precision recall f1-score support micro avg 0.248974 0.714660 0.369293 764.0 macro avg 0.272232 0.524172 0.306814 764.0 weighted avg 0.275572 0.714660 0.364002 764.0 samples avg 0.273428 0.776291 0.383235 764.0 -------------- Epoch: 4/4 -------------- Train loss: 1.61 Val loss: 1.68 best prec: 0.1882, thr: 5 precision recall f1-score support micro avg 0.244105 0.731675 0.366077 764.0 macro avg 0.288398 0.552318 0.310797 764.0 weighted avg 0.294521 0.731675 0.369942 764.0 samples avg 0.267708 0.795730 0.381341 764.0 -------------- Completed --------------  Save the fine-tuned model for later encoding.\nfrom transformers import WEIGHTS_NAME, CONFIG_NAME, BertTokenizer output_dir = \u0026quot;./data/models/bert_finetuned_tagthr_20/\u0026quot; if not os.path.exists(output_dir): os.makedirs(output_dir) # Step 1: Save a model, configuration and vocabulary that you have fine-tuned # If we have a distributed model, save only the encapsulated model # (it was wrapped in PyTorch DistributedDataParallel or DataParallel) model_to_save = model.module if hasattr(model, 'module') else model # If we save using the predefined names, we can load using `from_pretrained` output_model_file = os.path.join(output_dir, WEIGHTS_NAME) output_config_file = os.path.join(output_dir, CONFIG_NAME) torch.save(model_to_save.state_dict(), output_model_file) model_to_save.config.to_json_file(output_config_file) tokenizer.save_vocabulary(output_dir)  ('./data/models/bert_finetuned_tagthr_20/vocab.txt',)  Now let\u0026rsquo;s use the fine-tuned model to get the embeddings for the same SVM classification.\ncol_text = 'description' ds_param = dict(from_batch_cache='info', lan='en', concate_title=True, filter_tags_threshold=20) ds = dataset.ds_info_tags(**ds_param) test_size = 0.3 train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split( ds.data, ds.target, test_size=test_size, cols=ds.data.columns) train_features, train_labels = dataset.augmented_samples( train_features, train_labels, level=2, crop_ratio=0.1) batch_size = 128 model_name = output_dir train_features, test_features = bert_transform( train_features, test_features, col_text, model_name, batch_size) clf = OneVsRestClassifier(LinearSVC()) C_OPTIONS = [0.1, 1, 10] parameters = { 'estimator__penalty': ['l2'], 'estimator__dual': [True], 'estimator__C': C_OPTIONS, } micro_f05_sco = metrics.make_scorer( metrics.fbeta_score, beta=0.5, average='micro') gs_clf = GridSearchCV(clf, parameters, scoring=micro_f05_sco, cv=3, n_jobs=-1) gs_clf.fit(train_features, train_labels) print(gs_clf.best_params_) print(gs_clf.best_score_) Y_predicted = gs_clf.predict(test_features) report = metrics.classification_report( test_labels, Y_predicted, output_dict=True) df_report = pd.DataFrame(report).transpose() cols_avg = ['micro avg', 'macro avg', 'weighted avg', 'samples avg'] df_report.loc[cols_avg]  {'estimator__C': 0.1, 'estimator__dual': True, 'estimator__penalty': 'l2'} 0.945576388765271   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    precision recall f1-score support     micro avg 0.793605 0.714660 0.752066 764.0   macro avg 0.757259 0.642333 0.671768 764.0   weighted avg 0.782557 0.714660 0.732094 764.0   samples avg 0.798664 0.762087 0.754289 764.0     There\u0026rsquo;s quite a big improvement to both precision and recall after fine-tuning. This result makes the model quite usable.\nComeback test with tf-idf Comparing to the early post that the model uses tf-idf to transform the text, we\u0026rsquo;ve made some changes to the dataset loading, spliting and augmentation. I\u0026rsquo;m curious to see if these changes would improve the performance when using tf-idf other than BERT.\nLet\u0026rsquo;s start with samples only in English still.\ncol_text = 'description' ds_param = dict(from_batch_cache='info', lan='en', concate_title=True, filter_tags_threshold=20) ds = dataset.ds_info_tags(**ds_param) train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split( ds.data, ds.target, test_size=0.3, cols=ds.data.columns) train_features, train_labels = dataset.augmented_samples( train_features, train_labels, level=4, crop_ratio=0.2) clf = Pipeline([ ('vect', TfidfVectorizer(use_idf=True, max_df=0.8)), ('clf', OneVsRestClassifier(LinearSVC(penalty='l2', dual=True))), ]) C_OPTIONS = [0.1, 1, 10] parameters = { 'vect__ngram_range': [(1, 4)], 'clf__estimator__C': C_OPTIONS, } gs_clf = GridSearchCV(clf, parameters, cv=3, n_jobs=-1) gs_clf.fit(train_features[col_text], train_labels) print(gs_clf.best_params_) print(gs_clf.best_score_) Y_predicted = gs_clf.predict(test_features[col_text]) classification_report_avg(test_labels, Y_predicted)  {'clf__estimator__C': 10, 'vect__ngram_range': (1, 4)} 0.9986905637969986   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    precision recall f1-score support     micro avg 0.955782 0.367801 0.531191 764.0   macro avg 0.740347 0.250587 0.353632 764.0   weighted avg 0.847419 0.367801 0.487887 764.0   samples avg 0.452608 0.396469 0.412913 764.0     Now let\u0026rsquo;s try samples in both English and Chinese.\ncol_text = 'description' ds_param = dict(from_batch_cache='info', lan=None, concate_title=True, filter_tags_threshold=20) ds = dataset.ds_info_tags(**ds_param) train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split( ds.data, ds.target, test_size=0.3, cols=ds.data.columns) train_features, train_labels = dataset.augmented_samples( train_features, train_labels, level=4, crop_ratio=0.2) clf = Pipeline([ ('vect', TfidfVectorizer(use_idf=True, max_df=0.8)), ('clf', OneVsRestClassifier(LinearSVC(penalty='l2', dual=True))), ]) C_OPTIONS = [0.1, 1, 10] parameters = { 'vect__ngram_range': [(1, 4)], 'clf__estimator__C': C_OPTIONS, } gs_clf = GridSearchCV(clf, parameters, cv=3, n_jobs=-1) gs_clf.fit(train_features[col_text], train_labels) print(gs_clf.best_params_) print(gs_clf.best_score_) Y_predicted = gs_clf.predict(test_features[col_text]) classification_report_avg(test_labels, Y_predicted)  {'clf__estimator__C': 10, 'vect__ngram_range': (1, 4)} 0.9962557077625571   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    precision recall f1-score support     micro avg 0.884273 0.417952 0.567619 1426.0   macro avg 0.804614 0.311396 0.423867 1426.0   weighted avg 0.849494 0.417952 0.532041 1426.0   samples avg 0.487522 0.433421 0.446447 1426.0     We can see that, for both the models, the micro average precision is quite high and the recalls are still low. However, the macro averages are much better since we filtered out minority tags. The model trained on samples with both languages has a lower precisions but higher recalls.\n","date":1581585619,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585660214,"objectID":"b7762b2d95ca593bfa29eedd6ad85406","permalink":"https://pcx.linkedinfo.co/post/text-tag-prediction-bert/","publishdate":"2020-02-13T11:20:19+02:00","relpermalink":"/post/text-tag-prediction-bert/","section":"post","summary":"Introduction This is a follow up post of Multi-label classification to predict topic tags of technical articles from LinkedInfo.co. We will continute the same task by using BERT.\nFirstly we\u0026rsquo;ll just use the embeddings from BERT, and then feed them to the same classification model used in the last post, SVM with linear kenel. The reason of keep using SVM is that the size of the dataset is quite small.","tags":["BERT","Machine Learning","Multi-label classification","Text classification","LinkedInfo.co"],"title":"Using BERT to perform Topic Tag Prediction for Technical Articles","type":"post"},{"authors":[],"categories":[],"content":" Introduction This is a brief walk through of the Kaggle challenge IEEE-CIS Fraud Detection. The process in this post is not meant to compete the top solution by performing an extre feature engineering and a greedy search for the best model with hyper-parameters. This is just to walk through the problem and demonstrate a relatively good solution, by doing feature analysis and a few experiments with reference to other\u0026rsquo;s methods.\nThe problem of this challenge is to detect payment frauds by using the data of the transactions and identities. The performance of the prediction is evaluated on ROC AUC. The reason why this measure is suitable for this problem (rather than Precision-Recall) can refer to the discussion here.\nLook into the data The provided dataset is broken into two files named identity and transaction, which are joined by TransactionID (note that NOT all the transactions have corresponding identity information).\nTransaction Table  TransactionDT: timedelta from a given reference datetime (not an actual timestamp), the number of seconds in a day (60 * 60 * 24 = 86400) TransactionAMT: transaction payment amount in USD ProductCD: product code, the product for each transaction card1 - card6: payment card information, such as card type, card category, issue bank, country, etc. addr: address dist: distance P_ and (R__) emaildomain: purchaser and recipient email domain C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked. D1-D15: timedelta, such as days between previous transaction, etc. M1-M9: match, such as names on card and address, etc. Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.  Among these variables, categorical variables are: - ProductCD - card1 - card6 - addr1, addr2 - Pemaildomain Remaildomain - M1 - M9\nIdentity Table All the variable in this table are categorical: - DeviceType - DeviceInfo - id12 - id38\nA more detailed explanation of the data can be found in the reply of this discussion.\nNow let\u0026rsquo;s have a close look at the data.\nfrom IPython.core.interactiveshell import InteractiveShell InteractiveShell.ast_node_interactivity = \u0026quot;all\u0026quot;  import numpy as np import pandas as pd import plotly.express as px DATA_DIR = '/content/drive/My Drive/colab-data/fraud detect/data' tran_train = reduce_mem_usage(pd.read_csv(f'{DATA_DIR}/train_transaction.csv')) id_train = reduce_mem_usage(pd.read_csv(f'{DATA_DIR}/train_identity.csv')) tran_train.info() tran_train.head() id_train.info() id_train.head()  Mem. usage decreased to 542.35 Mb (69.4% reduction) Mem. usage decreased to 25.86 Mb (42.7% reduction) \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 590540 entries, 0 to 590539 Columns: 394 entries, TransactionID to V339 dtypes: float16(332), float32(44), int16(1), int32(2), int8(1), object(14) memory usage: 542.3+ MB   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    TransactionID isFraud TransactionDT TransactionAmt ProductCD card1 card2 card3 card4 card5 card6 addr1 addr2 dist1 dist2 P_emaildomain R_emaildomain C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 C13 C14 D1 D2 D3 D4 D5 D6 D7 D8 D9 ... V300 V301 V302 V303 V304 V305 V306 V307 V308 V309 V310 V311 V312 V313 V314 V315 V316 V317 V318 V319 V320 V321 V322 V323 V324 V325 V326 V327 V328 V329 V330 V331 V332 V333 V334 V335 V336 V337 V338 V339     0 2987000 0 86400 68.5 W 13926 NaN 150.0 discover 142.0 credit 315.0 87.0 19.0 NaN NaN NaN 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 2.0 0.0 1.0 1.0 14.0 NaN 13.0 NaN NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 1.0 0.0 117.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 117.0 0.0 0.0 0.0 0.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN   1 2987001 0 86401 29.0 W 2755 404.0 150.0 mastercard 102.0 credit 325.0 87.0 NaN NaN gmail.com NaN 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0 NaN NaN 0.0 NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN   2 2987002 0 86469 59.0 W 4663 490.0 150.0 visa 166.0 debit 330.0 87.0 287.0 NaN outlook.com NaN 1.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 NaN NaN 0.0 NaN NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN   3 2987003 0 86499 50.0 W 18132 567.0 150.0 mastercard 117.0 debit 476.0 87.0 NaN NaN yahoo.com NaN 2.0 5.0 0.0 0.0 0.0 4.0 0.0 0.0 1.0 0.0 1.0 0.0 25.0 1.0 112.0 112.0 0.0 94.0 0.0 NaN NaN NaN NaN ... 0.0 0.0 0.0 0.0 0.0 1.0 50.0 1758.0 925.0 0.0 354.0 0.0 135.0 0.0 0.0 0.0 50.0 1404.0 790.0 0.0 0.0 0.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN   4 2987004 0 86506 50.0 H 4497 514.0 150.0 mastercard 102.0 credit 420.0 87.0 NaN NaN gmail.com NaN 1.0 1.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0 1.0 0.0 1.0 1.0 0.0 NaN NaN NaN NaN NaN NaN NaN NaN ... 0.0 0.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0    5 rows × 394 columns\n \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 144233 entries, 0 to 144232 Data columns (total 41 columns): TransactionID 144233 non-null int32 id_01 144233 non-null float16 id_02 140872 non-null float32 id_03 66324 non-null float16 id_04 66324 non-null float16 id_05 136865 non-null float16 id_06 136865 non-null float16 id_07 5155 non-null float16 id_08 5155 non-null float16 id_09 74926 non-null float16 id_10 74926 non-null float16 id_11 140978 non-null float16 id_12 144233 non-null object id_13 127320 non-null float16 id_14 80044 non-null float16 id_15 140985 non-null object id_16 129340 non-null object id_17 139369 non-null float16 id_18 45113 non-null float16 id_19 139318 non-null float16 id_20 139261 non-null float16 id_21 5159 non-null float16 id_22 5169 non-null float16 id_23 5169 non-null object id_24 4747 non-null float16 id_25 5132 non-null float16 id_26 5163 non-null float16 id_27 5169 non-null object id_28 140978 non-null object id_29 140978 non-null object id_30 77565 non-null object id_31 140282 non-null object id_32 77586 non-null float16 id_33 73289 non-null object id_34 77805 non-null object id_35 140985 non-null object id_36 140985 non-null object id_37 140985 non-null object id_38 140985 non-null object DeviceType 140810 non-null object DeviceInfo 118666 non-null object dtypes: float16(22), float32(1), int32(1), object(17) memory usage: 25.9+ MB   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    TransactionID id_01 id_02 id_03 id_04 id_05 id_06 id_07 id_08 id_09 id_10 id_11 id_12 id_13 id_14 id_15 id_16 id_17 id_18 id_19 id_20 id_21 id_22 id_23 id_24 id_25 id_26 id_27 id_28 id_29 id_30 id_31 id_32 id_33 id_34 id_35 id_36 id_37 id_38 DeviceType DeviceInfo     0 2987004 0.0 70787.0 NaN NaN NaN NaN NaN NaN NaN NaN 100.0 NotFound NaN -480.0 New NotFound 166.0 NaN 542.0 144.0 NaN NaN NaN NaN NaN NaN NaN New NotFound Android 7.0 samsung browser 6.2 32.0 2220x1080 match_status:2 T F T T mobile SAMSUNG SM-G892A Build/NRD90M   1 2987008 -5.0 98945.0 NaN NaN 0.0 -5.0 NaN NaN NaN NaN 100.0 NotFound 49.0 -300.0 New NotFound 166.0 NaN 621.0 500.0 NaN NaN NaN NaN NaN NaN NaN New NotFound iOS 11.1.2 mobile safari 11.0 32.0 1334x750 match_status:1 T F F T mobile iOS Device   2 2987010 -5.0 191631.0 0.0 0.0 0.0 0.0 NaN NaN 0.0 0.0 100.0 NotFound 52.0 NaN Found Found 121.0 NaN 410.0 142.0 NaN NaN NaN NaN NaN NaN NaN Found Found NaN chrome 62.0 NaN NaN NaN F F T T desktop Windows   3 2987011 -5.0 221832.0 NaN NaN 0.0 -6.0 NaN NaN NaN NaN 100.0 NotFound 52.0 NaN New NotFound 225.0 NaN 176.0 507.0 NaN NaN NaN NaN NaN NaN NaN New NotFound NaN chrome 62.0 NaN NaN NaN F F T T desktop NaN   4 2987016 0.0 7460.0 0.0 0.0 1.0 0.0 NaN NaN 0.0 0.0 100.0 NotFound NaN -300.0 Found Found 166.0 15.0 529.0 575.0 NaN NaN NaN NaN NaN NaN NaN Found Found Mac OS X 10_11_6 chrome 62.0 24.0 1280x800 match_status:2 T F T T desktop MacOS     is_fraud = tran_train[['isFraud', 'TransactionID']].groupby('isFraud').count() is_fraud['ratio'] = is_fraud['TransactionID'] / is_fraud['TransactionID'].sum() fig_Y = px.bar(is_fraud, x=is_fraud.index, y='TransactionID', text='ratio', labels={'TransactionID': 'Number of transactions', 'x': 'is fraud'}) fig_Y.update_traces(texttemplate='%{text:.6p}')      if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});} window.PlotlyConfig = {MathJaxConfig: 'local'};   window.PLOTLYENV=window.PLOTLYENV || {}; if (document.getElementById(\"ac80ada0-952e-4492-9d69-bdab360ef9d6\")) { Plotly.newPlot( 'ac80ada0-952e-4492-9d69-bdab360ef9d6', [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"is fraud=%{x}\nNumber of transactions=%{y}\nratio=%{text}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"text\": [0.9650099908558268, 0.03499000914417313], \"textposition\": \"auto\", \"texttemplate\": \"%{text:.6p}\", \"type\": \"bar\", \"x\": [0, 1], \"xaxis\": \"x\", \"y\": [569877, 20663], \"yaxis\": \"y\"}], {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"is fraud\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Number of transactions\"}}}, {\"responsive\": true} ).then(function(){ var gd = document.getElementById('ac80ada0-952e-4492-9d69-bdab360ef9d6'); var x = new MutationObserver(function (mutations, observer) {{ var display = window.getComputedStyle(gd).display; if (!display || display === 'none') {{ console.log([gd, 'removed!']); Plotly.purge(gd); observer.disconnect(); }} }}); // Listen for the removal of the full notebook cells var notebookContainer = gd.closest('#notebook-container'); if (notebookContainer) {{ x.observe(notebookContainer, {childList: true}); }} // Listen for the clearing of the current output cell var outputEl = gd.closest('.output'); if (outputEl) {{ x.observe(outputEl, {childList: true}); }} }) };     Very imbalanced target varible Positives of isFraud is very low of 3.5% in the entire dataset. For this classification problem, it\u0026rsquo;s very important to have high true positive rate. That is, how good can the model identify the fraud cases from all the fraud cases. So recall is in a sense more important than precision in this problem. Macro average of recall would be a good side metric for this problem. Of cource, in reality we need to consider the belance between the cost of a few frauds and the cost of handling cases.\nIn addition, we need to put some effort on the sampling and train-val split method, to ensure that the minority class samples have enough impact to the model while training. Class weights of the model could be set to see if there\u0026rsquo;s difference in performance.\nCheck missing values Now let\u0026rsquo;s have a look at if there\u0026rsquo;s any missing value in the dataset. We can see from the table below that there\u0026rsquo;re quite a lot of missing values.\nIt\u0026rsquo;s hard to tell how we should handle with them before we look into each variable. Sometimes a missing value stands for something. It also depends on what kind of model we are going to use. We can leave them as missing value when using a tree model.\ndef missing_ratio_col(df): df_na = (df.isna().sum() / len(df)) * 100 if isinstance(df, pd.DataFrame): df_na = df_na.drop( df_na[df_na == 0].index).sort_values(ascending=False) missing_data = pd.DataFrame( {'Missing Ratio %': df_na}) else: missing_data = pd.DataFrame( {'Missing Ratio %': df_na}, index=[0]) return missing_data missing_ratio_col(tran_train) missing_ratio_col(id_train)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Missing Ratio %     dist2 93.628374   D7 93.409930   D13 89.509263   D14 89.469469   D12 89.041047   ... ...   V307 0.002032   V299 0.002032   V309 0.002032   V310 0.002032   V308 0.002032    374 rows × 1 columns\n  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Missing Ratio %     id_24 96.708798   id_25 96.441868   id_07 96.425922   id_08 96.425922   id_21 96.423149   id_26 96.420375   id_27 96.416215   id_23 96.416215   id_22 96.416215   id_18 68.722137   id_04 54.016071   id_03 54.016071   id_33 49.187079   id_10 48.052110   id_09 48.052110   id_30 46.222432   id_32 46.207872   id_34 46.056034   id_14 44.503685   DeviceInfo 17.726179   id_13 11.726165   id_16 10.325654   id_05 5.108401   id_06 5.108401   id_20 3.447200   id_19 3.407681   id_17 3.372321   id_31 2.739318   DeviceType 2.373243   id_02 2.330257   id_11 2.256765   id_28 2.256765   id_29 2.256765   id_35 2.251912   id_36 2.251912   id_15 2.251912   id_37 2.251912   id_38 2.251912     Detailed look at each variable There\u0026rsquo;re very good references of EDA and feature engineering on the dataset, so it\u0026rsquo;s meaningless to repeat here. Please check the list here if you\u0026rsquo;re interested: - Feature Engineering Techniques - EDA for Columns V and ID - XGB Fraud with Magic scores LB 0.96\nData transformation pipeline Based on the references and my own analysis, here we have a pipeline of the transformations to perform on the dataset. It can be adjusted for experimenting. Explanation of the transformations see in code comments.\nimport numpy as np import pandas as pd import plotly.express as px from sklearn.base import BaseEstimator, TransformerMixin from sklearn.pipeline import Pipeline from sklearn import preprocessing from sklearn.model_selection import train_test_split from typing import List, Callable DATA_DIR = '/content/drive/My Drive/colab-data/fraud detect/data' def reduce_mem_usage(df, verbose=True): numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64'] start_mem = df.memory_usage().sum() / 1024**2 for col in df.columns: col_type = df[col].dtypes if col_type in numerics: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == 'int': if c_min \u0026gt; np.iinfo(np.int8).min and c_max \u0026lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min \u0026gt; np.iinfo(np.int16).min and c_max \u0026lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min \u0026gt; np.iinfo(np.int32).min and c_max \u0026lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min \u0026gt; np.iinfo(np.int64).min and c_max \u0026lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if c_min \u0026gt; np.finfo(np.float16).min and c_max \u0026lt; np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min \u0026gt; np.finfo(np.float32).min and c_max \u0026lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) end_mem = df.memory_usage().sum() / 1024**2 if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem)) return df def load_df(test_set: bool = False, nrows: int = None, sample_ratio: float = None, reduce_mem: bool = True) -\u0026gt; pd.DataFrame: if test_set: tran = pd.read_csv(f'{DATA_DIR}/test_transaction.csv', nrows=nrows) ids = pd.read_csv(f'{DATA_DIR}/test_identity.csv', nrows=nrows) else: tran = pd.read_csv(f'{DATA_DIR}/train_transaction.csv', nrows=nrows) ids = pd.read_csv(f'{DATA_DIR}/train_identity.csv', nrows=nrows) if sample_ratio: size = int(len(tran) * sample_ratio) tran = tran.sample(n=size, random_state=RAND_STATE) ids = ids.sample(n=size, random_state=RAND_STATE) df = tran.merge(ids, how='left', on='TransactionID') if reduce_mem: reduce_mem_usage(df) return df def cat_cols(df: pd.DataFrame) -\u0026gt; List[str]: cols: List[str] = [] cols.append('ProductCD') cols_card = [c for c in df.columns if 'card' in c] cols.extend(cols_card) cols_addr = ['addr1', 'addr2'] cols.extend(cols_addr) cols_emaildomain = [c for c in df if 'email' in c] cols.extend(cols_emaildomain) cols_M = [c for c in df if c.startswith('M')] cols.extend(cols_M) cols.extend(['DeviceType', 'DeviceInfo']) cols_id = [c for c in df if c.startswith('id')] cols.extend(cols_id) return cols def num_cols(df: pd.DataFrame, target_col='isFraud') -\u0026gt; List[str]: cols_cat = cat_cols(df) cats = df[cols_cat] cols_num = list(set(df.columns) - set(cols_cat)) if target_col in cols_num: cols_num.remove(target_col) return cols_num def missing_ratio_col(df): df_na = (df.isna().sum() / len(df)) * 100 if isinstance(df, pd.DataFrame): df_na = df_na.drop( df_na[df_na == 0].index).sort_values(ascending=False) missing_data = pd.DataFrame({'Missing Ratio %': df_na}) else: missing_data = pd.DataFrame({'Missing Ratio %': df_na}, index=[0]) return missing_data class NumColsNaMedianFiller(TransformerMixin, BaseEstimator): def fit(self, X, y=None): return self def transform(self, df): cols_cat = cat_cols(df) cols_num = list(set(df.columns) - set(cols_cat)) for col in cols_num: median = df[col].median() df[col].fillna(median, inplace=True) return df class NumColsNegFiller(TransformerMixin, BaseEstimator): def fit(self, X, y=None): return self def transform(self, df): cols_num = num_cols(df) for col in cols_num: df[col].fillna(-999, inplace=True) return df class NumColsRatioDropper(TransformerMixin, BaseEstimator): def __init__(self, ratio: float = 0.5): self.ratio = ratio def fit(self, X, y=None): return self def transform(self, df): # print(X[self.attribute_names].columns) cols_cat = cat_cols(df) cats = df[cols_cat] # nums = df.drop(columns=cols_cat) # cols_num = df[~df[cols_cat]].columns cols_num = list(set(df.columns) - set(cols_cat)) nums = df[cols_num] ratio = self.ratio * 100 missings = missing_ratio_col(nums) # print(missings) inds = missings[missings['Missing Ratio %'] \u0026gt; ratio].index df = df.drop(columns=inds) return df class ColsDropper(TransformerMixin, BaseEstimator): def __init__(self, cols: List[str]): self.cols = cols def fit(self, X, y=None): return self def transform(self, df): return df.drop(columns=self.cols) class DataFrameSelector(TransformerMixin, BaseEstimator): def __init__(self, col_names): self.attribute_names = col_names def fit(self, X, y=None): return self def transform(self, X): print(X[self.attribute_names].columns) return X[self.attribute_names].values class DummyEncoder(TransformerMixin, BaseEstimator): def fit(self, X, y=None): return self def transform(self, df): cols_cat = cat_cols(df) cats = df[cols_cat] noncats = df.drop(columns=cols_cat) cats = cats.astype('category') cats_enc = pd.get_dummies(cats, prefix=cols_cat, dummy_na=True) return noncats.join(cats_enc) # Label encoding is OK when we're using tree models class MyLabelEncoder(TransformerMixin, BaseEstimator): def fit(self, X, y=None): return self def transform(self, df): cols_cat = cat_cols(df) for col in cols_cat: df[col] = df[col].astype('category').cat.add_categories( 'missing').fillna('missing') le = preprocessing.LabelEncoder() # TODO add test set together to encoding # le.fit(df[col].astype(str).values) df[col] = le.fit_transform(df[col].astype(str).values) return df class FrequencyEncoder(TransformerMixin, BaseEstimator): def __init__(self, cols): self.cols = cols def fit(self, X, y=None): return self def transform(self, df): for col in self.cols: vc = df[col].value_counts(dropna=True, normalize=True).to_dict() vc[-1] = -1 nm = col + '_FE' df[nm] = df[col].map(vc) df[nm] = df[nm].astype('float32') return df class CombineEncoder(TransformerMixin, BaseEstimator): def __init__(self, cols_pairs: List[List[str]]): self.cols_pairs = cols_pairs def fit(self, X, y=None): return self def transform(self, df): for pair in self.cols_pairs: col1 = pair[0] col2 = pair[1] nm = col1 + '_' + col2 df[nm] = df[col1].astype(str) + '_' + df[col2].astype(str) df[nm] = df[nm].astype('category') # print(nm, ', ', end='') return df class AggregateEncoder(TransformerMixin, BaseEstimator): def __init__(self, main_cols: List[str], uids: List[str], aggr_types: List[str], fill_na: bool = True, use_na: bool = False): self.main_cols = main_cols self.uids = uids self.aggr_types = aggr_types self.use_na = use_na self.fill_na = fill_na def fit(self, X, y=None): return self def transform(self, df): for col in self.main_cols: for uid in self.uids: for aggr_type in self.aggr_types: col_new = f'{col}_{uid}_{aggr_type}' tmp = df.groupby([uid])[col].agg([aggr_type]).reset_index().rename( columns={aggr_type: col_new}) tmp.index = list(tmp[uid]) tmp = tmp[col_new].to_dict() df[col_new] = df[uid].map(tmp).astype('float32') if self.fill_na: df[col_new].fillna(-1, inplace=True) return df  from sklearn.pipeline import Pipeline pipe = Pipeline(steps=[ # Based on feature engineering from # https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600#Encoding-Functions ('combine_enc', CombineEncoder( [['card1', 'addr1'], ['card1_addr1', 'P_emaildomain']])), ('freq_enc', FrequencyEncoder( ['addr1', 'card1', 'card2', 'card3', 'P_emaildomain'])), ('aggr_enc', AggregateEncoder(['TransactionAmt', 'D9', 'D11'], [ 'card1', 'card1_addr1', 'card1_addr1_P_emaildomain'], ['mean', 'std'])), # Drop columns that have certain high ratio of missing values, and then fill # in values e.g. median value. May not be used if using a tree model. ('reduce_missing', NumColsRatioDropper(0.5)), ('fillna_median', NumColsNaMedianFiller()), # Drop some columns that will not be used ('drop_cols_basic', ColsDropper(['TransactionID', 'TransactionDT', 'D6', 'D7', 'D8', 'D9', 'D12', 'D13', 'D14', 'C3', 'M5', 'id_08', 'id_33', 'card4', 'id_07', 'id_14', 'id_21', 'id_30', 'id_32', 'id_34'])), # Drop some columns based on feature importance got from a model. ('drop_cols_feat_importance', ColsDropper( ['v107', 'v117', 'v119', 'v120', 'v27', 'v28', 'v305'])), ('fillna_negative', NumColsNegFiller()), # Encode categorical variables. Depending on the kind of model we use, # we can choose between label encoding and onehot encoding. # ('onehot_enc', DummyEncoder()), ('label_enc', MyLabelEncoder()), ])  Split dataset And as we want to predict future payment fraud based on the past data, so we should not shuffle the dataset when split training and testing sets, but just time-based split.\nAs this is a imbalanced dataset with 1 class of the target variable have only about 3.5%, so we may want to try sampling methods like over-sampling or SMOTE sampling on the training dataset.\nRAND_STATE = 20200119 def data_split_v1(X: pd.DataFrame, y: pd.Series): X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.25, shuffle=False, random_state=RAND_STATE) return X_train, X_val, y_train, y_val def data_split_oversample_v1(X: pd.DataFrame, y: pd.Series): from imblearn.over_sampling import RandomOverSampler X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.25, shuffle=False, random_state=RAND_STATE) ros = RandomOverSampler(random_state=RAND_STATE) X_train, y_train = ros.fit_resample(X_train, y_train) return X_train, X_val, y_train, y_val def data_split_smoteenn_v1(X: pd.DataFrame, y: pd.Series): from imblearn.combine import SMOTEENN X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.2, shuffle=False, random_state=RAND_STATE) ros = SMOTEENN(random_state=RAND_STATE) X_train, y_train = ros.fit_resample(X_train, y_train) return X_train, X_val, y_train, y_val  Experiments Now let\u0026rsquo;s start play with experimenting with simple models like Logistic Regression, or complex models like Gradient Boosting.\nHere below is a scaffold for performing experiments.\nimport os from datetime import datetime import json import pprint from sklearn import metrics from sklearn.pipeline import Pipeline from typing import List, Callable EXP_DIR = 'exp' class Experiment: def __init__(self, df_nrows: int = None, transform_pipe: Pipeline = None, data_split: Callable = None, model=None, model_class=None, model_param: dict = None): self.df_nrows = df_nrows self.pipe = transform_pipe if data_split is None: self.data_split = data_split_v1 else: self.data_split = data_split if model_class: self.model = model_class(**model_param) else: self.model = model self.model_param = model_param def transform(self, X): return self.pipe.fit_transform(X) def run(self, df_train: pd.DataFrame, save_exp: bool = True) -\u0026gt; float: # self.df = load_df(nrows=self.df_nrows) y = df_train['isFraud'] X = df_train.drop(columns=['isFraud']) X = self.transform(X) X_train, X_val, Y_train, Y_val = self.data_split(X, y) # del X # gc.collect() self.model.fit(X_train, Y_train) Y_pred = self.model.predict(X_val) self.last_roc_auc = metrics.roc_auc_score(Y_val, Y_pred) if save_exp: self.save_result() return self.last_roc_auc def save_result(self, feature_importance:bool=False): save_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S') result = {} result['roc_auc'] = self.last_roc_auc result['transform'] = list(self.pipe.named_steps.keys()) result['model'] = self.model.__class__.__name__ result['model_param'] = self.model_param result['data_split'] = self.data_split.__name__ result['num_sample_rows'] = self.df_nrows result['save_time'] = save_time if feature_importance: if hasattr(self.model, 'feature_importances_'): result['feature_importances_'] = dict( zip(self.X.columns, self.model.feature_importances_.tolist())) if hasattr(self.model, 'feature_importance'): result['feature_importances_'] = dict( zip(self.df.columns, self.model.feature_importance.tolist())) pprint.pprint(result, indent=4) if not os.path.exists(EXP_DIR): os.makedirs(EXP_DIR) with open(f'{EXP_DIR}/exp_{save_time}_{self.last_roc_auc:.4f}.json', 'w') as f: json.dump(result, f, indent=4)  import gc del tran_train, id_train gc.collect() df_train = load_df()  df_train = load_df()  Mem. usage decreased to 650.48 Mb (66.8% reduction)  Logistic Regression as baseline def exp1(): from sklearn.linear_model import LogisticRegression pipe = Pipeline(steps=[ ('combine_enc', CombineEncoder( [['card1', 'addr1'], ['card1_addr1', 'P_emaildomain']])), ('freq_enc', FrequencyEncoder( ['addr1', 'card1', 'card2', 'card3', 'P_emaildomain'])), ('aggr_enc', AggregateEncoder(['TransactionAmt', 'D9', 'D11'], [ 'card1', 'card1_addr1', 'card1_addr1_P_emaildomain'], ['mean', 'std'])), ('reduce_missing', NumColsRatioDropper(0.3)), ('fillna_median', NumColsNaMedianFiller()), ('drop_cols_basic', ColsDropper(['TransactionID', 'TransactionDT', 'C3', 'M5', 'id_08', 'id_33', 'card4', 'id_07', 'id_14', 'id_21', 'id_30', 'id_32', 'id_34'])), # Though onehot encoding is more appropriate for logistic regression, we # don't have enough memory to encode that many variables. So we take a # step back using label encoding. # ('onehot_enc', DummyEncoder()), ('label_enc', MyLabelEncoder()), ]) exp = Experiment(transform_pipe=pipe, data_split=data_split_v1, model_class=LogisticRegression, # just use the default hyper paramenters model_param={}, ) exp.run(df_train=df_train) exp1()  /usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression { 'data_split': 'data_split_v1', 'model': 'LogisticRegression', 'model_param': {}, 'num_sample_rows': None, 'roc_auc': 0.4956463187232307, 'save_time': '2020-03-26_20-27-08', 'transform': [ 'combine_enc', 'freq_enc', 'aggr_enc', 'reduce_missing', 'fillna_median', 'drop_cols_basic', 'label_enc']}  Gradient Boosting with LightGBM Now let\u0026rsquo;s try a Gradient Boosting tree model using the LightGBM implementation, and tune a little on the hyper-parameters to make it a more complex model.\nimport lightgbm as lgb class LgbmWrapper: def __init__(self, **param): self.param = param self.trained = None def fit(self, X_train, y_train): train = lgb.Dataset(X_train, label=y_train) self.trained = lgb.train(self.param, train) self.feature_importances_ = self.trained.feature_importance() return self.trained def predict(self, X_val): return self.trained.predict(X_val, num_iteration=self.trained.best_iteration) def exp2(): pipe = Pipeline(steps=[ # Based on feature engineering from # https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600#Encoding-Functions ('combine_enc', CombineEncoder( [['card1', 'addr1'], ['card1_addr1', 'P_emaildomain']])), ('freq_enc', FrequencyEncoder( ['addr1', 'card1', 'card2', 'card3', 'P_emaildomain'])), ('aggr_enc', AggregateEncoder(['TransactionAmt', 'D9', 'D11'], [ 'card1', 'card1_addr1', 'card1_addr1_P_emaildomain'], ['mean', 'std'])), # Drop some columns that will not be used ('drop_cols_basic', ColsDropper(['TransactionID', 'TransactionDT', 'D6', 'D7', 'D8', 'D9', 'D12', 'D13', 'D14', 'C3', 'M5', 'id_08', 'id_33', 'card4', 'id_07', 'id_14', 'id_21', 'id_30', 'id_32', 'id_34'])), # Drop some columns based on feature importance got from a model. # ('drop_cols_feat_importance', ColsDropper( # ['v107', 'v117', 'v119', 'v120', 'v27', 'v28', 'v305'])), ('fillna_negative', NumColsNegFiller()), # Label encoding used for tree models. # ('onehot_enc', DummyEncoder()), ('label_enc', MyLabelEncoder()), ]) param_lgbm = {'objective': 'binary', 'boosting_type': 'gbdt', 'metric': 'auc', 'learning_rate': 0.01, 'num_leaves': 2**8, 'max_depth': -1, 'tree_learner': 'serial', 'colsample_bytree': 0.7, 'subsample_freq': 1, 'subsample': 0.7, 'n_estimators': 10000, # 'n_estimators': 80000, 'max_bin': 255, 'n_jobs': -1, 'verbose': -1, 'seed': RAND_STATE, # 'early_stopping_rounds': 100, } exp = Experiment(transform_pipe=pipe, data_split=data_split_v1, model_class=LgbmWrapper, model_param=param_lgbm, ) exp.run(df_train=df_train) exp2()  /usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument { 'data_split': 'data_split_v1', 'model': 'LgbmWrapper', 'model_param': { 'boosting_type': 'gbdt', 'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_bin': 255, 'max_depth': -1, 'metric': 'auc', 'n_estimators': 10000, 'n_jobs': -1, 'num_leaves': 256, 'objective': 'binary', 'seed': 20200119, 'subsample': 0.7, 'subsample_freq': 1, 'tree_learner': 'serial', 'verbose': -1}, 'num_sample_rows': None, 'roc_auc': 0.919589853747652, 'save_time': '2020-03-27_09-55-43', 'transform': [ 'combine_enc', 'freq_enc', 'aggr_enc', 'drop_cols_basic', 'fillna_negative', 'label_enc']}  So we got local validation ROC AUC of about 0.9196, this is a looks OK score.\nThis model\u0026rsquo;s prediction on the test dataset got 0.9398 on publica leader board, and 0.9058 on private leader board. These scores have a somehow big gap to the top scores, but still good enough as there\u0026rsquo;re potentially many ways for improvement. For example, more different ways of transformations and engineering could be performed on the features, try model implementation like CatBoost and XGB, and search for better hyper-parameters. But it assumes you have plenty of computation resource and time.\n","date":1581325631,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581325631,"objectID":"8d9d81b1d3a5878bcd95afccd9acfdb8","permalink":"https://pcx.linkedinfo.co/post/fraud-detection/","publishdate":"2020-02-10T10:07:11+01:00","relpermalink":"/post/fraud-detection/","section":"post","summary":"Introduction This is a brief walk through of the Kaggle challenge IEEE-CIS Fraud Detection. The process in this post is not meant to compete the top solution by performing an extre feature engineering and a greedy search for the best model with hyper-parameters. This is just to walk through the problem and demonstrate a relatively good solution, by doing feature analysis and a few experiments with reference to other\u0026rsquo;s methods.","tags":["Machine Learning","Fraud Detection"],"title":"A Walk Through of the IEEE-CIS Fraud Detection Challenge","type":"post"},{"authors":[],"categories":[],"content":"A skin lesion classifier that uses a deep neural network trained on the HAM10000 dataset. An implementation of the ISIC challenge 2018 task 3.\n","date":1581101486,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581101486,"objectID":"8f731303727c161cda9ba8e8f449fe58","permalink":"https://pcx.linkedinfo.co/project/skin-lesion-classifier/","publishdate":"2020-02-07T19:51:26+01:00","relpermalink":"/project/skin-lesion-classifier/","section":"project","summary":"A skin lesion classifier that uses a deep neural network trained on the HAM10000 dataset. An implementation of the ISIC challenge 2018 task 3.","tags":["Image Classification","Deep Learning","Machine Learning"],"title":"Skin Lesion Classifier","type":"project"},{"authors":[],"categories":[],"content":"A topic tag prediction service for technical articles. The model uses a pre-trained BERT and fine-tuned on the dataset of LinkedInfo.co.\n","date":1578162146,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578162146,"objectID":"66ad7102777a5d710213f4883bb122dc","permalink":"https://pcx.linkedinfo.co/project/topic-tag-predictor/","publishdate":"2020-01-04T19:22:26+01:00","relpermalink":"/project/topic-tag-predictor/","section":"project","summary":"A topic tag prediction service for technical articles. The model uses a pre-trained BERT and fine-tuned on the dataset of LinkedInfo.co.","tags":["Text Classification","BERT","Deep Learning","Machine Learning"],"title":"Topic Tag Predictor","type":"project"},{"authors":[],"categories":[],"content":" Introduction In this post we will show how to do skin lesion image classification with deep neural networks. It is an image classifier trained on the HAM10000 dataset, the same problem in the International Skin Imaging Collaboration (ISIC) 2018 challenge task3.\nThe solution in this post is mainly based on some web posts and methods from the ISIC2018 leadboard.\nThe classification neural network model is tested with pretrained ResNet and DenseNet and implemented with PyTOrch. The model with the highest mean of recalls (0.9369 on 20% test set) is a ensemble of ImageNet pretrained and fine-tuned DenseNet161 + ResNet152.\n# Confusion matrix of the mdoel with the best recall from IPython.display import Image Image('test_results/ensemble_dense161_6_res152_4_lesion/confusion_matrix.png', width=900)  Here below we go through the process how I worked on this problem.\nA look at the data Before diving into the models and metrics, we need to firstly have a look at the dataset.\nimport pandas as pd df = pd.read_csv('data/HAM10000_metadata.csv', index_col='image_id') df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    lesion_id dx dx_type age sex localization   image_id           ISIC_0027419 HAM_0000118 bkl histo 80.0 male scalp   ISIC_0025030 HAM_0000118 bkl histo 80.0 male scalp   ISIC_0026769 HAM_0002730 bkl histo 80.0 male scalp   ISIC_0025661 HAM_0002730 bkl histo 80.0 male scalp   ISIC_0031633 HAM_0001466 bkl histo 75.0 male ear     import seaborn as sns sns.countplot(df['dx'])  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x126f39eb8\u0026gt;  pd.DataFrame({'counts':df['dx'].value_counts(), 'percent': df['dx'].value_counts() / len(df)})   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    counts percent     nv 6705 0.669496   mel 1113 0.111133   bkl 1099 0.109735   bcc 514 0.051323   akiec 327 0.032651   vasc 142 0.014179   df 115 0.011483     We can see that the samples for each class are very imbalanced. The class melanocytic nevi (nv) has about 67% of the dataset. The most minority class has only about 1% of the dataset.\nWhen we organize the rows by lesion_id, we can see that many lesions have more than 1 images. The description of ham10000 says the dataset includes lesions with multiple images, which can be tracked by the lesion_id column.\ndfr=df.reset_index(col_level='lesion_id').set_index(['lesion_id','image_id']) dfr.head(10)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }     dx dx_type age sex localization   lesion_id image_id          HAM_0000118 ISIC_0027419 bkl histo 80.0 male scalp   ISIC_0025030 bkl histo 80.0 male scalp   HAM_0002730 ISIC_0026769 bkl histo 80.0 male scalp   ISIC_0025661 bkl histo 80.0 male scalp   HAM_0001466 ISIC_0031633 bkl histo 75.0 male ear   ISIC_0027850 bkl histo 75.0 male ear   HAM_0002761 ISIC_0029176 bkl histo 60.0 male face   ISIC_0029068 bkl histo 60.0 male face   HAM_0005132 ISIC_0025837 bkl histo 70.0 female back   ISIC_0025209 bkl histo 70.0 female back     import matplotlib.pyplot as plt import matplotlib.image as mpimg from matplotlib import rcParams %matplotlib inline # figure size in inches optional rcParams['figure.figsize'] = 10 ,5 def plot_by_lesion(): grouped = df.groupby(['lesion_id']) lesions = [] for i, lesion in enumerate(grouped): cnt = len(lesion[1].index) if cnt \u0026gt; 1: fig, axes = plt.subplots(1, cnt) for ax, name in zip(axes, lesion[1].index): img = mpimg.imread(f'data/{name}.jpg') ax.imshow(img) if i \u0026gt; 4: break plot_by_lesion()  We can seee that the multiple images capture the same lesion with differences in color, scaling, orientation.\nNow let\u0026rsquo;s count the images by lesion_id.\ndf['lesion_id'].nunique()  7470  cnt = df.groupby('dx')['lesion_id'].nunique() per = df.groupby('dx')['lesion_id'].nunique().div(df['lesion_id'].nunique()) pd.DataFrame({'counts':cnt, 'percent': per})   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    counts percent   dx       akiec 228 0.030522   bcc 327 0.043775   bkl 727 0.097323   df 73 0.009772   mel 614 0.082195   nv 5403 0.723293   vasc 98 0.013119     Now we see there\u0026rsquo;re 7470 unique lesions there, and the class imbalance got evern worse when we group them by lesion_id. We need to keep this in mind for the later actions like sampling and choosing evaluation metrics.\nMetrics For classfication problem, the commonly used metrics are Precision/Recall/F-measures, ROC_AUC, Accuracy Score (ACC) and so on. But for this imbalanced dataset, we need to think more on the choice of metrics.\nIn the ISIC 2018 challenge report, it mentioned that \u0026gt; Use of balanced accuracy is critical to select the best unbiased classifier, rather than one that overfits to arbitrary dataset prevalence, as is the case with accuracy.\nBased on the description, it\u0026rsquo;s the class-wise mean of recall. The recall_score(average='macro') in scikit-learn just calculates this score:\n$$ \\frac{1}{|L|} \\sum_{l \\in L} R \\left( y_{l}, \\hat{y}_{l} \\right) $$\nThe more details of the Balanced Multiclass Accuracy can refer to\n description from the tooltip on the ISIC 2018 leaderboard webpage. an explanation on the ISIC discussion forum. description on ISIC 2019 introduction.   So we\u0026rsquo;ll use the balanced accuracy (BACC) or mean of recalls of the 7 classes as the main metric for this assignment.\nThe mean reason is that this is a very imbalanced dataset, it is a big problem we need to handel carefully. For this multiclass classification with very imbalanced dataset:\n It\u0026rsquo;s important for the model to have good performance on all the classes, other than a few majority classes. The different classes have equal importance. Mean recall is good because it counts the model\u0026rsquo;s classification performance on all the classes equally, no matter how many samples belong to a class. So global accuracy score, micro average of recalls or so are not good metrics to measure the performance in this case.  And this is a medical diagnosis, it\u0026rsquo;s important to have a high true positive rate (to minimize the false negatives), so it\u0026rsquo;s better to focus more on recall over precision.\nBut we\u0026rsquo;ll also use other metrics togher to have more insights. A confusion matrix plot is also a good way to present how does the model performed for each class. One of the metrics that is also good for a imbalanced classification is Matthews correlation coefficient (MCC), it ranges between −1 to 1\n 1 score shows a perfect prediction 0 equals to the random prediction −1 indicates total disagreement between predicted scores and true labels’ values   $$m c c=\\frac{t p \\cdot t n-f p \\cdot f n}{\\sqrt{(t p+f p) \\cdot(t p+f n) \\cdot(t n+f p) \\cdot(t n+f n)}} $$\nPreprocess dataset Sampling Since the dataset is very imbalanced, so even though we could use the mean recall and loss function with class weights, it would be still troublesome to train the model for the under-represented minority classes. And the under-represented classes are likely to be missing or very few samples in a subsample or split, especially when the fraction is small. So we need to do something for the train-validation-test set sampling and split.\n2 methods were applied to deal with the problem, with the assumption that new data follow a close imbalanced distribution as the labelled dataset.\n Subsampling based on the classes distribution of all the samples. So a small fraction train, validation or set will still have the same distribution of different classes. Oversampling training set for the under-represented classess (with random transformations) to equalize the distribution. Since the dataset is considered small so we will use oversampling on the minority classes other than undersampling on the majority classes.  For simplicity, I\u0026rsquo;ll just use the first image of each lesion_id. The code snippet below processes the dataset with oversampling. The parameter over_rate controls how much to over sample the minority classes.\nimport functools exclude_set = [] weighted = True imbalance_eq = True remove_dup_img = True over_rate = 4 train_fraction = 0.8 val_fraction = 0.2 meta_data = pd.read_csv('data/HAM10000_metadata.csv', index_col='image_id') # for reproducibility, just keep 1st image of each lesion_id if remove_dup_img: lesion_ids = [] for index, row in meta_data.iterrows(): if row['lesion_id'] not in lesion_ids: lesion_ids.append(row['lesion_id']) else: meta_data = meta_data.drop(index=index) if len(exclude_set) \u0026gt; 0: meta_data = meta_data.drop(index=exclude_set) image_ids = meta_data.index.tolist() num_images = len(image_ids) num_train_ids = int(num_images * train_fraction) num_val_ids = int(num_images * val_fraction) # sampling based on the distribution of classees if weighted: size_total = num_train_ids + num_val_ids df_c = meta_data['dx'].astype('category').value_counts() weights = df_c / len(meta_data) def sampling(df, replace=False, total=size_total): return df.sample(n=int(weights[df.name] * total), replace=replace) train_val = meta_data.groupby('dx', as_index=False).apply( sampling).reset_index(0, drop=True) train_sampling = functools.partial(sampling, total=num_train_ids) train = train_val.groupby('dx', as_index=False).apply( train_sampling).reset_index(0, drop=True) val = train_val.drop(index=train.index) if imbalance_eq: bal_rate = 1 / weights / over_rate for k, v in bal_rate.to_dict().items(): if v \u0026gt; 2: train = train.append( [train.loc[train['dx'] == k, :]] * int(v), ignore_index=False) sns.countplot(train['dx'])  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x12738aa58\u0026gt;  Fractions of the entire set is splitted as below:\n training 0.64 (0.8*0.8) validation 0.16 (0.8*0.2) testing 0.2 (the same set for all the experiments)  Loss function Cross Entropy Loss function will be used. As for this multiclass classification problem, I don\u0026rsquo;t have a good reason to use other loss functions over cross entropy.\nOn whether or not the loss criterion should also be weighted according to the imbalanced classes, I think it needs to be based on how we sample the training and validation set.\n If we sample the subsets as the sample distribution as the entire dataset, then we could use the weighted loss criterion so that it gives penalty to the majority classes. If we are to perform some sampling method like oversampling, it already gives some penalty to the majority classes, then I think we should use a loss criterion without weighted.   Data transformation Some data transformations were performed to all the input images. It is performed according to the description in torchvision documentation for pre-trained models (as we will use these pre-trained models). It says Height and Width are expected to be at least 224, so we will resize all the input images into 224x224 to save some computation. We also normalize the iamges by the same mean and std as mentioned in the documentation.\n# Transformation for validation and testing sets transforms.Compose([ transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])  As we oversampled minority classes in the training set, we should perform some random transformations, such as random horizontal-vertical flip, rotation and color jitter (saturation not used since I thought it might affect the preciseness of lesion area).\n# Transformation for training set transforms_train = transforms.Compose([ transforms.Resize(300), transforms.RandomHorizontalFlip(0.5), transforms.RandomVerticalFlip(0.5), transforms.RandomRotation(20), transforms.RandomResizedCrop(224), transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])  Models When choosing a neural netowrk model for a certain task, we need to consider about several factors, e.g., the performance (accuracy) of a model, the appropriation of a model architecture for the task (in terms of pretrained model, the pretrained dataset should be more similar to the new task\u0026rsquo;s dataset), and computation efficiency.\nFor this assignment, we need to make a tradeoff between the performance and computation efficiency since I don\u0026rsquo;t have much of the computation resources.\nPre-trained models By glancing through the methods in the ISIC2018 leadboard, most of the top methods used ensemble models of pre-trained models such as ResNet, Densenet, Inception and so on. There\u0026rsquo;re also methods used a single pre-trained model achieved a high rank.\nAnd also as quoted from cs231n notes states that it\u0026rsquo;s now rarely people will train a network from scratch due to insufficient data size and expensiveness of training. It\u0026rsquo;s common to start from a model pre-trained on a very large dataset and use it as an initialization or a fixed feature extractor for a new task.\nTherefore, we\u0026rsquo;ll start from a pre-trained model. As suggested in this post, ResNet-34 would be a good choice to start. So the initial plan was use the pre-trained ResNet model as a fixed feature extractor to see how it performs, and then try to \u0026ldquo;fine-tune\u0026rdquo; the weights of some layers.\nHowever, after a few preliminary short-run experiments, I found it\u0026rsquo;s slow to train and the metrics didn\u0026rsquo;t show a potential improvement.\nIn Keres documentation there\u0026rsquo;s a table lists some stats like accuracy and number of parameters for some widely used models. As a tradeoff between accuracy and trainability (number of parameters), I started to focus more on the DenseNet161 and ResNet152.\nAfter some preliminary experiments on using the networks as a feature extractor, which didn\u0026rsquo;t give a encouraging result, I decided to fine-tune the whole network.\nExperiments The experiments were early stopped when I think it might stop improving. Though it will possibly improve as the training continues, the time is precious.\nTraining and validation DenseNet161 For the DenseNet161, the best validation mean of recalls is about 0.6845.\nimport torch best_dense161_lesion = torch.load( 'experiments/dense161_eq3_exclutest_lesion_v1/model_best.pth.tar', map_location=torch.device('cpu')) recall_val_dense161_lesion = best_dense161_lesion['metrics']['recall']  recall_val_dense161_lesion  0.684509306993473  Image('experiments/dense161_eq3_exclutest_lesion_v1/confusion_matrix.png', width=900)  ResNet152 For the ResNet152, the best validation mean of recalls is about 0.7202.\nbest_res152_lesion = torch.load( 'experiments/res152_eq3_exclutest_lesion_v1/model_best.pth.tar', map_location=torch.device('cpu')) recall_val_res152_lesion = best_res152_lesion['metrics']['recall']  recall_val_res152_lesion  0.7202260074093291  Image('experiments/res152_eq3_exclutest_lesion_v1/recall.png', width=700)  import numpy as np import matplotlib.pyplot as plt def plot_metric(train_loss, test_loss, name, plot_type='loss'): epochs = range(len(train_losses)) f = plt.figure() plt.title(f\u0026quot;{name} {plot_type} plot\u0026quot;) plt.xlabel(\u0026quot;epoch\u0026quot;) plt.ylabel(f\u0026quot;{plot_type}\u0026quot;) plt.grid(True) plt.plot(epochs, train_loss, 'b', marker='o', label=f'train {plot_type}') plt.plot(epochs, test_loss, 'r', marker='o', label=f'val {plot_type}') plt.legend() train_losses, test_losses = np.load( 'experiments/res152_eq3_exclutest_lesion_v1/final_results.npy') plot_metric(train_losses, test_losses, 'ResNet152_lesion', 'loss')  # ResNet152 val Image('experiments/res152_eq3_exclutest_lesion_v1/confusion_matrix.png', width=900)  As we can see from the results, it\u0026rsquo;s not a satisfactory performance for both the DenseNet161 and ResNet152 as they have only around 0.7 mean of recalls. No matter what, let\u0026rsquo;s have a look at how they perform on the test set.\nMetrics on the test set import os import json import numpy as np import pandas as pd from sklearn import metrics test_results_path = 'test_results' # model_ids =['dense161','res101','res152'] result_paths = [d for d in os.listdir( test_results_path) if not d.startswith('.')] result_paths = [d for d in result_paths if 'lesion' in d] # print(result_paths) model_metrics = {} for i in result_paths: fp = os.path.join(test_results_path, i, 'metrics_results.json') y_true = np.load(os.path.join(test_results_path, i, 'val_true.npy')) y_pred = np.load(os.path.join(test_results_path, i, 'val_pred.npy')) with open(fp) as f: rec = json.load(f) rec['f1'] = metrics.f1_score(y_true, y_pred, average='macro') rec['mcc'] = metrics.matthews_corrcoef(y_true, y_pred) model_metrics[i] = rec df_results_lesion = pd.read_json(json.dumps(model_metrics), orient='index').drop( columns=['bacc']).sort_values(by='recall', ascending=False) df_results_lesion['acc'] = df_results_lesion['acc'] / 100 df_results_lesion = df_results_lesion[['recall', 'prec', 'f1', 'mcc', 'acc']] df_results_lesion.columns = ['Recall', 'Precision', 'F1', 'MCC', 'ACC']  df_results_lesion.loc[['dense161_lesion','res152_lesion']].round(4)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Recall Precision F1 MCC ACC     dense161_lesion 0.9105 0.8085 0.8504 0.8210 0.9111   res152_lesion 0.8594 0.7542 0.7971 0.7072 0.8465     The surperising result shows a much higher mean of recalls for both of the models on the test dataset, from around 0.7 to 0.9105 and 0.8594.\nI also tested ensembles of the the trained models with different weights on each (though without grid search).\nBesides pick the model when with the highest mean of recalls, I also used the DenseNet161 model with the highest MCC score during validation.\nThe results are also surprisingly good.\ndf_results_lesion.round(4)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Recall Precision F1 MCC ACC     ensemble_dense161_6_res152_4_lesion 0.9369 0.7939 0.8558 0.8067 0.9018   ensemble_dense161_4_res152_6_lesion 0.9206 0.8132 0.8610 0.7808 0.8884   dense161_lesion 0.9105 0.8085 0.8504 0.8210 0.9111   dense161_lesion_mcc 0.9095 0.8540 0.8789 0.8236 0.9144   ensemble_dense161_res152_lesion 0.9055 0.8052 0.8491 0.7931 0.8960   res152_lesion 0.8594 0.7542 0.7971 0.7072 0.8465     Image('test_results/dense161_lesion/confusion_matrix.png', width=900)  Image('test_results/dense161_lesion_mcc/confusion_matrix.png', width=900)  Image('test_results/ensemble_dense161_6_res152_4_lesion/confusion_matrix.png', width=900)  Image('test_results/res152_lesion/confusion_matrix.png', width=900)  However, I still doubt why the test metrics are much higher than when in validation (even higher than those deeply hacked top ranks in the ISIC 2018 challenge).\nI\u0026rsquo;ve thought about and also discussed with others on the possible flaws in my solution. However, I couldn\u0026rsquo;t find a likely problem caused the very high mean of recalls on the tes tset. There\u0026rsquo;s no leakage of information from training set to test set, as I groupped and splitted the datasets according to the lesion_id.\nYou\u0026rsquo;re very welcomed to contact me if you have any idea.\nDiscussion of limitations and possible improvements  Should use all the images for each lesion_id Could\u0026rsquo;ve train longer Sacrifice majority classes for the performance on the minority classes, nv could be better as the given data The experiments were not well controled, no comparison on the performance when a single variable changed, such as  the use of oversampling, fine-tune to get a balance different ways of training set transformations \u0026hellip;  Fine-tune hyper parameters Look into other variables in meta_data, if could be combined to improve the classification performance Input images of only lesion region, as semantic segmentation (the task 1 of ISIC 2018) Color constancy (mentioned in leadboard high-rank manuscript) Focal loss function (1 report mentioned smaller variance on accuracy) Get extra data Exploration of other models  References not mentioned yet  Deep Learning for Diagnosis of Skin Images with fastai Improving Skin Cancer Detection with Deep Learning  ","date":1575318393,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578337009,"objectID":"6d564641e637ea112d208ca0b7b4db37","permalink":"https://pcx.linkedinfo.co/post/skin-lesion-cls/","publishdate":"2019-12-02T21:26:33+01:00","relpermalink":"/post/skin-lesion-cls/","section":"post","summary":"Introduction In this post we will show how to do skin lesion image classification with deep neural networks. It is an image classifier trained on the HAM10000 dataset, the same problem in the International Skin Imaging Collaboration (ISIC) 2018 challenge task3.\nThe solution in this post is mainly based on some web posts and methods from the ISIC2018 leadboard.\nThe classification neural network model is tested with pretrained ResNet and DenseNet and implemented with PyTOrch.","tags":["Image Classification","Deep Learning","Machine Learning"],"title":"Skin Lesion Image Classification with Deep Convolutional Neural Networks","type":"post"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1575154800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575154800,"objectID":"f4b2a9c2e4fef84d4079ba45856b2f4f","permalink":"https://pcx.linkedinfo.co/publication/pcx-2019-b/","publishdate":"2019-12-01T00:00:00+01:00","relpermalink":"/publication/pcx-2019-b/","section":"publication","summary":"Health data integration enables a collaborative utilization of data across different systems. It not only provides a comprehensive view of a patient's health but can also potentially cope with challenges faced by the current healthcare system. In this literature review, we investigated the existing work on heterogeneous health data integration as well as the methods of utilizing the integrated health data. Our search was narrowed down to 32 articles for analysis. The integration approaches in the reviewed articles were classified into three classifications, and the utilization approaches were classified into five classifications. The topic of health data integration is still under debate and problems are far from being resolved. This review suggests the need for a more efficient way to invoke the various services for aggregating health data, as well as a more effective way to integrate the aggregated health data for supporting collaborative utilization. We have found that the combination of Web Application Programming Interface and Semantic Web technologies has the potential to cope with the challenges based on our analysis of the review result.","tags":null,"title":"A literature review of current technologies on health data integration for patient-centered health management","type":"publication"},{"authors":[],"categories":[],"content":"  This code snippet is to predict topic tags based on the text of an article. Each article could have 1 or more tags (usually have at least 1 tag), and the tags are not mutually exclusive. So this is a multi-label classification problem. It\u0026#39;s different from multi-class classification, the classes in multi-class classification are mutually exclusive, i.e., each item belongs to 1 and only 1 class.  In this snippet, we will use OneVsRestClassifier (the One-Vs-the-Rest) in scikit-learn to process the multi-label classification. The article data will be retrieved from LinkedInfo.co via Web API. The methods in this snippet should give credits to Working With Text Data - scikit-learn and this post. Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB Preprocessing data and explore the method   dataset.df_tags fetches the data set from LinkedInfo.co. It calls Web API of LinkedInfo.co to retrieve the article list, and then download and extract the full text of each article based on an article\u0026#39;s url. The tags of each article are encoded using MultiLabelBinarizer in scikit-learn. The implementation of the code could be found in dataset.py. We\u0026#39;ve set the parameter of content_length_threshold to 100 to screen out the articles with less than 100 for the description or full text. import dataset ds = dataset.df_tags(content_length_threshold=100)   The dataset contains 3353 articles by the time retrieved the data. The dataset re returned as an object with the following attribute:     ds.data: pandas.DataFrame with cols of title, description, fulltext    ds.target: encoding of tagsID    ds.target_names: tagsID    ds.target_decoded: the list of lists contains tagsID for each info    \u0026gt;\u0026gt; ds.data.head()      description fulltext title   0 Both HTTP 1.x and HTTP/2 rely on lower level c… [Stressgrid]()\\n\\n__\\n\\n[]( \u0026#34;home\u0026#34;)\\n\\n * [… Achieving 100k connections per second with Elixir   1 At Phusion we run a simple multithreaded HTTP … [![Hongli Lai](images/avatar-b64f1ad5.png)](… What causes Ruby memory bloat?   2 Have you ever wanted to contribute to a projec… [ ![Real Python](/static/real-python-logo.ab1a… Managing Multiple Python Versions With pyenv   3 安卓在版本Pie中第一次引入了ART优化配置文件，这个新特性利用发送到Play Cloud的… 安卓在版本Pie中第一次引入了[ART优化配置文件](https://youtu.be/Yi... ART云配置文件，提高安卓应用的性能   4 I work at Red Hat on GCC, the GNU Compiler Col… [ ![Red Hat\\nLogo](https://developers.redhat.c... Usability improvements in GCC 9    \u0026gt;\u0026gt; ds.target[:5] array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]])  \u0026gt;\u0026gt; ds.target_names[:5] array([\u0026#39;academia\u0026#39;, \u0026#39;access-control\u0026#39;, \u0026#39;activemq\u0026#39;, \u0026#39;aes\u0026#39;, \u0026#39;agile\u0026#39;], dtype=object)  \u0026gt;\u0026gt; ds.target_decoded[:5] [[\u0026#39;concurrency\u0026#39;, \u0026#39;elixir\u0026#39;], [\u0026#39;ruby\u0026#39;], [\u0026#39;python\u0026#39;, \u0026#39;virtualenv\u0026#39;], [\u0026#39;android\u0026#39;], [\u0026#39;gcc\u0026#39;]]   The following snippet is the actual process of getting the above dataset, by reading from file. import json import pandas as pd from sklearn.preprocessing import MultiLabelBinarizer infos_file = \u0026#39;data/infos/infos_0_3353_fulltext.json\u0026#39; with open(infos_file, \u0026#39;r\u0026#39;) as f: infos = json.load(f) content_length_threshold = 100 data_lst = [] tags_lst = [] for info in infos[\u0026#39;content\u0026#39;]: if len(info[\u0026#39;fulltext\u0026#39;]) \u0026lt; content_length_threshold: continue if len(info[\u0026#39;description\u0026#39;]) \u0026lt; content_length_threshold: continue data_lst.append({\u0026#39;title\u0026#39;: info[\u0026#39;title\u0026#39;], \u0026#39;description\u0026#39;: info[\u0026#39;description\u0026#39;], \u0026#39;fulltext\u0026#39;: info[\u0026#39;fulltext\u0026#39;]}) tags_lst.append([tag[\u0026#39;tagID\u0026#39;] for tag in info[\u0026#39;tags\u0026#39;]]) df_data = pd.DataFrame(data_lst) df_tags = pd.DataFrame(tags_lst) # fit and transform the binarizer mlb = MultiLabelBinarizer() Y = mlb.fit_transform(tags_lst) Y.shape  (3221, 560)   Now we\u0026#39;ve transformed the target (tags) but we cannot directly perform the algorithms on the text data, so we have to process and transform them into vectors. In order to do this, we will use TfidfVectorizer to preprocess, tokenize, filter stop words and transform the text data. The TfidfVectorizer implements the tf-idf (Term Frequency-Inverse Document Frequency) to reflect how important a word is to to a document in a collection of documents. from sklearn.feature_extraction.text import TfidfVectorizer # Use the default parameters for now, use_idf=True in default vectorizer = TfidfVectorizer() # Use the short descriptions for now for faster processing X = vectorizer.fit_transform(df_data.description) X.shape  (3221, 35506)   As mentioned in the beginning, this is a multi-label classification problem, we will use OneVsRestClassifier to tackle our problem. And firstly we will use the SVM (Support Vector Machines) with linear kernel, implemented as LinearSVC in scikit-learn, to do the classification. from sklearn.multiclass import OneVsRestClassifier from sklearn.svm import LinearSVC from sklearn.model_selection import train_test_split # Use default parameters, and train and test with small set of samples. clf = OneVsRestClassifier(LinearSVC()) from sklearn.utils import resample X_sample, Y_sample = resample( X, Y, n_samples=1000, replace=False, random_state=7) # X_sample_test, Y_sample_test = resample( # X, Y, n_samples=10, replace=False, random_state=1) X_sample_train, X_sample_test, Y_sample_train, Y_sample_test = train_test_split( X_sample, Y_sample, test_size=0.01, random_state=42) clf.fit(X_sample, Y_sample) Y_sample_pred = clf.predict(X_sample_test) # Inverse transform the vectors back to tags pred_transformed = mlb.inverse_transform(Y_sample_pred) test_transformed = mlb.inverse_transform(Y_sample_test) for (t, p) in zip(test_transformed, pred_transformed): print(f\u0026#39;tags: {t} predicted as: {p}\u0026#39;)  tags: ('javascript',) predicted as: ('javascript',) tags: ('erasure-code', 'storage') predicted as: () tags: ('mysql', 'network') predicted as: () tags: ('token',) predicted as: () tags: ('flask', 'python', 'web') predicted as: () tags: ('refactoring',) predicted as: () tags: ('emacs',) predicted as: () tags: ('async', 'javascript', 'promises') predicted as: ('async', 'javascript') tags: ('neural-networks',) predicted as: () tags: ('kubernetes',) predicted as: ('kubernetes',)   Though not very satisfied, this classifier predicted right a few tags. Next we\u0026#39;ll try to search for the best parameters for the classifier and train with fulltext of articles. Search for best model parameters for SVM with linear kernel   For the estimators TfidfVectorizer and LinearSVC, they both have many parameters could be tuned for better performance. We\u0026#39;ll the GridSearchCV to search for the best parameters with the help of Pipeline. from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split, GridSearchCV # Split the dataset into training and test set, and use fulltext of articles: X_train, X_test, Y_train, Y_test = train_test_split( df_data.fulltext, Y, test_size=0.5, random_state=42) # Build vectorizer classifier pipeline clf = Pipeline([ (\u0026#39;vect\u0026#39;, TfidfVectorizer()), (\u0026#39;clf\u0026#39;, OneVsRestClassifier(LinearSVC())), ]) # Grid search parameters, I minimized the parameter set based on previous # experience to accelerate the processing speed. # And the combination of penalty=\u0026#39;l1\u0026#39; and loss=\u0026#39;squared_hinge\u0026#39; are not supported when dual=True parameters = { \u0026#39;vect__ngram_range\u0026#39;: [(1, 2), (1, 3), (1, 4)], \u0026#39;vect__max_df\u0026#39;: [1, 0.9, 0.8, 0.7], \u0026#39;vect__min_df\u0026#39;: [1, 0.9, 0.8, 0.7, 0], \u0026#39;vect__use_idf\u0026#39;: [True, False], \u0026#39;clf__estimator__penalty\u0026#39;: [\u0026#39;l1\u0026#39;, \u0026#39;l2\u0026#39;], \u0026#39;clf__estimator__C\u0026#39;: [1, 10, 100, 1000], \u0026#39;clf__estimator__dual\u0026#39;: [False], } gs_clf = GridSearchCV(clf, parameters, cv=5, n_jobs=-1) gs_clf.fit(X_train, Y_train)  import datetime from sklearn import metrics # Predict the outcome on the testing set in a variable named y_predicted Y_predicted = gs_clf.predict(X_test) print(metrics.classification_report(Y_test, Y_predicted)) print(gs_clf.best_params_) print(gs_clf.best_score_) # Export some of the result cols cols = [ \u0026#39;mean_test_score\u0026#39;, \u0026#39;mean_fit_time\u0026#39;, \u0026#39;param_vect__ngram_range\u0026#39;, ] df_result = pd.DataFrame(gs_clf.cv_results_) df_result = df_result.sort_values(by=\u0026#39;rank_test_score\u0026#39;) df_result = df_result[cols] timestamp = datetime.now().strftime(\u0026#39;%Y-%m-%d_%H-%M-%S\u0026#39;) df_result.to_html( f\u0026#39;data/results/gridcv_results_{timestamp}_linearSVC.html\u0026#39;)   Here we attach the top-5 performed classifiers with selected parameters.   rank_test_score mean_test_score mean_fit_time param_vect__max_df param_vect__ngram_range param_vect__use_idf param_clf__estimator__penalty param_clf__estimator__C     64 1 0.140811 96.127405 0.8 (1, 4) True l1 10   70 2 0.140215 103.252332 0.7 (1, 4) True l1 10   58 2 0.140215 98.990952 0.9 (1, 4) True l1 10   154 2 0.140215 1690.433151 0.9 (1, 4) True l1 1000   68 5 0.139618 70.778621 0.7 (1, 3) True l1 10    Training and testing with the best parameters   Based on the grid search results, we found the following parameters combined with the default parameters have the best performance. Now let\u0026#39;s see how it will perform. X_train, X_test, Y_train, Y_test = train_test_split( df_data, Y, test_size=0.2, random_state=42) clf = Pipeline([ (\u0026#39;vect\u0026#39;, TfidfVectorizer(use_idf=True, max_df=0.8, ngram_range=[1, 4])), (\u0026#39;clf\u0026#39;, OneVsRestClassifier(LinearSVC(penalty=\u0026#39;l1\u0026#39;, C=10, dual=False))), ]) clf.fit(X_train.fulltext, Y_train) Y_pred = clf.predict(X_test.fulltext) # Inverse transform the vectors back to tags pred_transformed = mlb.inverse_transform(Y_pred) test_transformed = mlb.inverse_transform(Y_test) for (title, t, p) in zip(X_test.title, test_transformed, pred_transformed): print(f\u0026#39;Article title: {title} \\n\u0026#39; f\u0026#39;Manual tags: {t} \\n\u0026#39; f\u0026#39;predicted as: {p}\\n\u0026#39;)   Here below is a fraction of the list that shows the manually input tags and the predicted tags. We can see that usually the more frequently appeared and more popular tags have better change to be correctly predicted. Personally, I would say the prediction is satisfied to me comparing when I tag the articles manually. However, there\u0026#39;s much room for improvement. Article title: Will PWAs Replace Native Mobile Apps? Manual tags: (\u0026#39;pwa\u0026#39;,) predicted as: (\u0026#39;pwa\u0026#39;,) Article title: 基于Consul的分布式信号量实现 Manual tags: (\u0026#39;consul\u0026#39;, \u0026#39;distributed-system\u0026#39;) predicted as: (\u0026#39;microservices\u0026#39;, \u0026#39;multithreading\u0026#39;) Article title: commit 和 branch 理解深入 Manual tags: (\u0026#39;git\u0026#39;,) predicted as: (\u0026#39;git\u0026#39;,) Article title: Existential types in Scala Manual tags: (\u0026#39;scala\u0026#39;,) predicted as: (\u0026#39;scala\u0026#39;,) Article title: Calling back into Python from llvmlite-JITed code Manual tags: (\u0026#39;jit\u0026#39;, \u0026#39;python\u0026#39;) predicted as: (\u0026#39;compiler\u0026#39;, \u0026#39;python\u0026#39;) Article title: Writing a Simple Linux Kernel Module Manual tags: (\u0026#39;kernel\u0026#39;, \u0026#39;linux\u0026#39;) predicted as: (\u0026#39;linux\u0026#39;,) Article title: Semantic segmentation with OpenCV and deep learning Manual tags: (\u0026#39;deep-learning\u0026#39;, \u0026#39;opencv\u0026#39;) predicted as: (\u0026#39;deep-learning\u0026#39;, \u0026#39;image-classification\u0026#39;, \u0026#39;opencv\u0026#39;) Article title: Transducers: Efficient Data Processing Pipelines in JavaScript Manual tags: (\u0026#39;javascript\u0026#39;,) predicted as: (\u0026#39;javascript\u0026#39;,) Article title: C++之stl::string写时拷贝导致的问题 Manual tags: (\u0026#39;cpp\u0026#39;,) predicted as: (\u0026#39;functional-programming\u0026#39;,) Article title: WebSocket 浅析 Manual tags: (\u0026#39;websocket\u0026#39;,) predicted as: (\u0026#39;websocket\u0026#39;,) Article title: You shouldn’t name your variables after their types for the same reason you wouldn’t name your pets “dog” or “cat” Manual tags: (\u0026#39;golang\u0026#39;,) predicted as: (\u0026#39;golang\u0026#39;,) Article title: Introduction to Data Visualization using Python Manual tags: (\u0026#39;data-visualization\u0026#39;, \u0026#39;python\u0026#39;) predicted as: (\u0026#39;data-visualization\u0026#39;, \u0026#39;matplotlib\u0026#39;, \u0026#39;python\u0026#39;) Article title: How JavaScript works: A comparison with WebAssembly + why in certain cases it’s better to use it over JavaScript Manual tags: (\u0026#39;javascript\u0026#39;, \u0026#39;webassembly\u0026#39;) predicted as: (\u0026#39;javascript\u0026#39;, \u0026#39;webassembly\u0026#39;) Article title: Parsing logs 230x faster with Rust Manual tags: (\u0026#39;log\u0026#39;, \u0026#39;rust\u0026#39;) predicted as: (\u0026#39;rust\u0026#39;,) Article title: Troubleshooting Memory Issues in Java Applications Manual tags: (\u0026#39;java\u0026#39;, \u0026#39;memory\u0026#39;) predicted as: (\u0026#39;java\u0026#39;,) Article title: How to use Docker for Node.js development Manual tags: (\u0026#39;docker\u0026#39;, \u0026#39;node.js\u0026#39;) predicted as: (\u0026#39;docker\u0026#39;,)  A glance at the different evaluation metrics   Now let\u0026#39;s have a look at the evaluation metrics on the prediction performance. Evaluating multi-label classification is very different from evaluating binary classification. There\u0026#39;re quite many different evaluation methods for different situations in the model evaluation part of scikit-learn\u0026#39;s documentation. We will take a look at the ones that suit this problem.  We can start with the accuracy_score function in metrics module. As mentioned in scikit-learn documentation, in multi-label classification, a subset accuracy is 1.0 when the entire set of predicted labels for a sample matches strictly with the true label set. The equation is simple like this: $$\\operatorname{accuracy}(y, \\hat{y})=\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {minples }}-1} 1\\left(\\hat{y}_{i}=y_{i}\\right)$$ from sklearn import metrics import matplotlib.pyplot as plt metrics.accuracy_score(Y_test, Y_pred)  0.26356589147286824   The score is somehow low. But we should be noted that for this problem, an inexact match of the labels is acceptable in many cases, e.g., an article talks about the golang\u0026#39;s interface is predicted with an only label golang while it was manually labeled with golang and interface. So to my opinion, this accuracy_score is not a good evaluation metric for this problem.  Now let\u0026#39;s see the classification_report that presents averaged precision, recall and f1-score. print(metrics.classification_report(Y_test, Y_pred))       precision recall f1-score support   micro avg 0.74 0.42 0.54 1186   macro avg 0.17 0.13 0.14 1186   weighted avg 0.60 0.42 0.48 1186     Let\u0026#39;s look at the micro row. Why? Let me quote scikit-learn\u0026#39;s documentation:   \u0026#34;micro\u0026#34; gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.   Here we\u0026#39;re more interested in the average precision, which is 0.74. As we mentioned, for this problem and for me, it\u0026#39;s more important to not predict a label that should be negative to an article. Some of the labels for an article, e.g., the label interface for the just mentioned article, are less important. So I\u0026#39;m OK for having a low score of recall, which measure how good the model predicts all the labels as the manually labeled.  However, there\u0026#39;s much room for improvement.    Many of the labels have very few appearances or even once. These labels could be filtered out or oversampling with text augmentation to mitigate the impact to model performance.    The training-test set split should be controlled by methods like stratified sampling, so that all the labels would appear in both sets with similar percentages. But again this problem is unlikely to be solved by now since there isn\u0026#39;t enough samples.    Another problem to be though about is, the training samples are not equally labeled, i.e., for the same example all the articles talking about golang\u0026#39;s interface, some of them labeled with golang + interface while some of them labeled only golang.   ","date":1568201803,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568374603,"objectID":"76132e465e55402b9d745c8015ea71fc","permalink":"https://pcx.linkedinfo.co/post/text-tag-prediction/","publishdate":"2019-09-11T13:36:43+02:00","relpermalink":"/post/text-tag-prediction/","section":"post","summary":"This code snippet is to predict topic tags based on the text of an article. Each article could have 1 or more tags (usually have at least 1 tag), and the tags are not mutually exclusive. So this is a multi-label classification problem. It\u0026#39;s different from multi-class classification, the classes in multi-class classification are mutually exclusive, i.e., each item belongs to 1 and only 1 class.  In this snippet, we will use OneVsRestClassifier (the One-Vs-the-Rest) in scikit-learn to process the multi-label classification.","tags":["Machine Learning","Multi-label classification","Text classification","LinkedInfo.co"],"title":"Multi-label classification to predict topic tags of technical articles from LinkedInfo.co","type":"post"},{"authors":[],"categories":[],"content":" Thanks to pmarcelino and serigne for their great work.\nThis is my second kaggle competition to practice on the knowledge of data analysis and machine learning. Unlike the Titanic competition, this house prices is a regression problem. So there will be much difference from the previous binary classification. For this competition, we will have 79 variables that describe various aspects of a house and with a price in the training data set. And then predict the prices of houses in the testing set based on the 79 variables. This will be a long journey with the 79 variables. So let\u0026rsquo;s start to explore the data with the data description. Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nimport os # from typing import List, Union # from pysnooper import snoop import pandas as pd # import matplotlib.pyplot as plt # import numpy as np loc = 'house price' if os.getcwd().split('/')[-1] != loc: os.chdir(loc) df_train = pd.read_csv(f'input/train.csv') df_test = pd.read_csv(f'input/test.csv')  Data exploration Let\u0026rsquo;s firstly have a look at the data we have.\nprint(df_train.shape) df_train.head()  (1460, 81)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice     0 1 60 RL 65.0 8450 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 2 2008 WD Normal 208500   1 2 20 RL 80.0 9600 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 5 2007 WD Normal 181500   2 3 60 RL 68.0 11250 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 9 2008 WD Normal 223500   3 4 70 RL 60.0 9550 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 2 2006 WD Abnorml 140000   4 5 60 RL 84.0 14260 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 12 2008 WD Normal 250000    5 rows × 81 columns\n print(df_test.shape) df_test.head()  (1459, 80)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition     0 1461 20 RH 80.0 11622 Pave NaN Reg Lvl AllPub ... 120 0 NaN MnPrv NaN 0 6 2010 WD Normal   1 1462 20 RL 81.0 14267 Pave NaN IR1 Lvl AllPub ... 0 0 NaN NaN Gar2 12500 6 2010 WD Normal   2 1463 60 RL 74.0 13830 Pave NaN IR1 Lvl AllPub ... 0 0 NaN MnPrv NaN 0 3 2010 WD Normal   3 1464 60 RL 78.0 9978 Pave NaN IR1 Lvl AllPub ... 0 0 NaN NaN NaN 0 6 2010 WD Normal   4 1465 120 RL 43.0 5005 Pave NaN IR1 HLS AllPub ... 144 0 NaN NaN NaN 0 1 2010 WD Normal    5 rows × 80 columns\n So we have 1460 rows in training set and 1459 rows in testing set. Besides the price col in the training set, both data sets have 79 cols of variables + 1 col of \u0026lsquo;Id\u0026rsquo;.\nCheck missing values Now let\u0026rsquo;s check if there is any missing value in the data.\ndef cols_missing_value(df): df_null_sum = df.isnull().sum() df_na = (df.isnull().sum() / len(df)) * 100 missing_data = pd.concat({'Missing Ratio %': df_na, 'Total': df_null_sum}, axis='columns') return missing_data.drop(missing_data[missing_data['Total'] == 0].index ).sort_values(by='Total', ascending=False) cols_missing_value(df_train)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Missing Ratio % Total     PoolQC 99.520548 1453   MiscFeature 96.301370 1406   Alley 93.767123 1369   Fence 80.753425 1179   FireplaceQu 47.260274 690   LotFrontage 17.739726 259   GarageType 5.547945 81   GarageYrBlt 5.547945 81   GarageFinish 5.547945 81   GarageQual 5.547945 81   GarageCond 5.547945 81   BsmtExposure 2.602740 38   BsmtFinType2 2.602740 38   BsmtFinType1 2.534247 37   BsmtCond 2.534247 37   BsmtQual 2.534247 37   MasVnrArea 0.547945 8   MasVnrType 0.547945 8   Electrical 0.068493 1     cols_missing_value(pd.concat((df_train[df_test.columns], df_test)))   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Missing Ratio % Total     PoolQC 99.657417 2909   MiscFeature 96.402878 2814   Alley 93.216855 2721   Fence 80.438506 2348   FireplaceQu 48.646797 1420   LotFrontage 16.649538 486   GarageFinish 5.447071 159   GarageQual 5.447071 159   GarageCond 5.447071 159   GarageYrBlt 5.447071 159   GarageType 5.378554 157   BsmtExposure 2.809181 82   BsmtCond 2.809181 82   BsmtQual 2.774923 81   BsmtFinType2 2.740665 80   BsmtFinType1 2.706406 79   MasVnrType 0.822199 24   MasVnrArea 0.787941 23   MSZoning 0.137033 4   BsmtFullBath 0.068517 2   BsmtHalfBath 0.068517 2   Functional 0.068517 2   Utilities 0.068517 2   GarageArea 0.034258 1   GarageCars 0.034258 1   Electrical 0.034258 1   KitchenQual 0.034258 1   TotalBsmtSF 0.034258 1   BsmtUnfSF 0.034258 1   BsmtFinSF2 0.034258 1   BsmtFinSF1 0.034258 1   Exterior2nd 0.034258 1   Exterior1st 0.034258 1   SaleType 0.034258 1     There are quite a lot of missing values, some cols are missing almost all of the data. We need to handle the missing values by imputation or other methods later.\nA look at distributions As we\u0026rsquo;re predicting the \u0026lsquo;SalePrice\u0026rsquo;, so we should have a look at the stats of this col.\ndf_train['SalePrice'].describe()  count 1460.000000 mean 180921.195890 std 79442.502883 min 34900.000000 25% 129975.000000 50% 163000.000000 75% 214000.000000 max 755000.000000 Name: SalePrice, dtype: float64  df_train['SalePrice'].hist(bins=30)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x11414e4a8\u0026gt;  The values of \u0026lsquo;SalePrice\u0026rsquo; does fall in a normal distribution. In general, learning algorithms benefit from standardization of the data set. So we\u0026rsquo;ll transform the target values by QuantileTransformer and TransformedTargetRegressor later when training and testing.\nNow let\u0026rsquo;s have a look at other columns\u0026rsquo; skewnesses.\nfrom scipy.stats import skew # Concat training and testing sets together to see the full picture df_all = pd.concat((df_train, df_test)).reset_index( drop=True).drop(['SalePrice'], axis='columns') numeric_cols = df_all.select_dtypes( exclude=['object', 'category']).columns # Check the skewness of the numerical cols skewed_cols = df_all[numeric_cols].apply( lambda col: skew(col)).sort_values(ascending=False) skewness = pd.DataFrame({'Skewness': skewed_cols}) skewness.head(10) skewness = skewness[abs(skewness['Skewness']) \u0026gt; 0.75] print(f'{skewness.shape[0]} skewed numerical columns.') df_all[skewness.index].hist(figsize=(14, 12))  /Users/pcx/.pyenv/versions/ml/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass 'sort=False'. To retain the current behavior and silence the warning, pass 'sort=True'. \u0026quot;\u0026quot;\u0026quot; 15 skewed numerical columns. array([[\u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x1209d3550\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x1041d86d8\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x104200c50\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x104233208\u0026gt;], [\u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120b97780\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120bc1cf8\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120bef2b0\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120c17860\u0026gt;], [\u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120c17898\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120c71358\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120f2a8d0\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120f53e48\u0026gt;], [\u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120f84400\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120fac978\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120fd3ef0\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x1210054a8\u0026gt;]], dtype=object)  We also need to handle the skewed variables later.\nPreprocessing data Impute missing values There are quite a lot of missing values, some cols are missing almost all of the data. Now look into the data description to see what the variables really are and how should we deal with them. We\u0026rsquo;re now concating the training set and testing set since we need to handle the missing values in both data sets. We will split them when we need.\n# keep Id col for later unpack training and testing df ids_train = df_train['Id'] ids_test = df_test['Id'] Y_train = df_train['SalePrice'].values df_all = pd.concat((df_train, df_test)).reset_index( drop=True).drop(['SalePrice'], axis='columns')  /Users/pcx/.pyenv/versions/ml/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass 'sort=False'. To retain the current behavior and silence the warning, pass 'sort=True'.  \u0026lsquo;PoolQC\u0026rsquo; (Pool quality) is the one with most missing values, and NA stands for \u0026ldquo;No Pool\u0026rdquo; (described in data_description.txt), so the missing values should be replaced by str \u0026ldquo;No Pool\u0026rdquo;. And this col should be an ordered categorical variable.\ndf_all['PoolQC'] = df_all['PoolQC'].fillna(\u0026quot;No Pool\u0026quot;)  The same applies to \u0026lsquo;MiscFeature\u0026rsquo;, \u0026lsquo;Alley\u0026rsquo;, \u0026lsquo;Fence\u0026rsquo;, \u0026lsquo;FireplaceQu\u0026rsquo;, \u0026lsquo;GarageType\u0026rsquo;, \u0026lsquo;GarageFinish\u0026rsquo;, \u0026lsquo;GarageQual\u0026rsquo;, \u0026lsquo;GarageCond\u0026rsquo;, \u0026lsquo;BsmtQual\u0026rsquo;, \u0026lsquo;BsmtCond\u0026rsquo;, \u0026lsquo;BsmtExposure\u0026rsquo;, \u0026lsquo;BsmtFinType1\u0026rsquo;, \u0026lsquo;BsmtFinType2\u0026rsquo;, \u0026lsquo;MasVnrType\u0026rsquo;\ndf_all['MiscFeature'] = df_all['MiscFeature'].fillna(\u0026quot;None\u0026quot;) df_all['Alley'] = df_all['Alley'].fillna(\u0026quot;No Alley access\u0026quot;) df_all['Fence'] = df_all['Fence'].fillna(\u0026quot;No Fence\u0026quot;) df_all['FireplaceQu'] = df_all['FireplaceQu'].fillna(\u0026quot;No Fireplace\u0026quot;) df_all['GarageType'] = df_all['GarageType'].fillna(\u0026quot;No Garage\u0026quot;) df_all['GarageFinish'] = df_all['GarageFinish'].fillna(\u0026quot;No Garage\u0026quot;) df_all['GarageQual'] = df_all['GarageQual'].fillna(\u0026quot;No Garage\u0026quot;) df_all['GarageCond'] = df_all['GarageCond'].fillna(\u0026quot;No Garage\u0026quot;) df_all['BsmtCond'] = df_all['BsmtCond'].fillna(\u0026quot;No Basement\u0026quot;) df_all['BsmtQual'] = df_all['BsmtQual'].fillna(\u0026quot;No Basement\u0026quot;) df_all['BsmtExposure'] = df_all['BsmtExposure'].fillna(\u0026quot;No Basement\u0026quot;) df_all['BsmtFinType1'] = df_all['BsmtFinType1'].fillna(\u0026quot;No Basement\u0026quot;) df_all['BsmtFinType2'] = df_all['BsmtFinType2'].fillna(\u0026quot;No Basement\u0026quot;)  Now let\u0026rsquo;s check \u0026lsquo;GarageYrBlt\u0026rsquo;, \u0026lsquo;GarageArea\u0026rsquo;, \u0026lsquo;GarageCars\u0026rsquo;. Since only 1 record of \u0026lsquo;GarageCars\u0026rsquo; is missing, and it\u0026rsquo;s \u0026lsquo;GarageType\u0026rsquo; is \u0026lsquo;Detchd\u0026rsquo;, so let\u0026rsquo;s make it as size of the mode/median of \u0026lsquo;GarageCars\u0026rsquo; when type is \u0026lsquo;Detchd\u0026rsquo;.\ndf_all[df_all['GarageCars'].isnull()] df_all[df_all['GarageCars'].isnull()]['GarageType'] df_all['GarageCars'] = df_all['GarageCars'].fillna( int(df_all[df_all['GarageType'] == 'Detchd']['GarageCars'].mode()))  It\u0026rsquo;s the same record for the missing \u0026lsquo;GarageArea\u0026rsquo; value, as we filled its \u0026lsquo;GarageCars\u0026rsquo; to the mode value, we will fill the area as the mean value of \u0026lsquo;GarageArea\u0026rsquo; where the \u0026lsquo;GarageCars\u0026rsquo; == mode value of \u0026lsquo;Detchd\u0026rsquo;.\ndf_all[df_all['GarageArea'].isnull()] df_all['GarageArea'] = df_all['GarageArea'].fillna( df_all[df_all['GarageType'] == 'Detchd']['GarageArea'].mean()) # df_all[df_all['GarageYrBlt'].isnull()]['GarageType']  For the records that have no garage, we set the null value of \u0026lsquo;GarageYrBlt\u0026rsquo; to 0, but for the records with type \u0026lsquo;Detchd\u0026rsquo;, we set the null value to the median value of the built year with type \u0026lsquo;Detchd\u0026rsquo;.\nyear_median = df_all[df_all['GarageType'] == 'Detchd']['GarageYrBlt'].median() df_all['GarageYrBlt'] = df_all['GarageYrBlt'][ df_all['GarageType'] == 'Detchd'].fillna(year_median) df_all['GarageYrBlt'] = df_all['GarageYrBlt'].fillna(0)  Since there are quite many missing value for \u0026lsquo;LotFrontage\u0026rsquo; (16.65%), we would drop this col.\ndf_all = df_all.drop('LotFrontage', axis='columns')  Filling with 0 for those likely to be 0.\nbsmt_zero_missing = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'] for col in bsmt_zero_missing: df_all[col] = df_all[col].fillna(0)  \u0026lsquo;MasVnrArea\u0026rsquo; and \u0026lsquo;MasVnrType\u0026rsquo;\ndf_all[df_all['MasVnrType'].isnull()]['MasVnrArea'] df_all['MasVnrType'].astype('category').value_counts()  None 1742 BrkFace 879 Stone 249 BrkCmn 25 Name: MasVnrType, dtype: int64  For all the records with missing values of \u0026lsquo;MasVnrType\u0026rsquo;, 1 record with \u0026lsquo;MasVnrArea\u0026rsquo; is not NaN, so we filling its type as \u0026lsquo;BrkFace\u0026rsquo;, which is the most occurred none-None type. Other missing values of \u0026lsquo;MasVnrType\u0026rsquo; we will fill in with the most common None, so its \u0026lsquo;MasVnrArea\u0026rsquo; will be 0.\ndf_all['MasVnrType'] = df_all['MasVnrType'][ df_all['MasVnrArea'].notna()].fillna('BrkFace') df_all['MasVnrType'] = df_all['MasVnrType'].fillna('None') df_all['MasVnrArea'] = df_all['MasVnrArea'].fillna(0)  Set the NaN to the mostly occurred value \u0026lsquo;RL\u0026rsquo;.\ndf_all['MSZoning'].astype('category').value_counts() df_all['MSZoning'] = df_all['MSZoning'].fillna('RL')  # Set the NaN to the mostly occurred value 'AllPub'. df_all['Utilities'].astype('category').value_counts() df_all['Utilities'] = df_all['Utilities'].fillna('AllPub') # keep or not? df_all = df_all.drop(['Utilities'], axis='columns')  Set NaN to mostly occurred value for the rest cols.\ncols_nan_mode = ['Functional', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'MSSubClass'] for col in cols_nan_mode: df_all[col] = df_all[col].fillna(df_all[col].mode()[0]) cols_missing_value(df_all)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Missing Ratio % Total       Now there\u0026rsquo;s no missing values. Let\u0026rsquo;s move to the next part.\nTransform categorical variables We\u0026rsquo;ll firstly transform some of the variables from numerical to categorical as they should be. And add one variable.\ncols_num_cat = ['MSSubClass', 'YrSold', 'MoSold'] for col in cols_num_cat: df_all[col] = df_all[col].astype('category') # Adding total sqfootage feature df_all['TotalSF'] = df_all['TotalBsmtSF'] + \\ df_all['1stFlrSF'] + df_all['2ndFlrSF']  Check and handle outliers After handling the missing values, now we have a look at if there are outliers in the training set with the target variable by scatter plots.\nimport matplotlib.pyplot as plt df_train = df_all[:len(ids_train)] df_test = df_all[len(ids_train):] cols = df_train.select_dtypes(['int64', 'float64']) # cols = df_train.select_dtypes(['int64', 'float64']) df_train = pd.concat([df_train, pd.DataFrame( Y_train, columns=['SalePrice'])], axis='columns') fig, axes = plt.subplots(6, 6, figsize=(30, 30)) for i, col in enumerate(cols): df_train.plot.scatter(x=col, y='SalePrice', ax=axes[i // 6, i % 6])  The continuous variable \u0026lsquo;GrLivArea\u0026rsquo; seems having 2 values have very different \u0026ldquo;hehavior\u0026rdquo;. The 2 bottom right dots may be very inferential that have quite big areas but low prices. Let\u0026rsquo;s remove them to see if it\u0026rsquo;s better for the results. After removing these 2 rows, we would see that outliers in other cols such \u0026lsquo;TotalBsmtSF\u0026rsquo; and \u0026lsquo;TotalSF\u0026rsquo; are disappeared as well.\ndf_train = df_train.drop(df_train[(df_train['GrLivArea'] \u0026gt; 4000) \u0026amp; (df_train['SalePrice'] \u0026lt; 250000)].index)  # Packing back data sets after removing outliers in training set. ids_train = df_train['Id'] ids_test = df_test['Id'] Y_train = df_train['SalePrice'].values df_all = pd.concat((df_train, df_test)).reset_index( drop=True).drop(['SalePrice'], axis='columns')  Transform skewed variables We will transform the skewed variables into normal distributions by quantile_transform.\nnumeric_cols = df_all.select_dtypes( exclude=['object', 'category']).columns # Check the skewnesses of the numerical cols skewed_cols = df_all[numeric_cols].apply( lambda col: skew(col)).sort_values(ascending=False) skewness = pd.DataFrame({'Skewness': skewed_cols}) skewness = skewness[abs(skewness['Skewness']) \u0026gt; 0.75] print(f'{skewness.shape[0]} skewed numerical columns.') from sklearn.preprocessing import quantile_transform import numpy as np skewed_features = skewness.index df_all[skewed_features] = quantile_transform( df_all[skewed_features], output_distribution='normal', copy=True)  20 skewed numerical columns.  # Check again for the skewnesses of the numerical cols skewed_cols = df_all[numeric_cols].apply( lambda col: skew(col)).sort_values(ascending=False) skewness = pd.DataFrame({'Skewness': skewed_cols}) skewness = skewness[abs(skewness['Skewness']) \u0026gt; 0.75] print(f'{skewness.shape[0]} skewed numerical columns.')  11 skewed numerical columns.  Encode categorical valuee Transform categorical cols by using pd.get_dummies().\nprint(df_all.shape) # Column names in the DataFrame to be encoded. If columns is None then all the # columns with object or category dtype will be converted. df_all = pd.get_dummies(df_all) print(df_all.shape)  (2917, 79) (2917, 330)  Training and testing Base model Now we will start to train and test with a base model with default parameters to see how it would perform as a base line. Root-Mean-Squared-Error (RMSE) as the evaluation metric for the competition, the equation is:\n$$\\operatorname{RMSE}(y, \\hat{y})=\\sqrt{\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {symples }}-1}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}$$.\n# Unpack training and testing data sets df_train = df_all[:len(ids_train)].drop(['Id'], axis='columns') df_test = df_all[len(ids_train):].drop(['Id'], axis='columns') X_train = df_train.values X_test = df_test  import numpy as np from sklearn.pipeline import Pipeline from sklearn.linear_model import Lasso, ElasticNet, Ridge from sklearn.model_selection import cross_val_score from sklearn.metrics import mean_squared_error, make_scorer from sklearn.compose import TransformedTargetRegressor from sklearn.preprocessing import QuantileTransformer Y_train_norm = np.log1p(Y_train) # there's no implementation of RMSE in the scikit-learn library, so we have to # define a scorer of RMSE def rmse_cal(y_true, y_pred): return np.sqrt(mean_squared_error(y_true, y_pred)) # return np.sqrt(np.sum(np.square(y_pred - y_true)) / len(y_pred)) # if the custom score function is a loss (greater_is_better=False), the output # of the python function is negated by the scorer object, conforming to the # cross validation convention that scorers return higher values for better # models. rmse = make_scorer(rmse_cal, greater_is_better=False) # ridgepip = Pipeline([ # ('tran', TransformedTargetRegressor( # regressor=Lasso(), func=np.log1p, inverse_func=np.expm1)), # ('tran', TransformedTargetRegressor( # regressor=Ridge(), func=np.log1p, inverse_func=np.expm1)), # ]) models = [ Lasso(), # ridgepip, # # ElasticNet(), Ridge(), ] CV = 5 for m in models: scores = -cross_val_score(m, X_train, Y_train_norm, scoring=rmse, cv=5, n_jobs=-1) print(f'{type(m).__name__}\\n' f'Scores: {scores}\\n' # +/-std*2 for 95% confidence interval f'Accuracy: {scores.mean(): 0.4f} (+/-{scores.std() * 2: 0.4f})\\n' f'{\u0026quot;-\u0026quot;*20}')  Lasso Scores: [0.22425222 0.23934427 0.23998284 0.24165163 0.23227816] Accuracy: 0.2355 (+/- 0.0129) -------------------- Ridge Scores: [0.11456344 0.12197379 0.13560006 0.1083432 0.1172416 ] Accuracy: 0.1195 (+/- 0.0183) --------------------  GridSearch for best model with best parameters The base models give somehow good results. The CV RMSE score of the /Ridge/ model is around the top-1000 in the competition\u0026rsquo;s leaderboard. Now let\u0026rsquo;s try to find the best parameters for these and other models with GridSearchCV.\nfrom sklearn.svm import SVR from sklearn.pipeline import Pipeline from sklearn.preprocessing import RobustScaler from sklearn.kernel_ridge import KernelRidge from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.ensemble import GradientBoostingRegressor from sklearn import metrics Y_train_norm = np.log1p(Y_train) X_train_cv, X_test_cv, Y_train_cv, Y_test_cv = train_test_split( X_train, Y_train_norm, test_size=0.3) param_space = { 'rob_lasso': { 'model': Pipeline([('sca', RobustScaler()), ('model', Lasso())]), 'params': { 'model__alpha': [0.00005, 0.0004, 0.0005, 0.0007, 0.005, 0.05, 0.5, 0.8, 1], } }, 'ridge': { 'model': Ridge(), 'params': { 'alpha': [1e-3, 1e-2, 1e-1, 1, 10], } }, 'kernel_ridge': { 'model': KernelRidge(), 'params': { 'alpha': [1e-3, 1e-2, 1e-1, 1, 10], } }, 'elastic_net': { 'model': Pipeline([('sca', RobustScaler()), ('model', ElasticNet())]), 'params': { 'model__alpha': [0.00005, 0.0004, 0.0005, 0.0007, 0.005, 0.05, 0.5, 0.8, 1], # Note that a good choice of list of values for l1_ratio is often to # put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. # Ridge) 'model__l1_ratio': [.1, .5, .7, .75, .8, .85, .9, .95, .97, .99, .995, 1], } }, # 'gboost': { # 'model': GradientBoostingRegressor(), # 'params': { # 'loss': ['ls', 'lad', 'huber', 'quantile'], # 'learning_rate': [0.01, 0.1], # 'n_estimators': [100, 500, 1000, 3000], # 'max_depth': [2, 3, 4], # 'min_samples_split': [2, 5, 10], # } # }, # 'svr': { # 'model': SVR(), # 'params': { # 'kernel': ['linear', 'rbf'], # 'C': [1, 10], # } # }, } gs_rec = [] # grid search parameters for name, pair in param_space.items(): print(f'{name}---------------') gs_rg = GridSearchCV(pair['model'], pair['params'], scoring=rmse, cv=CV, error_score=0, n_jobs=-1) gs_rg.fit(X_train, Y_train_norm) print(gs_rg.best_params_) print(gs_rg.best_score_) gs_rg_cv = GridSearchCV(pair['model'], pair['params'], scoring=rmse, cv=CV, error_score=0, n_jobs=-1) gs_rg_cv.fit(X_train_cv, Y_train_cv) pred_test = gs_rg_cv.predict(X_test_cv) y_score = rmse_cal(Y_test_cv, pred_test) print(gs_rg_cv.best_params_) print(gs_rg_cv.best_score_) print(y_score) gs_rec.append({ 'name': name, 'params': gs_rg.best_params_, 'score': -gs_rg.best_score_, 'cv_test_params': gs_rg_cv.best_params_, 'cv_test_score': y_score }) df_gs = pd.DataFrame(gs_rec, columns=['name', 'score', 'params', 'cv_test_score', 'cv_test_params'] ).sort_values(by=['score', 'cv_test_score']) df_gs  rob_lasso--------------- {'model__alpha': 0.0005} -0.1108321642082426 {'model__alpha': 0.0005} -0.11385591248537665 0.1092651116732159 ridge--------------- {'alpha': 10} -0.11417733254437629 {'alpha': 10} -0.11723423641202352 0.11022009984391984 kernel_ridge--------------- {'alpha': 10} -0.11675117173959225 {'alpha': 10} -0.1209044169077714 0.11171230919473786 elastic_net--------------- {'model__alpha': 0.0005, 'model__l1_ratio': 0.9} -0.11081242246612653 {'model__alpha': 0.0007, 'model__l1_ratio': 0.8} -0.1138195082928615 0.10934894252124043   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    name score params cv_test_score cv_test_params     3 elastic_net 0.110812 {'model__alpha': 0.0005, 'model__l1_ratio': 0.9} 0.109349 {'model__alpha': 0.0007, 'model__l1_ratio': 0.8}   0 rob_lasso 0.110832 {'model__alpha': 0.0005} 0.109265 {'model__alpha': 0.0005}   1 ridge 0.114177 {'alpha': 10} 0.110220 {'alpha': 10}   2 kernel_ridge 0.116751 {'alpha': 10} 0.111712 {'alpha': 10}     Now let\u0026rsquo;s Train with the best model so far and predict on the test data. As aforementioned, the values of \u0026lsquo;SalePrice\u0026rsquo; does fall in a normal distribution. So we\u0026rsquo;ll transform the target values by QuantileTransformer and TransformedTargetRegressor.\nfrom datetime import datetime # model = Pipeline( # [('sca', RobustScaler()), ('model', TransformedTargetRegressor( # regressor=ElasticNet(alpha=0.0005, l1_ratio=0.85), func=np.log1p, inverse_func=np.expm1))]) model = Pipeline( [('sca', RobustScaler()), ('model', TransformedTargetRegressor( regressor=ElasticNet(alpha=0.0005, l1_ratio=0.85), # regressor=Lasso(alpha=0.0005), transformer=QuantileTransformer(output_distribution='normal')))]) model.fit(X_train, Y_train) pred = model.predict(X_test) def submit(ids, pred, suffix): sub = pd.DataFrame() sub['Id'] = ids_test sub['SalePrice'] = pred timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S') # sub.to_csv( # f'result/kaggle1_sub_{suffix}_{score:.5f}.csv', index=False) sub.to_csv( f'submissions/{suffix}_{timestamp}.csv.gz', index=False, compression='gzip') submit(ids_test, pred, 'elastic_net')  ","date":1560267513,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568405173,"objectID":"1aaef6f96a32e5ce264193307b750ff7","permalink":"https://pcx.linkedinfo.co/post/houseprice/","publishdate":"2019-06-11T17:38:33+02:00","relpermalink":"/post/houseprice/","section":"post","summary":"Thanks to pmarcelino and serigne for their great work.\nThis is my second kaggle competition to practice on the knowledge of data analysis and machine learning. Unlike the Titanic competition, this house prices is a regression problem. So there will be much difference from the previous binary classification. For this competition, we will have 79 variables that describe various aspects of a house and with a price in the training data set.","tags":["Machine Learning","Regression"],"title":"Explore the house prices kaggle competition","type":"post"},{"authors":["Cong Peng","Prashant Goswami"],"categories":null,"content":"","date":1554069600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554069600,"objectID":"60d09f5ebc761287248b9f096f0928f1","permalink":"https://pcx.linkedinfo.co/publication/pcx-2019-a/","publishdate":"2019-04-01T00:00:00+02:00","relpermalink":"/publication/pcx-2019-a/","section":"publication","summary":"The development of electronic health records, wearable devices, health applications and Internet of Things (IoT)-empowered smart homes is promoting various applications. It also makes health self-management much more feasible, which can partially mitigate one of the challenges that the current healthcare system is facing. Effective and convenient self-management of health requires the collaborative use of health data and home environment data from different services, devices, and even open data on the Web. Although health data interoperability standards including HL7 Fast Healthcare Interoperability Resources (FHIR) and IoT ontology including Semantic Sensor Network (SSN) have been developed and promoted, it is impossible for all the different categories of services to adopt the same standard in the near future. This study presents a method that applies Semantic Web technologies to integrate the health data and home environment data from heterogeneously built services and devices. We propose a Web Ontology Language (OWL)-based integration ontology that models health data from HL7 FHIR standard implemented services, normal Web services and Web of Things (WoT) services and Linked Data together with home environment data from formal ontology-described WoT services. It works on the resource integration layer of the layered integration architecture. An example use case with a prototype implementation shows that the proposed method successfully integrates the health data and home environment data into a resource graph. The integrated data are annotated with semantics and ontological links, which make them machine-understandable and cross-system reusable.","tags":["FHIR","REST","Semantic Web","Web service","WoT","eHealth","health data integration","ontology","smart homes"],"title":"Meaningful Integration of Data from Heterogeneous Health Services and Home Environment Based on Ontology","type":"publication"},{"authors":null,"categories":null,"content":"The Web should be an open web. All the informations published on the Web are meant to be shared, share through links by search engines, rss, social networks, etc. This site is yet another method that tries to link all the informations (but starts with only technical articles on LinkedInfo) and share them.\nThe original idea of this side project is to utilize Semantic Web technologies and Machine learning to link the informations. Noble ambition shall start from basic, it needs to be improved little by little.\n","date":1548584274,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548584274,"objectID":"6268ed86f6986d5dbdacded5a30b3972","permalink":"https://pcx.linkedinfo.co/project/linkedinfo/","publishdate":"2019-01-27T11:17:54+01:00","relpermalink":"/project/linkedinfo/","section":"project","summary":"The Web should be an open web, all the informations published on the Web are meant to be shared. Linkedinfo.co is a Web service uses Semantic Web technologies and Machine Learning to link and share technical articles on the Web.","tags":["Semantic Web","Web technologies","Machine Learning"],"title":"LinkedInfo.co","type":"project"},{"authors":["Cong Peng"],"categories":null,"content":"","date":1546297200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546297200,"objectID":"8ca333643ce6917c513fec55c6c10a0c","permalink":"https://pcx.linkedinfo.co/publication/peng-2019/","publishdate":"2019-01-01T00:00:00+01:00","relpermalink":"/publication/peng-2019/","section":"publication","summary":"Group work-based learning is encouraged in higher education on account of both ped-agogical benefits and industrial employers's requirements. However, although a plenty of studies have been performed, there are still various factors that will affect students' group work-based learning in practice. It is important for the teachers to understand which factors are influenceable and what can be done to influence. This paper performs a literature review to identify the factors that has been investigated and reported in journal articles. Fifteen journal articles were found relevant and fifteen factors were identified, which could be influenced by instructors directly or indirectly. However, more evidence is needed to support the conclusion of some studies since they were performed only in one single course. Therefore, more studies are required on this topic to investigate the factors in different subject areas.","tags":["Group work","TBL","collaborative learning","teamwork"],"title":"What Can Teachers Do to Make the Group Work Learning Effective - a Literature Review","type":"publication"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1538344800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538344800,"objectID":"9706bb40671e2c103cc24202b650b5b7","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-d/","publishdate":"2018-10-01T00:00:00+02:00","relpermalink":"/publication/pcx-2018-d/","section":"publication","summary":"Effective and convenient self-management of health requires collaborative utilization of health data from different services provided by healthcare providers, consumer-facing products and even open data on the Web. Although health data interoperability standards include Fast Healthcare Interoperability Resources (FHIR) have been developed and promoted, it is impossible for all the different categories of services to adopt in the near future. The objective of this study aims to apply Semantic Web technologies to integrate the health data from heterogeneously built services. We present an Web Ontology Language (OWL)-based ontology that models together health data from FHIR standard implemented services, normal Web services and Linked Data. It works on the resource integration layer of the presented layered integration architecture. An example use case that demonstrates how this method integrates the health data into a linked semantic health resource graph with the proposed ontology is presented.","tags":["FHIR","Health data integration","REST","Semantic Web","Web service","eHealth","ontology"],"title":"An Ontological Approach to Integrate Health Resources from Different Categories of Services","type":"publication"},{"authors":[],"categories":[],"content":"","date":1515554369,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516245569,"objectID":"a0b16b66c87372eb421e9e35ae610b5b","permalink":"https://pcx.linkedinfo.co/post/web-notes/","publishdate":"2018-01-10T05:19:29+02:00","relpermalink":"/post/web-notes/","section":"post","summary":"Typically, the term “Web services” is used to label the older, XML-based interfaces on top of HTTP. The term “Web APIs” is more fashionable for interfaces that use HTTP and JSON. ","tags":["Web","Semantic Web"],"title":"Web Notes","type":"post"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"2979f299debfb142891b565c7962abe8","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-c/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-c/","section":"publication","summary":"The vast amount of Web services brings the problem of discovering desired services for composition and orchestration. The syntactic service matching methods based on the classical set theory have a difficulty to capture the imprecise information. Therefore, an approximate service matching approach based on fuzzy control is explored in this paper. A service description matching model to the OpenAPI specification, which is the most widely used standard for describing the defacto REST Web services, is proposed to realize the fuzzy service matching with the fuzzy inference method developed by Mamdani and Assilian. An evaluation shows that the fuzzy service matching approach performed slightly better than the weighted approach in the setting context.","tags":null,"title":"Fuzzy Matching of OpenAPI Described REST Services","type":"publication"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"07ecb044ca58895dc86215b3dd2d5afb","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-b/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-b/","section":"publication","summary":"Various health Web services host a huge amount of health data about patients. The heterogeneity of the services hinders the collaborative utilization of these health data, which can provide a valuable support for the self-management of chronic diseases. The combination of REST Web services and Semantic Web technologies has proven to be a viable approach to address the problem. This paper proposes a method to add semantic annotations to the REST Web services. The service descriptions and the resource representations with semantic annotations can be transformed into a resource graph. It integrates health data from different services, and can link to the health-domain ontologies and Linked Open Health Data to support health management and imaginative applications. The feasibility of our method is demonstrated by realizing with OpenAPI service description and JSON-LD representation in an example use case.","tags":null,"title":"Linking Health Web Services as Resource Graph by Semantic REST Resource Tagging","type":"publication"},{"authors":["Cong Peng","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"bd953f4e0ce85f1fbf6d4b9aa345c53d","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-a/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-a/","section":"publication","summary":"The utilization of Web services is becoming a human labor consuming work as the rapid growth of Web. The semantic annotated service description can support more automatic ways on tasks such as service discovery, invocation and composition. But the adoption of existed Semantic Web Services solutions is hindering by their overly complexity and high expertise demand. In this paper we propose a highly lightweight and non-intrusive method to enrich the REST Web service resources with semantic annotations to support a more autonomous Web service utilization and generic client service interaction. It is achieved by turning the service description into a semantic resource graph represented in RDF, with the added tag based semantic annotation and a small vocabulary. The method is implemented with the popular OpenAPI service description format, and illustrated by a simple use case example.","tags":["OpenAPI","REST","Semantic Annotation","Semantic Web Services","Service Discovery","Web Service Description"],"title":"Using Tag based Semantic Annotation to Empower Client and REST Service Interaction","type":"publication"},{"authors":["Cong Peng"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"bf2e6fb9722806fbbc16597bcfe691f8","permalink":"https://pcx.linkedinfo.co/publication/peng-2017/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/peng-2017/","section":"publication","summary":"","tags":["Record keeping","research ethics"],"title":"Good Record Keeping for Conducting Research Ethically Correct","type":"publication"},{"authors":["Cong Peng","Guohua Bai"],"categories":null,"content":"","date":1451602800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451602800,"objectID":"31e3a3b71b5ab915c6d140c2e759a6b2","permalink":"https://pcx.linkedinfo.co/publication/pcx-2016/","publishdate":"2016-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2016/","section":"publication","summary":"Health data sharing can benefit patients to self-manage the challenging chronic diseases out of hospital. The patient controlled electronic Personal Health Record (PHR), as a tool manages comprehensive health data, is absolutely a good entry point to share health data with multiple parties for mutual benefits in the long-term. However, sharing health data from PHR remains challenges. The sharing of health data has to be considered holistically together with the key issues such as privacy, compatibility, evolvement and so on. A PHR system should be flexible to aggregate health data of a patient from various sources to make it comprehensive and up-to-date, should be flexible to share different categories and levels of health data for various utilizations and should be flexible to embed emerging access control mechanisms to ensure privacy and security under different sceneries. Therefore, the flexibility of system architecture on the integration of existed and future diversifications is crucial for PHR's practical long-term usability. This paper discussed the necessity and some possible solution, based on the reviewed literatures and the experience from a previous study, of flexible PHR system architecture on the mentioned aspects.","tags":null,"title":"Flexible System Architecture of PHR to Support Sharing Health Data for Chronic Disease Self-Management","type":"publication"},{"authors":["Y. Hu","C. Peng","G. Bai"],"categories":null,"content":"","date":1420066800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420066800,"objectID":"4de5be7c4b3d9f4859435c10012402f6","permalink":"https://pcx.linkedinfo.co/publication/huyanpcx-2015/","publishdate":"2015-01-01T00:00:00+01:00","relpermalink":"/publication/huyanpcx-2015/","section":"publication","summary":"Nowadays, patient self-management is encouraged in home-based healthcare, especially for chronic disease care. Sharing health information could improve the quality of patient self-management. In this paper, we introduce cloud computing as a potential technology to provide a more sustainable long-term solution compared with other technologies. A hybrid cloud is identified as a suitable way to enable patients to share health information for promoting the treatment of chronic diseases. And then a prototype on the case of type 2 diabetes is implemented to prove the feasibility of the proposed solution.","tags":["Chronic disease","Hybrid cloud","cloud computing"],"title":"Sharing health data through hybrid cloud for self-management","type":"publication"},{"authors":[],"categories":[],"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"112a179f289faa12977d845909944856","permalink":"https://pcx.linkedinfo.co/post/swift-brief-intro/","publishdate":"2014-07-01T00:00:00Z","relpermalink":"/post/swift-brief-intro/","section":"post","summary":"Highly Available, distributed, eventually consistent object store","tags":["OpenStack Swift"],"title":"OpenStack Swift Brief Intro","type":"post"}]