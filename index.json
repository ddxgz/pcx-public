[{"authors":null,"categories":null,"content":"Cong Peng is a PhD student in the Department of Computer Science at Blekinge Institute of Technology (BTH). He has master’s degrees in Computer Science from BTH, and Software Engineering from Zhejiang University of Technology in China. His research interests are Semantic Web, Machine Learning, eHealth and Web services. He has been involved in the works of teaching assistant, bachelor thesis supervision and review.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pcx.linkedinfo.co/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Cong Peng is a PhD student in the Department of Computer Science at Blekinge Institute of Technology (BTH). He has master’s degrees in Computer Science from BTH, and Software Engineering from Zhejiang University of Technology in China. His research interests are Semantic Web, Machine Learning, eHealth and Web services. He has been involved in the works of teaching assistant, bachelor thesis supervision and review.","tags":null,"title":"PENG, Cong","type":"authors"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1575154800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575154800,"objectID":"f4b2a9c2e4fef84d4079ba45856b2f4f","permalink":"https://pcx.linkedinfo.co/publication/pcx-2019-b/","publishdate":"2019-12-01T00:00:00+01:00","relpermalink":"/publication/pcx-2019-b/","section":"publication","summary":"Health data integration enables a collaborative utilization of data across different systems. It not only provides a comprehensive view of a patient's health but can also potentially cope with challenges faced by the current healthcare system. In this literature review, we investigated the existing work on heterogeneous health data integration as well as the methods of utilizing the integrated health data. Our search was narrowed down to 32 articles for analysis. The integration approaches in the reviewed articles were classified into three classifications, and the utilization approaches were classified into five classifications. The topic of health data integration is still under debate and problems are far from being resolved. This review suggests the need for a more efficient way to invoke the various services for aggregating health data, as well as a more effective way to integrate the aggregated health data for supporting collaborative utilization. We have found that the combination of Web Application Programming Interface and Semantic Web technologies has the potential to cope with the challenges based on our analysis of the review result.","tags":null,"title":"A literature review of current technologies on health data integration for patient-centered health management","type":"publication"},{"authors":[],"categories":[],"content":"  This code snippet is to predict topic tags based on the text of an article. Each article could have 1 or more tags (usually have at least 1 tag), and the tags are not mutually exclusive. So this is a multi-label classification problem. It\u0026#39;s different from multi-class classification, the classes in multi-class classification are mutually exclusive, i.e., each item belongs to 1 and only 1 class.  In this snippet, we will use OneVsRestClassifier (the One-Vs-the-Rest) in scikit-learn to process the multi-label classification. The article data will be retrieved from LinkedInfo.co via Web API. The methods in this snippet should give credits to Working With Text Data - scikit-learn and this post. Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB Preprocessing data and explore the method   dataset.df_tags fetches the data set from LinkedInfo.co. It calls Web API of LinkedInfo.co to retrieve the article list, and then download and extract the full text of each article based on an article\u0026#39;s url. The tags of each article are encoded using MultiLabelBinarizer in scikit-learn. The implementation of the code could be found in dataset.py. We\u0026#39;ve set the parameter of content_length_threshold to 100 to screen out the articles with less than 100 for the description or full text. import dataset ds = dataset.df_tags(content_length_threshold=100)   The dataset contains 3353 articles by the time retrieved the data. The dataset re returned as an object with the following attribute:     ds.data: pandas.DataFrame with cols of title, description, fulltext    ds.target: encoding of tagsID    ds.target_names: tagsID    ds.target_decoded: the list of lists contains tagsID for each info    \u0026gt;\u0026gt; ds.data.head()      description fulltext title   0 Both HTTP 1.x and HTTP/2 rely on lower level c… [Stressgrid]()\\n\\n__\\n\\n[]( \u0026#34;home\u0026#34;)\\n\\n * [… Achieving 100k connections per second with Elixir   1 At Phusion we run a simple multithreaded HTTP … [![Hongli Lai](images/avatar-b64f1ad5.png)](… What causes Ruby memory bloat?   2 Have you ever wanted to contribute to a projec… [ ![Real Python](/static/real-python-logo.ab1a… Managing Multiple Python Versions With pyenv   3 安卓在版本Pie中第一次引入了ART优化配置文件，这个新特性利用发送到Play Cloud的… 安卓在版本Pie中第一次引入了[ART优化配置文件](https://youtu.be/Yi... ART云配置文件，提高安卓应用的性能   4 I work at Red Hat on GCC, the GNU Compiler Col… [ ![Red Hat\\nLogo](https://developers.redhat.c... Usability improvements in GCC 9    \u0026gt;\u0026gt; ds.target[:5] array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]])  \u0026gt;\u0026gt; ds.target_names[:5] array([\u0026#39;academia\u0026#39;, \u0026#39;access-control\u0026#39;, \u0026#39;activemq\u0026#39;, \u0026#39;aes\u0026#39;, \u0026#39;agile\u0026#39;], dtype=object)  \u0026gt;\u0026gt; ds.target_decoded[:5] [[\u0026#39;concurrency\u0026#39;, \u0026#39;elixir\u0026#39;], [\u0026#39;ruby\u0026#39;], [\u0026#39;python\u0026#39;, \u0026#39;virtualenv\u0026#39;], [\u0026#39;android\u0026#39;], [\u0026#39;gcc\u0026#39;]]   The following snippet is the actual process of getting the above dataset, by reading from file. import json import pandas as pd from sklearn.preprocessing import MultiLabelBinarizer infos_file = \u0026#39;data/infos/infos_0_3353_fulltext.json\u0026#39; with open(infos_file, \u0026#39;r\u0026#39;) as f: infos = json.load(f) content_length_threshold = 100 data_lst = [] tags_lst = [] for info in infos[\u0026#39;content\u0026#39;]: if len(info[\u0026#39;fulltext\u0026#39;]) \u0026lt; content_length_threshold: continue if len(info[\u0026#39;description\u0026#39;]) \u0026lt; content_length_threshold: continue data_lst.append({\u0026#39;title\u0026#39;: info[\u0026#39;title\u0026#39;], \u0026#39;description\u0026#39;: info[\u0026#39;description\u0026#39;], \u0026#39;fulltext\u0026#39;: info[\u0026#39;fulltext\u0026#39;]}) tags_lst.append([tag[\u0026#39;tagID\u0026#39;] for tag in info[\u0026#39;tags\u0026#39;]]) df_data = pd.DataFrame(data_lst) df_tags = pd.DataFrame(tags_lst) # fit and transform the binarizer mlb = MultiLabelBinarizer() Y = mlb.fit_transform(tags_lst) Y.shape  (3221, 560)   Now we\u0026#39;ve transformed the target (tags) but we cannot directly perform the algorithms on the text data, so we have to process and transform them into vectors. In order to do this, we will use TfidfVectorizer to preprocess, tokenize, filter stop words and transform the text data. The TfidfVectorizer implements the tf-idf (Term Frequency-Inverse Deocument Frequency) to reflect how important a word is to to a docuemnt in a collection of documents. from sklearn.feature_extraction.text import TfidfVectorizer # Use the default parameters for now, use_idf=True in default vectorizer = TfidfVectorizer() # Use the short descriptions for now for faster processing X = vectorizer.fit_transform(df_data.description) X.shape  (3221, 35506)   As mentioned in the beginning, this is a multi-label classification problem, we will use OneVsRestClassifier to tackle our problem. And firstly we will use the SVM (Support Vector Machines) with linear kernel, implemented as LinearSVC in scikit-learn, to do the classification. from sklearn.multiclass import OneVsRestClassifier from sklearn.svm import LinearSVC from sklearn.model_selection import train_test_split # Use default parameters, and train and test with small set of samples. clf = OneVsRestClassifier(LinearSVC()) from sklearn.utils import resample X_sample, Y_sample = resample( X, Y, n_samples=1000, replace=False, random_state=7) # X_sample_test, Y_sample_test = resample( # X, Y, n_samples=10, replace=False, random_state=1) X_sample_train, X_sample_test, Y_sample_train, Y_sample_test = train_test_split( X_sample, Y_sample, test_size=0.01, random_state=42) clf.fit(X_sample, Y_sample) Y_sample_pred = clf.predict(X_sample_test) # Inverse transform the vectors back to tags pred_transformed = mlb.inverse_transform(Y_sample_pred) test_transformed = mlb.inverse_transform(Y_sample_test) for (t, p) in zip(test_transformed, pred_transformed): print(f\u0026#39;tags: {t} predicted as: {p}\u0026#39;)  tags: ('javascript',) predicted as: ('javascript',) tags: ('erasure-code', 'storage') predicted as: () tags: ('mysql', 'network') predicted as: () tags: ('token',) predicted as: () tags: ('flask', 'python', 'web') predicted as: () tags: ('refactoring',) predicted as: () tags: ('emacs',) predicted as: () tags: ('async', 'javascript', 'promises') predicted as: ('async', 'javascript') tags: ('neural-networks',) predicted as: () tags: ('kubernetes',) predicted as: ('kubernetes',)   Though not very satisfied, this classifier predicted right a few tags. Next we\u0026#39;ll try to search for the best parameters for the classifier and train with fulltext of articles. Search for best model parameters for SVM with linear kernel   For the estimators TfidfVectorizer and LinearSVC, they both have many parameters could be tuned for better performance. We\u0026#39;ll the GridSearchCV to search for the best parameters with the help of Pipeline. from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split, GridSearchCV # Split the dataset into training and test set, and use fulltext of articles: X_train, X_test, Y_train, Y_test = train_test_split( df_data.fulltext, Y, test_size=0.5, random_state=42) # Build vectorizer classifier pipeline clf = Pipeline([ (\u0026#39;vect\u0026#39;, TfidfVectorizer()), (\u0026#39;clf\u0026#39;, OneVsRestClassifier(LinearSVC())), ]) # Grid search parameters, I minimized the parameter set based on previous # experience to accelerate the processing speed. # And the combination of penalty=\u0026#39;l1\u0026#39; and loss=\u0026#39;squared_hinge\u0026#39; are not supported when dual=True parameters = { \u0026#39;vect__ngram_range\u0026#39;: [(1, 2), (1, 3), (1, 4)], \u0026#39;vect__max_df\u0026#39;: [1, 0.9, 0.8, 0.7], \u0026#39;vect__min_df\u0026#39;: [1, 0.9, 0.8, 0.7, 0], \u0026#39;vect__use_idf\u0026#39;: [True, False], \u0026#39;clf__estimator__penalty\u0026#39;: [\u0026#39;l1\u0026#39;, \u0026#39;l2\u0026#39;], \u0026#39;clf__estimator__C\u0026#39;: [1, 10, 100, 1000], \u0026#39;clf__estimator__dual\u0026#39;: [False], } gs_clf = GridSearchCV(clf, parameters, cv=5, n_jobs=-1) gs_clf.fit(X_train, Y_train)  import datetime from sklearn import metrics # Predict the outcome on the testing set in a variable named y_predicted Y_predicted = gs_clf.predict(X_test) print(metrics.classification_report(Y_test, Y_predicted)) print(gs_clf.best_params_) print(gs_clf.best_score_) # Export some of the result cols cols = [ \u0026#39;mean_test_score\u0026#39;, \u0026#39;mean_fit_time\u0026#39;, \u0026#39;param_vect__ngram_range\u0026#39;, ] df_result = pd.DataFrame(gs_clf.cv_results_) df_result = df_result.sort_values(by=\u0026#39;rank_test_score\u0026#39;) df_result = df_result[cols] timestamp = datetime.now().strftime(\u0026#39;%Y-%m-%d_%H-%M-%S\u0026#39;) df_result.to_html( f\u0026#39;data/results/gridcv_results_{timestamp}_linearSVC.html\u0026#39;)   Here we attach the top-5 performed classifiers with selected parameters.   rank_test_score mean_test_score mean_fit_time param_vect__max_df param_vect__ngram_range param_vect__use_idf param_clf__estimator__penalty param_clf__estimator__C     64 1 0.140811 96.127405 0.8 (1, 4) True l1 10   70 2 0.140215 103.252332 0.7 (1, 4) True l1 10   58 2 0.140215 98.990952 0.9 (1, 4) True l1 10   154 2 0.140215 1690.433151 0.9 (1, 4) True l1 1000   68 5 0.139618 70.778621 0.7 (1, 3) True l1 10    Training and testing with the best parameters   Based on the grid search results, we found the following parameters combined with the default parameters have the best performance. Now let\u0026#39;s see how it will perform. X_train, X_test, Y_train, Y_test = train_test_split( df_data, Y, test_size=0.2, random_state=42) clf = Pipeline([ (\u0026#39;vect\u0026#39;, TfidfVectorizer(use_idf=True, max_df=0.8, ngram_range=[1, 4])), (\u0026#39;clf\u0026#39;, OneVsRestClassifier(LinearSVC(penalty=\u0026#39;l1\u0026#39;, C=10, dual=False))), ]) clf.fit(X_train.fulltext, Y_train) Y_pred = clf.predict(X_test.fulltext) # Inverse transform the vectors back to tags pred_transformed = mlb.inverse_transform(Y_pred) test_transformed = mlb.inverse_transform(Y_test) for (title, t, p) in zip(X_test.title, test_transformed, pred_transformed): print(f\u0026#39;Article title: {title} \\n\u0026#39; f\u0026#39;Manual tags: {t} \\n\u0026#39; f\u0026#39;predicted as: {p}\\n\u0026#39;)   Here below is a fraction of the list that shows the manually input tags and the predicted tags. We can see that usually the more frequently appeared and more popular tags have better change to be correctly predicted. Personally, I would say the prediction is satisfied to me comparing when I tag the articles manually. However, there\u0026#39;s much room for improvement. Article title: Will PWAs Replace Native Mobile Apps? Manual tags: (\u0026#39;pwa\u0026#39;,) predicted as: (\u0026#39;pwa\u0026#39;,) Article title: 基于Consul的分布式信号量实现 Manual tags: (\u0026#39;consul\u0026#39;, \u0026#39;distributed-system\u0026#39;) predicted as: (\u0026#39;microservices\u0026#39;, \u0026#39;multithreading\u0026#39;) Article title: commit 和 branch 理解深入 Manual tags: (\u0026#39;git\u0026#39;,) predicted as: (\u0026#39;git\u0026#39;,) Article title: Existential types in Scala Manual tags: (\u0026#39;scala\u0026#39;,) predicted as: (\u0026#39;scala\u0026#39;,) Article title: Calling back into Python from llvmlite-JITed code Manual tags: (\u0026#39;jit\u0026#39;, \u0026#39;python\u0026#39;) predicted as: (\u0026#39;compiler\u0026#39;, \u0026#39;python\u0026#39;) Article title: Writing a Simple Linux Kernel Module Manual tags: (\u0026#39;kernel\u0026#39;, \u0026#39;linux\u0026#39;) predicted as: (\u0026#39;linux\u0026#39;,) Article title: Semantic segmentation with OpenCV and deep learning Manual tags: (\u0026#39;deep-learning\u0026#39;, \u0026#39;opencv\u0026#39;) predicted as: (\u0026#39;deep-learning\u0026#39;, \u0026#39;image-classification\u0026#39;, \u0026#39;opencv\u0026#39;) Article title: Transducers: Efficient Data Processing Pipelines in JavaScript Manual tags: (\u0026#39;javascript\u0026#39;,) predicted as: (\u0026#39;javascript\u0026#39;,) Article title: C++之stl::string写时拷贝导致的问题 Manual tags: (\u0026#39;cpp\u0026#39;,) predicted as: (\u0026#39;functional-programming\u0026#39;,) Article title: WebSocket 浅析 Manual tags: (\u0026#39;websocket\u0026#39;,) predicted as: (\u0026#39;websocket\u0026#39;,) Article title: You shouldn’t name your variables after their types for the same reason you wouldn’t name your pets “dog” or “cat” Manual tags: (\u0026#39;golang\u0026#39;,) predicted as: (\u0026#39;golang\u0026#39;,) Article title: Introduction to Data Visualization using Python Manual tags: (\u0026#39;data-visualization\u0026#39;, \u0026#39;python\u0026#39;) predicted as: (\u0026#39;data-visualization\u0026#39;, \u0026#39;matplotlib\u0026#39;, \u0026#39;python\u0026#39;) Article title: How JavaScript works: A comparison with WebAssembly + why in certain cases it’s better to use it over JavaScript Manual tags: (\u0026#39;javascript\u0026#39;, \u0026#39;webassembly\u0026#39;) predicted as: (\u0026#39;javascript\u0026#39;, \u0026#39;webassembly\u0026#39;) Article title: Parsing logs 230x faster with Rust Manual tags: (\u0026#39;log\u0026#39;, \u0026#39;rust\u0026#39;) predicted as: (\u0026#39;rust\u0026#39;,) Article title: Troubleshooting Memory Issues in Java Applications Manual tags: (\u0026#39;java\u0026#39;, \u0026#39;memory\u0026#39;) predicted as: (\u0026#39;java\u0026#39;,) Article title: How to use Docker for Node.js development Manual tags: (\u0026#39;docker\u0026#39;, \u0026#39;node.js\u0026#39;) predicted as: (\u0026#39;docker\u0026#39;,)  A glance at the different evaluation metrics   Now let\u0026#39;s have a look at the evaluation metrics on the prediction performance. Evaluating multi-label classification is very different from evaluating binary classification. There\u0026#39;re quite many different evaluation methods for different situations in the model evaluation part of scikit-learn\u0026#39;s documentation. We will take a look at the ones that suit this problem.  We can start with the accuracy_score function in metrics module. As mentioned in scikit-learn documentation, in multi-label classification, a subset accuracy is 1.0 when the entire set of predicted labels for a sample matches strictly with the true label set. The equation is simple like this: $$\\operatorname{accuracy}(y, \\hat{y})=\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {minples }}-1} 1\\left(\\hat{y}_{i}=y_{i}\\right)$$ from sklearn import metrics import matplotlib.pyplot as plt metrics.accuracy_score(Y_test, Y_pred)  0.26356589147286824   The score is somehow low. But we should be noted that for this problem, an inexact match of the labels is acceptable in many cases, e.g., an article talks about the golang\u0026#39;s interface is predicted with an only label golang while it was manually labeled with golang and interface. So to my opinion, this accuracy_score is not a good evaluation metric for this problem.  Now let\u0026#39;s see the classification_report that presents averaged precision, recall and f1-score. print(metrics.classification_report(Y_test, Y_pred))       precision recall f1-score support   micro avg 0.74 0.42 0.54 1186   macro avg 0.17 0.13 0.14 1186   weighted avg 0.60 0.42 0.48 1186     Let\u0026#39;s look at the micro row. Why? Let me quote scikit-learn\u0026#39;s documentation:   \u0026#34;micro\u0026#34; gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.   Here we\u0026#39;re more interested in the average precision, which is 0.74. As we mentioned, for this problem and for me, it\u0026#39;s more important to not predict a label that should be negative to an article. Some of the labels for an article, e.g., the label interface for the just mentioned article, are less important. So I\u0026#39;m OK for having a low score of recall, which measure how good the model predicts all the labels as the manually labeled.  However, there\u0026#39;s much room for improvement. Another problem to be though about is, the training samples are not equally labeled, i.e., for the same example all the articles talking about golang\u0026#39;s interface, some of them labeled with golang + interface while some of them labeled only golang. ","date":1568201803,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568374603,"objectID":"ac7a73f355de3ce2c7a7929c89ab4b51","permalink":"https://pcx.linkedinfo.co/post/texttagspred/","publishdate":"2019-09-11T13:36:43+02:00","relpermalink":"/post/texttagspred/","section":"post","summary":"This code snippet is to predict topic tags based on the text of an article. Each article could have 1 or more tags (usually have at least 1 tag), and the tags are not mutually exclusive. So this is a multi-label classification problem. It\u0026#39;s different from multi-class classification, the classes in multi-class classification are mutually exclusive, i.e., each item belongs to 1 and only 1 class.  In this snippet, we will use OneVsRestClassifier (the One-Vs-the-Rest) in scikit-learn to process the multi-label classification.","tags":["Machine Learning","Multi-label classification","Text classification","LinkedInfo.co"],"title":"Multi-label classification to predict topic tags of technical articles from LinkedInfo.co","type":"post"},{"authors":[],"categories":[],"content":" Thanks to pmarcelino and serigne for their great work.\nThis is my second kaggle competition to practice on the knowledge of data analysis and machine learning. Unlike the Titanic competition, this house prices is a regression problem. So there will be much difference from the previous binary classification. For this competition, we will have 79 variables that describe various aspects of a house and with a price in the training data set. And then predict the prices of houses in the testing set based on the 79 variables. This will be a long journey with the 79 variables. So let\u0026rsquo;s start to explore the data with the data description. Table of Contents HAHAHUGOSHORTCODE-TOC0-HBHB\nimport os # from typing import List, Union # from pysnooper import snoop import pandas as pd # import matplotlib.pyplot as plt # import numpy as np loc = 'house price' if os.getcwd().split('/')[-1] != loc: os.chdir(loc) df_train = pd.read_csv(f'input/train.csv') df_test = pd.read_csv(f'input/test.csv')  Data exploration Let\u0026rsquo;s firstly have a look at the data we have.\nprint(df_train.shape) df_train.head()  (1460, 81)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice     0 1 60 RL 65.0 8450 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 2 2008 WD Normal 208500   1 2 20 RL 80.0 9600 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 5 2007 WD Normal 181500   2 3 60 RL 68.0 11250 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 9 2008 WD Normal 223500   3 4 70 RL 60.0 9550 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 2 2006 WD Abnorml 140000   4 5 60 RL 84.0 14260 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 12 2008 WD Normal 250000    5 rows × 81 columns\n print(df_test.shape) df_test.head()  (1459, 80)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition     0 1461 20 RH 80.0 11622 Pave NaN Reg Lvl AllPub ... 120 0 NaN MnPrv NaN 0 6 2010 WD Normal   1 1462 20 RL 81.0 14267 Pave NaN IR1 Lvl AllPub ... 0 0 NaN NaN Gar2 12500 6 2010 WD Normal   2 1463 60 RL 74.0 13830 Pave NaN IR1 Lvl AllPub ... 0 0 NaN MnPrv NaN 0 3 2010 WD Normal   3 1464 60 RL 78.0 9978 Pave NaN IR1 Lvl AllPub ... 0 0 NaN NaN NaN 0 6 2010 WD Normal   4 1465 120 RL 43.0 5005 Pave NaN IR1 HLS AllPub ... 144 0 NaN NaN NaN 0 1 2010 WD Normal    5 rows × 80 columns\n So we have 1460 rows in training set and 1459 rows in testing set. Besides the price col in the training set, both data sets have 79 cols of variables + 1 col of \u0026lsquo;Id\u0026rsquo;.\nCheck missing values Now let\u0026rsquo;s check if there is any missing value in the data.\ndef cols_missing_value(df): df_null_sum = df.isnull().sum() df_na = (df.isnull().sum() / len(df)) * 100 missing_data = pd.concat({'Missing Ratio %': df_na, 'Total': df_null_sum}, axis='columns') return missing_data.drop(missing_data[missing_data['Total'] == 0].index ).sort_values(by='Total', ascending=False) cols_missing_value(df_train)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Missing Ratio % Total     PoolQC 99.520548 1453   MiscFeature 96.301370 1406   Alley 93.767123 1369   Fence 80.753425 1179   FireplaceQu 47.260274 690   LotFrontage 17.739726 259   GarageType 5.547945 81   GarageYrBlt 5.547945 81   GarageFinish 5.547945 81   GarageQual 5.547945 81   GarageCond 5.547945 81   BsmtExposure 2.602740 38   BsmtFinType2 2.602740 38   BsmtFinType1 2.534247 37   BsmtCond 2.534247 37   BsmtQual 2.534247 37   MasVnrArea 0.547945 8   MasVnrType 0.547945 8   Electrical 0.068493 1     cols_missing_value(pd.concat((df_train[df_test.columns], df_test)))   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Missing Ratio % Total     PoolQC 99.657417 2909   MiscFeature 96.402878 2814   Alley 93.216855 2721   Fence 80.438506 2348   FireplaceQu 48.646797 1420   LotFrontage 16.649538 486   GarageFinish 5.447071 159   GarageQual 5.447071 159   GarageCond 5.447071 159   GarageYrBlt 5.447071 159   GarageType 5.378554 157   BsmtExposure 2.809181 82   BsmtCond 2.809181 82   BsmtQual 2.774923 81   BsmtFinType2 2.740665 80   BsmtFinType1 2.706406 79   MasVnrType 0.822199 24   MasVnrArea 0.787941 23   MSZoning 0.137033 4   BsmtFullBath 0.068517 2   BsmtHalfBath 0.068517 2   Functional 0.068517 2   Utilities 0.068517 2   GarageArea 0.034258 1   GarageCars 0.034258 1   Electrical 0.034258 1   KitchenQual 0.034258 1   TotalBsmtSF 0.034258 1   BsmtUnfSF 0.034258 1   BsmtFinSF2 0.034258 1   BsmtFinSF1 0.034258 1   Exterior2nd 0.034258 1   Exterior1st 0.034258 1   SaleType 0.034258 1     There are quite a lot of missing values, some cols are missing almost all of the data. We need to handle the missing values by imputation or other methods later.\nA look at distributions As we\u0026rsquo;re predicting the \u0026lsquo;SalePrice\u0026rsquo;, so we should have a look at the stats of this col.\ndf_train['SalePrice'].describe()  count 1460.000000 mean 180921.195890 std 79442.502883 min 34900.000000 25% 129975.000000 50% 163000.000000 75% 214000.000000 max 755000.000000 Name: SalePrice, dtype: float64  df_train['SalePrice'].hist(bins=30)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x11414e4a8\u0026gt;  The values of \u0026lsquo;SalePrice\u0026rsquo; does fall in a normal distribution. In general, learning algorithms benefit from standardization of the data set. So we\u0026rsquo;ll transform the target values by QuantileTransformer and TransformedTargetRegressor later when training and testing.\nNow let\u0026rsquo;s have a look at other columns\u0026rsquo; skewnesses.\nfrom scipy.stats import skew # Concat training and testing sets together to see the full picture df_all = pd.concat((df_train, df_test)).reset_index( drop=True).drop(['SalePrice'], axis='columns') numeric_cols = df_all.select_dtypes( exclude=['object', 'category']).columns # Check the skewness of the numerical cols skewed_cols = df_all[numeric_cols].apply( lambda col: skew(col)).sort_values(ascending=False) skewness = pd.DataFrame({'Skewness': skewed_cols}) skewness.head(10) skewness = skewness[abs(skewness['Skewness']) \u0026gt; 0.75] print(f'{skewness.shape[0]} skewed numerical columns.') df_all[skewness.index].hist(figsize=(14, 12))  /Users/pcx/.pyenv/versions/ml/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass 'sort=False'. To retain the current behavior and silence the warning, pass 'sort=True'. \u0026quot;\u0026quot;\u0026quot; 15 skewed numerical columns. array([[\u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x1209d3550\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x1041d86d8\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x104200c50\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x104233208\u0026gt;], [\u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120b97780\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120bc1cf8\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120bef2b0\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120c17860\u0026gt;], [\u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120c17898\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120c71358\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120f2a8d0\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120f53e48\u0026gt;], [\u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120f84400\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120fac978\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x120fd3ef0\u0026gt;, \u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x1210054a8\u0026gt;]], dtype=object)  We also need to handle the skewed variables later.\nPreprocessing data Impute missing values There are quite a lot of missing values, some cols are missing almost all of the data. Now look into the data description to see what the variables really are and how should we deal with them. We\u0026rsquo;re now concating the training set and testing set since we need to handle the missing values in both data sets. We will split them when we need.\n# keep Id col for later unpack training and testing df ids_train = df_train['Id'] ids_test = df_test['Id'] Y_train = df_train['SalePrice'].values df_all = pd.concat((df_train, df_test)).reset_index( drop=True).drop(['SalePrice'], axis='columns')  /Users/pcx/.pyenv/versions/ml/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass 'sort=False'. To retain the current behavior and silence the warning, pass 'sort=True'.  \u0026lsquo;PoolQC\u0026rsquo; (Pool quality) is the one with most missing values, and NA stands for \u0026ldquo;No Pool\u0026rdquo; (described in data_description.txt), so the missing values should be replaced by str \u0026ldquo;No Pool\u0026rdquo;. And this col should be an ordered categorical variable.\ndf_all['PoolQC'] = df_all['PoolQC'].fillna(\u0026quot;No Pool\u0026quot;)  The same applies to \u0026lsquo;MiscFeature\u0026rsquo;, \u0026lsquo;Alley\u0026rsquo;, \u0026lsquo;Fence\u0026rsquo;, \u0026lsquo;FireplaceQu\u0026rsquo;, \u0026lsquo;GarageType\u0026rsquo;, \u0026lsquo;GarageFinish\u0026rsquo;, \u0026lsquo;GarageQual\u0026rsquo;, \u0026lsquo;GarageCond\u0026rsquo;, \u0026lsquo;BsmtQual\u0026rsquo;, \u0026lsquo;BsmtCond\u0026rsquo;, \u0026lsquo;BsmtExposure\u0026rsquo;, \u0026lsquo;BsmtFinType1\u0026rsquo;, \u0026lsquo;BsmtFinType2\u0026rsquo;, \u0026lsquo;MasVnrType\u0026rsquo;\ndf_all['MiscFeature'] = df_all['MiscFeature'].fillna(\u0026quot;None\u0026quot;) df_all['Alley'] = df_all['Alley'].fillna(\u0026quot;No Alley access\u0026quot;) df_all['Fence'] = df_all['Fence'].fillna(\u0026quot;No Fence\u0026quot;) df_all['FireplaceQu'] = df_all['FireplaceQu'].fillna(\u0026quot;No Fireplace\u0026quot;) df_all['GarageType'] = df_all['GarageType'].fillna(\u0026quot;No Garage\u0026quot;) df_all['GarageFinish'] = df_all['GarageFinish'].fillna(\u0026quot;No Garage\u0026quot;) df_all['GarageQual'] = df_all['GarageQual'].fillna(\u0026quot;No Garage\u0026quot;) df_all['GarageCond'] = df_all['GarageCond'].fillna(\u0026quot;No Garage\u0026quot;) df_all['BsmtCond'] = df_all['BsmtCond'].fillna(\u0026quot;No Basement\u0026quot;) df_all['BsmtQual'] = df_all['BsmtQual'].fillna(\u0026quot;No Basement\u0026quot;) df_all['BsmtExposure'] = df_all['BsmtExposure'].fillna(\u0026quot;No Basement\u0026quot;) df_all['BsmtFinType1'] = df_all['BsmtFinType1'].fillna(\u0026quot;No Basement\u0026quot;) df_all['BsmtFinType2'] = df_all['BsmtFinType2'].fillna(\u0026quot;No Basement\u0026quot;)  Now let\u0026rsquo;s check \u0026lsquo;GarageYrBlt\u0026rsquo;, \u0026lsquo;GarageArea\u0026rsquo;, \u0026lsquo;GarageCars\u0026rsquo;. Since only 1 record of \u0026lsquo;GarageCars\u0026rsquo; is missing, and it\u0026rsquo;s \u0026lsquo;GarageType\u0026rsquo; is \u0026lsquo;Detchd\u0026rsquo;, so let\u0026rsquo;s make it as size of the mode/median of \u0026lsquo;GarageCars\u0026rsquo; when type is \u0026lsquo;Detchd\u0026rsquo;.\ndf_all[df_all['GarageCars'].isnull()] df_all[df_all['GarageCars'].isnull()]['GarageType'] df_all['GarageCars'] = df_all['GarageCars'].fillna( int(df_all[df_all['GarageType'] == 'Detchd']['GarageCars'].mode()))  It\u0026rsquo;s the same record for the missing \u0026lsquo;GarageArea\u0026rsquo; value, as we filled its \u0026lsquo;GarageCars\u0026rsquo; to the mode value, we will fill the area as the mean value of \u0026lsquo;GarageArea\u0026rsquo; where the \u0026lsquo;GarageCars\u0026rsquo; == mode value of \u0026lsquo;Detchd\u0026rsquo;.\ndf_all[df_all['GarageArea'].isnull()] df_all['GarageArea'] = df_all['GarageArea'].fillna( df_all[df_all['GarageType'] == 'Detchd']['GarageArea'].mean()) # df_all[df_all['GarageYrBlt'].isnull()]['GarageType']  For the records that have no garage, we set the null value of \u0026lsquo;GarageYrBlt\u0026rsquo; to 0, but for the records with type \u0026lsquo;Detchd\u0026rsquo;, we set the null value to the median value of the built year with type \u0026lsquo;Detchd\u0026rsquo;.\nyear_median = df_all[df_all['GarageType'] == 'Detchd']['GarageYrBlt'].median() df_all['GarageYrBlt'] = df_all['GarageYrBlt'][ df_all['GarageType'] == 'Detchd'].fillna(year_median) df_all['GarageYrBlt'] = df_all['GarageYrBlt'].fillna(0)  Since there are quite many missing value for \u0026lsquo;LotFrontage\u0026rsquo; (16.65%), we would drop this col.\ndf_all = df_all.drop('LotFrontage', axis='columns')  Filling with 0 for those likely to be 0.\nbsmt_zero_missing = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'] for col in bsmt_zero_missing: df_all[col] = df_all[col].fillna(0)  \u0026lsquo;MasVnrArea\u0026rsquo; and \u0026lsquo;MasVnrType\u0026rsquo;\ndf_all[df_all['MasVnrType'].isnull()]['MasVnrArea'] df_all['MasVnrType'].astype('category').value_counts()  None 1742 BrkFace 879 Stone 249 BrkCmn 25 Name: MasVnrType, dtype: int64  For all the records with missing values of \u0026lsquo;MasVnrType\u0026rsquo;, 1 record with \u0026lsquo;MasVnrArea\u0026rsquo; is not NaN, so we filling its type as \u0026lsquo;BrkFace\u0026rsquo;, which is the most occurred none-None type. Other missing values of \u0026lsquo;MasVnrType\u0026rsquo; we will fill in with the most common None, so its \u0026lsquo;MasVnrArea\u0026rsquo; will be 0.\ndf_all['MasVnrType'] = df_all['MasVnrType'][ df_all['MasVnrArea'].notna()].fillna('BrkFace') df_all['MasVnrType'] = df_all['MasVnrType'].fillna('None') df_all['MasVnrArea'] = df_all['MasVnrArea'].fillna(0)  Set the NaN to the mostly occurred value \u0026lsquo;RL\u0026rsquo;.\ndf_all['MSZoning'].astype('category').value_counts() df_all['MSZoning'] = df_all['MSZoning'].fillna('RL')  # Set the NaN to the mostly occurred value 'AllPub'. df_all['Utilities'].astype('category').value_counts() df_all['Utilities'] = df_all['Utilities'].fillna('AllPub') # keep or not? df_all = df_all.drop(['Utilities'], axis='columns')  Set NaN to mostly occurred value for the rest cols.\ncols_nan_mode = ['Functional', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'MSSubClass'] for col in cols_nan_mode: df_all[col] = df_all[col].fillna(df_all[col].mode()[0]) cols_missing_value(df_all)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Missing Ratio % Total       Now there\u0026rsquo;s no missing values. Let\u0026rsquo;s move to the next part.\nTransform categorical variables We\u0026rsquo;ll firstly transform some of the variables from numerical to categorical as they should be. And add one variable.\ncols_num_cat = ['MSSubClass', 'YrSold', 'MoSold'] for col in cols_num_cat: df_all[col] = df_all[col].astype('category') # Adding total sqfootage feature df_all['TotalSF'] = df_all['TotalBsmtSF'] + \\ df_all['1stFlrSF'] + df_all['2ndFlrSF']  Check and handle outliers After handling the missing values, now we have a look at if there are outliers in the training set with the target variable by scatter plots.\nimport matplotlib.pyplot as plt df_train = df_all[:len(ids_train)] df_test = df_all[len(ids_train):] cols = df_train.select_dtypes(['int64', 'float64']) # cols = df_train.select_dtypes(['int64', 'float64']) df_train = pd.concat([df_train, pd.DataFrame( Y_train, columns=['SalePrice'])], axis='columns') fig, axes = plt.subplots(6, 6, figsize=(30, 30)) for i, col in enumerate(cols): df_train.plot.scatter(x=col, y='SalePrice', ax=axes[i // 6, i % 6])  The continuous variable \u0026lsquo;GrLivArea\u0026rsquo; seems having 2 values have very different \u0026ldquo;hehavior\u0026rdquo;. The 2 bottom right dots may be very inferential that have quite big areas but low prices. Let\u0026rsquo;s remove them to see if it\u0026rsquo;s better for the results. After removing these 2 rows, we would see that outliers in other cols such \u0026lsquo;TotalBsmtSF\u0026rsquo; and \u0026lsquo;TotalSF\u0026rsquo; are disappeared as well.\ndf_train = df_train.drop(df_train[(df_train['GrLivArea'] \u0026gt; 4000) \u0026amp; (df_train['SalePrice'] \u0026lt; 250000)].index)  # Packing back data sets after removing outliers in training set. ids_train = df_train['Id'] ids_test = df_test['Id'] Y_train = df_train['SalePrice'].values df_all = pd.concat((df_train, df_test)).reset_index( drop=True).drop(['SalePrice'], axis='columns')  Transform skewed variables We will transform the skewed variables into normal distributions by quantile_transform.\nnumeric_cols = df_all.select_dtypes( exclude=['object', 'category']).columns # Check the skewnesses of the numerical cols skewed_cols = df_all[numeric_cols].apply( lambda col: skew(col)).sort_values(ascending=False) skewness = pd.DataFrame({'Skewness': skewed_cols}) skewness = skewness[abs(skewness['Skewness']) \u0026gt; 0.75] print(f'{skewness.shape[0]} skewed numerical columns.') from sklearn.preprocessing import quantile_transform import numpy as np skewed_features = skewness.index df_all[skewed_features] = quantile_transform( df_all[skewed_features], output_distribution='normal', copy=True)  20 skewed numerical columns.  # Check again for the skewnesses of the numerical cols skewed_cols = df_all[numeric_cols].apply( lambda col: skew(col)).sort_values(ascending=False) skewness = pd.DataFrame({'Skewness': skewed_cols}) skewness = skewness[abs(skewness['Skewness']) \u0026gt; 0.75] print(f'{skewness.shape[0]} skewed numerical columns.')  11 skewed numerical columns.  Encode categorical valuee Transform categorical cols by using pd.get_dummies().\nprint(df_all.shape) # Column names in the DataFrame to be encoded. If columns is None then all the # columns with object or category dtype will be converted. df_all = pd.get_dummies(df_all) print(df_all.shape)  (2917, 79) (2917, 330)  Training and testing Base model Now we will start to train and test with a base model with default parameters to see how it would perform as a base line. Root-Mean-Squared-Error (RMSE) as the evaluation metric for the competition, the equation is:\n$$\\operatorname{RMSE}(y, \\hat{y})=\\sqrt{\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {symples }}-1}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}$$.\n# Unpack training and testing data sets df_train = df_all[:len(ids_train)].drop(['Id'], axis='columns') df_test = df_all[len(ids_train):].drop(['Id'], axis='columns') X_train = df_train.values X_test = df_test  import numpy as np from sklearn.pipeline import Pipeline from sklearn.linear_model import Lasso, ElasticNet, Ridge from sklearn.model_selection import cross_val_score from sklearn.metrics import mean_squared_error, make_scorer from sklearn.compose import TransformedTargetRegressor from sklearn.preprocessing import QuantileTransformer Y_train_norm = np.log1p(Y_train) # there's no implementation of RMSE in the scikit-learn library, so we have to # define a scorer of RMSE def rmse_cal(y_true, y_pred): return np.sqrt(mean_squared_error(y_true, y_pred)) # return np.sqrt(np.sum(np.square(y_pred - y_true)) / len(y_pred)) # if the custom score function is a loss (greater_is_better=False), the output # of the python function is negated by the scorer object, conforming to the # cross validation convention that scorers return higher values for better # models. rmse = make_scorer(rmse_cal, greater_is_better=False) # ridgepip = Pipeline([ # ('tran', TransformedTargetRegressor( # regressor=Lasso(), func=np.log1p, inverse_func=np.expm1)), # ('tran', TransformedTargetRegressor( # regressor=Ridge(), func=np.log1p, inverse_func=np.expm1)), # ]) models = [ Lasso(), # ridgepip, # # ElasticNet(), Ridge(), ] CV = 5 for m in models: scores = -cross_val_score(m, X_train, Y_train_norm, scoring=rmse, cv=5, n_jobs=-1) print(f'{type(m).__name__}\\n' f'Scores: {scores}\\n' # +/-std*2 for 95% confidence interval f'Accuracy: {scores.mean(): 0.4f} (+/-{scores.std() * 2: 0.4f})\\n' f'{\u0026quot;-\u0026quot;*20}')  Lasso Scores: [0.22425222 0.23934427 0.23998284 0.24165163 0.23227816] Accuracy: 0.2355 (+/- 0.0129) -------------------- Ridge Scores: [0.11456344 0.12197379 0.13560006 0.1083432 0.1172416 ] Accuracy: 0.1195 (+/- 0.0183) --------------------  GridSearch for best model with best parameters The base models give somehow good results. The CV RMSE score of the /Ridge/ model is around the top-1000 in the competition\u0026rsquo;s leaderboard. Now let\u0026rsquo;s try to find the best parameters for these and other models with GridSearchCV.\nfrom sklearn.svm import SVR from sklearn.pipeline import Pipeline from sklearn.preprocessing import RobustScaler from sklearn.kernel_ridge import KernelRidge from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.ensemble import GradientBoostingRegressor from sklearn import metrics Y_train_norm = np.log1p(Y_train) X_train_cv, X_test_cv, Y_train_cv, Y_test_cv = train_test_split( X_train, Y_train_norm, test_size=0.3) param_space = { 'rob_lasso': { 'model': Pipeline([('sca', RobustScaler()), ('model', Lasso())]), 'params': { 'model__alpha': [0.00005, 0.0004, 0.0005, 0.0007, 0.005, 0.05, 0.5, 0.8, 1], } }, 'ridge': { 'model': Ridge(), 'params': { 'alpha': [1e-3, 1e-2, 1e-1, 1, 10], } }, 'kernel_ridge': { 'model': KernelRidge(), 'params': { 'alpha': [1e-3, 1e-2, 1e-1, 1, 10], } }, 'elastic_net': { 'model': Pipeline([('sca', RobustScaler()), ('model', ElasticNet())]), 'params': { 'model__alpha': [0.00005, 0.0004, 0.0005, 0.0007, 0.005, 0.05, 0.5, 0.8, 1], # Note that a good choice of list of values for l1_ratio is often to # put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. # Ridge) 'model__l1_ratio': [.1, .5, .7, .75, .8, .85, .9, .95, .97, .99, .995, 1], } }, # 'gboost': { # 'model': GradientBoostingRegressor(), # 'params': { # 'loss': ['ls', 'lad', 'huber', 'quantile'], # 'learning_rate': [0.01, 0.1], # 'n_estimators': [100, 500, 1000, 3000], # 'max_depth': [2, 3, 4], # 'min_samples_split': [2, 5, 10], # } # }, # 'svr': { # 'model': SVR(), # 'params': { # 'kernel': ['linear', 'rbf'], # 'C': [1, 10], # } # }, } gs_rec = [] # grid search parameters for name, pair in param_space.items(): print(f'{name}---------------') gs_rg = GridSearchCV(pair['model'], pair['params'], scoring=rmse, cv=CV, error_score=0, n_jobs=-1) gs_rg.fit(X_train, Y_train_norm) print(gs_rg.best_params_) print(gs_rg.best_score_) gs_rg_cv = GridSearchCV(pair['model'], pair['params'], scoring=rmse, cv=CV, error_score=0, n_jobs=-1) gs_rg_cv.fit(X_train_cv, Y_train_cv) pred_test = gs_rg_cv.predict(X_test_cv) y_score = rmse_cal(Y_test_cv, pred_test) print(gs_rg_cv.best_params_) print(gs_rg_cv.best_score_) print(y_score) gs_rec.append({ 'name': name, 'params': gs_rg.best_params_, 'score': -gs_rg.best_score_, 'cv_test_params': gs_rg_cv.best_params_, 'cv_test_score': y_score }) df_gs = pd.DataFrame(gs_rec, columns=['name', 'score', 'params', 'cv_test_score', 'cv_test_params'] ).sort_values(by=['score', 'cv_test_score']) df_gs  rob_lasso--------------- {'model__alpha': 0.0005} -0.1108321642082426 {'model__alpha': 0.0005} -0.11385591248537665 0.1092651116732159 ridge--------------- {'alpha': 10} -0.11417733254437629 {'alpha': 10} -0.11723423641202352 0.11022009984391984 kernel_ridge--------------- {'alpha': 10} -0.11675117173959225 {'alpha': 10} -0.1209044169077714 0.11171230919473786 elastic_net--------------- {'model__alpha': 0.0005, 'model__l1_ratio': 0.9} -0.11081242246612653 {'model__alpha': 0.0007, 'model__l1_ratio': 0.8} -0.1138195082928615 0.10934894252124043   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    name score params cv_test_score cv_test_params     3 elastic_net 0.110812 {'model__alpha': 0.0005, 'model__l1_ratio': 0.9} 0.109349 {'model__alpha': 0.0007, 'model__l1_ratio': 0.8}   0 rob_lasso 0.110832 {'model__alpha': 0.0005} 0.109265 {'model__alpha': 0.0005}   1 ridge 0.114177 {'alpha': 10} 0.110220 {'alpha': 10}   2 kernel_ridge 0.116751 {'alpha': 10} 0.111712 {'alpha': 10}     Now let\u0026rsquo;s Train with the best model so far and predict on the test data. As aforementioned, the values of \u0026lsquo;SalePrice\u0026rsquo; does fall in a normal distribution. So we\u0026rsquo;ll transform the target values by QuantileTransformer and TransformedTargetRegressor.\nfrom datetime import datetime # model = Pipeline( # [('sca', RobustScaler()), ('model', TransformedTargetRegressor( # regressor=ElasticNet(alpha=0.0005, l1_ratio=0.85), func=np.log1p, inverse_func=np.expm1))]) model = Pipeline( [('sca', RobustScaler()), ('model', TransformedTargetRegressor( regressor=ElasticNet(alpha=0.0005, l1_ratio=0.85), # regressor=Lasso(alpha=0.0005), transformer=QuantileTransformer(output_distribution='normal')))]) model.fit(X_train, Y_train) pred = model.predict(X_test) def submit(ids, pred, suffix): sub = pd.DataFrame() sub['Id'] = ids_test sub['SalePrice'] = pred timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S') # sub.to_csv( # f'result/kaggle1_sub_{suffix}_{score:.5f}.csv', index=False) sub.to_csv( f'submissions/{suffix}_{timestamp}.csv.gz', index=False, compression='gzip') submit(ids_test, pred, 'elastic_net')  ","date":1560267513,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568405173,"objectID":"1aaef6f96a32e5ce264193307b750ff7","permalink":"https://pcx.linkedinfo.co/post/houseprice/","publishdate":"2019-06-11T17:38:33+02:00","relpermalink":"/post/houseprice/","section":"post","summary":"Thanks to pmarcelino and serigne for their great work.\nThis is my second kaggle competition to practice on the knowledge of data analysis and machine learning. Unlike the Titanic competition, this house prices is a regression problem. So there will be much difference from the previous binary classification. For this competition, we will have 79 variables that describe various aspects of a house and with a price in the training data set.","tags":["Machine Learning","Regression"],"title":"Explore the house prices kaggle competition","type":"post"},{"authors":["Cong Peng","Prashant Goswami"],"categories":null,"content":"","date":1554069600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554069600,"objectID":"60d09f5ebc761287248b9f096f0928f1","permalink":"https://pcx.linkedinfo.co/publication/pcx-2019-a/","publishdate":"2019-04-01T00:00:00+02:00","relpermalink":"/publication/pcx-2019-a/","section":"publication","summary":"The development of electronic health records, wearable devices, health applications and Internet of Things (IoT)-empowered smart homes is promoting various applications. It also makes health self-management much more feasible, which can partially mitigate one of the challenges that the current healthcare system is facing. Effective and convenient self-management of health requires the collaborative use of health data and home environment data from different services, devices, and even open data on the Web. Although health data interoperability standards including HL7 Fast Healthcare Interoperability Resources (FHIR) and IoT ontology including Semantic Sensor Network (SSN) have been developed and promoted, it is impossible for all the different categories of services to adopt the same standard in the near future. This study presents a method that applies Semantic Web technologies to integrate the health data and home environment data from heterogeneously built services and devices. We propose a Web Ontology Language (OWL)-based integration ontology that models health data from HL7 FHIR standard implemented services, normal Web services and Web of Things (WoT) services and Linked Data together with home environment data from formal ontology-described WoT services. It works on the resource integration layer of the layered integration architecture. An example use case with a prototype implementation shows that the proposed method successfully integrates the health data and home environment data into a resource graph. The integrated data are annotated with semantics and ontological links, which make them machine-understandable and cross-system reusable.","tags":["FHIR","REST","Semantic Web","Web service","WoT","eHealth","health data integration","ontology","smart homes"],"title":"Meaningful Integration of Data from Heterogeneous Health Services and Home Environment Based on Ontology","type":"publication"},{"authors":null,"categories":null,"content":"The Web should be an open web. All the informations published on the Web are meant to be shared, share through links by search engines, rss, social networks, etc. This site is yet another method that tries to link all the informations (but starts with only technical articles on LinkedInfo) and share them.\nThe original idea of this side project is to utilize Semantic Web technologies and Machine learning to link the informations. Noble ambition shall start from basic, it needs to be improved little by little.\n","date":1548584274,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548584274,"objectID":"6268ed86f6986d5dbdacded5a30b3972","permalink":"https://pcx.linkedinfo.co/project/linkedinfo/","publishdate":"2019-01-27T11:17:54+01:00","relpermalink":"/project/linkedinfo/","section":"project","summary":"The Web should be an open web, all the informations published on the Web are meant to be shared. Linkedinfo.co is a Web service uses Semantic Web technologies and Machine Learning to link and share technical articles on the Web.","tags":["Semantic Web","Web technologies","Machine Learning"],"title":"LinkedInfo.co","type":"project"},{"authors":["Cong Peng"],"categories":null,"content":"","date":1546297200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546297200,"objectID":"8ca333643ce6917c513fec55c6c10a0c","permalink":"https://pcx.linkedinfo.co/publication/peng-2019/","publishdate":"2019-01-01T00:00:00+01:00","relpermalink":"/publication/peng-2019/","section":"publication","summary":"Group work-based learning is encouraged in higher education on account of both ped-agogical benefits and industrial employers's requirements. However, although a plenty of studies have been performed, there are still various factors that will affect students' group work-based learning in practice. It is important for the teachers to understand which factors are influenceable and what can be done to influence. This paper performs a literature review to identify the factors that has been investigated and reported in journal articles. Fifteen journal articles were found relevant and fifteen factors were identified, which could be influenced by instructors directly or indirectly. However, more evidence is needed to support the conclusion of some studies since they were performed only in one single course. Therefore, more studies are required on this topic to investigate the factors in different subject areas.","tags":["Group work","TBL","collaborative learning","teamwork"],"title":"What Can Teachers Do to Make the Group Work Learning Effective - a Literature Review","type":"publication"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1538344800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538344800,"objectID":"9706bb40671e2c103cc24202b650b5b7","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-d/","publishdate":"2018-10-01T00:00:00+02:00","relpermalink":"/publication/pcx-2018-d/","section":"publication","summary":"Effective and convenient self-management of health requires collaborative utilization of health data from different services provided by healthcare providers, consumer-facing products and even open data on the Web. Although health data interoperability standards include Fast Healthcare Interoperability Resources (FHIR) have been developed and promoted, it is impossible for all the different categories of services to adopt in the near future. The objective of this study aims to apply Semantic Web technologies to integrate the health data from heterogeneously built services. We present an Web Ontology Language (OWL)-based ontology that models together health data from FHIR standard implemented services, normal Web services and Linked Data. It works on the resource integration layer of the presented layered integration architecture. An example use case that demonstrates how this method integrates the health data into a linked semantic health resource graph with the proposed ontology is presented.","tags":["FHIR","Health data integration","REST","Semantic Web","Web service","eHealth","ontology"],"title":"An Ontological Approach to Integrate Health Resources from Different Categories of Services","type":"publication"},{"authors":[],"categories":[],"content":"","date":1515554369,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516245569,"objectID":"a0b16b66c87372eb421e9e35ae610b5b","permalink":"https://pcx.linkedinfo.co/post/web-notes/","publishdate":"2018-01-10T05:19:29+02:00","relpermalink":"/post/web-notes/","section":"post","summary":"Typically, the term “Web services” is used to label the older, XML-based interfaces on top of HTTP. The term “Web APIs” is more fashionable for interfaces that use HTTP and JSON. ","tags":["Web","Semantic Web"],"title":"Web Notes","type":"post"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"2979f299debfb142891b565c7962abe8","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-c/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-c/","section":"publication","summary":"The vast amount of Web services brings the problem of discovering desired services for composition and orchestration. The syntactic service matching methods based on the classical set theory have a difficulty to capture the imprecise information. Therefore, an approximate service matching approach based on fuzzy control is explored in this paper. A service description matching model to the OpenAPI specification, which is the most widely used standard for describing the defacto REST Web services, is proposed to realize the fuzzy service matching with the fuzzy inference method developed by Mamdani and Assilian. An evaluation shows that the fuzzy service matching approach performed slightly better than the weighted approach in the setting context.","tags":null,"title":"Fuzzy Matching of OpenAPI Described REST Services","type":"publication"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"07ecb044ca58895dc86215b3dd2d5afb","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-b/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-b/","section":"publication","summary":"Various health Web services host a huge amount of health data about patients. The heterogeneity of the services hinders the collaborative utilization of these health data, which can provide a valuable support for the self-management of chronic diseases. The combination of REST Web services and Semantic Web technologies has proven to be a viable approach to address the problem. This paper proposes a method to add semantic annotations to the REST Web services. The service descriptions and the resource representations with semantic annotations can be transformed into a resource graph. It integrates health data from different services, and can link to the health-domain ontologies and Linked Open Health Data to support health management and imaginative applications. The feasibility of our method is demonstrated by realizing with OpenAPI service description and JSON-LD representation in an example use case.","tags":null,"title":"Linking Health Web Services as Resource Graph by Semantic REST Resource Tagging","type":"publication"},{"authors":["Cong Peng","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"bd953f4e0ce85f1fbf6d4b9aa345c53d","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-a/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-a/","section":"publication","summary":"The utilization of Web services is becoming a human labor consuming work as the rapid growth of Web. The semantic annotated service description can support more automatic ways on tasks such as service discovery, invocation and composition. But the adoption of existed Semantic Web Services solutions is hindering by their overly complexity and high expertise demand. In this paper we propose a highly lightweight and non-intrusive method to enrich the REST Web service resources with semantic annotations to support a more autonomous Web service utilization and generic client service interaction. It is achieved by turning the service description into a semantic resource graph represented in RDF, with the added tag based semantic annotation and a small vocabulary. The method is implemented with the popular OpenAPI service description format, and illustrated by a simple use case example.","tags":["OpenAPI","REST","Semantic Annotation","Semantic Web Services","Service Discovery","Web Service Description"],"title":"Using Tag based Semantic Annotation to Empower Client and REST Service Interaction","type":"publication"},{"authors":null,"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"139f78476b81d743eb7410727891ddd5","permalink":"https://pcx.linkedinfo.co/post-old/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/post-old/","section":"","summary":"Hello!","tags":null,"title":"Posts","type":"widget_page"},{"authors":["Cong Peng"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"bf2e6fb9722806fbbc16597bcfe691f8","permalink":"https://pcx.linkedinfo.co/publication/peng-2017/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/peng-2017/","section":"publication","summary":"","tags":["Record keeping","research ethics"],"title":"Good Record Keeping for Conducting Research Ethically Correct","type":"publication"},{"authors":["Cong Peng","Guohua Bai"],"categories":null,"content":"","date":1451602800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451602800,"objectID":"31e3a3b71b5ab915c6d140c2e759a6b2","permalink":"https://pcx.linkedinfo.co/publication/pcx-2016/","publishdate":"2016-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2016/","section":"publication","summary":"Health data sharing can benefit patients to self-manage the challenging chronic diseases out of hospital. The patient controlled electronic Personal Health Record (PHR), as a tool manages comprehensive health data, is absolutely a good entry point to share health data with multiple parties for mutual benefits in the long-term. However, sharing health data from PHR remains challenges. The sharing of health data has to be considered holistically together with the key issues such as privacy, compatibility, evolvement and so on. A PHR system should be flexible to aggregate health data of a patient from various sources to make it comprehensive and up-to-date, should be flexible to share different categories and levels of health data for various utilizations and should be flexible to embed emerging access control mechanisms to ensure privacy and security under different sceneries. Therefore, the flexibility of system architecture on the integration of existed and future diversifications is crucial for PHR's practical long-term usability. This paper discussed the necessity and some possible solution, based on the reviewed literatures and the experience from a previous study, of flexible PHR system architecture on the mentioned aspects.","tags":null,"title":"Flexible System Architecture of PHR to Support Sharing Health Data for Chronic Disease Self-Management","type":"publication"},{"authors":["Y. Hu","C. Peng","G. Bai"],"categories":null,"content":"","date":1420066800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420066800,"objectID":"4de5be7c4b3d9f4859435c10012402f6","permalink":"https://pcx.linkedinfo.co/publication/huyanpcx-2015/","publishdate":"2015-01-01T00:00:00+01:00","relpermalink":"/publication/huyanpcx-2015/","section":"publication","summary":"Nowadays, patient self-management is encouraged in home-based healthcare, especially for chronic disease care. Sharing health information could improve the quality of patient self-management. In this paper, we introduce cloud computing as a potential technology to provide a more sustainable long-term solution compared with other technologies. A hybrid cloud is identified as a suitable way to enable patients to share health information for promoting the treatment of chronic diseases. And then a prototype on the case of type 2 diabetes is implemented to prove the feasibility of the proposed solution.","tags":["Chronic disease","Hybrid cloud","cloud computing"],"title":"Sharing health data through hybrid cloud for self-management","type":"publication"},{"authors":[],"categories":[],"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"112a179f289faa12977d845909944856","permalink":"https://pcx.linkedinfo.co/post/swift-brief-intro/","publishdate":"2014-07-01T00:00:00Z","relpermalink":"/post/swift-brief-intro/","section":"post","summary":"Highly Available, distributed, eventually consistent object store","tags":["OpenStack Swift"],"title":"OpenStack Swift Brief Intro","type":"post"}]