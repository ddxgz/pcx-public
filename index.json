[{"authors":null,"categories":null,"content":"Cong Peng is a PhD student in the Department of Creative Technologies at Blekinge Institute of Technology (BTH). He has master’s degrees in Computer Science from BTH, and Software Engineering from Zhejiang University of Technology in China. His research interests are Semantic Web, Machine Learning, eHealth and Web services. He has been involved in the works of teaching assistant, bachelor thesis supervision and review.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pcx.linkedinfo.co/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Cong Peng is a PhD student in the Department of Creative Technologies at Blekinge Institute of Technology (BTH). He has master’s degrees in Computer Science from BTH, and Software Engineering from Zhejiang University of Technology in China. His research interests are Semantic Web, Machine Learning, eHealth and Web services. He has been involved in the works of teaching assistant, bachelor thesis supervision and review.","tags":null,"title":"PENG, Cong","type":"authors"},{"authors":[],"categories":[],"content":" This code snippet is to predict topic tags based on the text of an article. Each article could have 1 or more tags (usually have at least 1 tag), and the tags are not mutually exclusive. So this is a multi-label classification problem. It\u0026rsquo;s different from multi-class classification, the classes in multi-class classification are mutually exclusive, i.e., each item belongs to 1 and only 1 class.\nIn this snippet, we will use OneVsRestClassifier (the One-Vs-the-Rest) in scikit-learn to process the multi-label classification. The article data will be retrieved from LinkedInfo.co via Web API.\n Preprocessing data and explore the method Search for best model include SVM and Neural Networks (Will be updated) Training and testing with the best parameters  The methods in this snippet should give credits to Working With Text Data - scikit-learn\nPreprocessing data and explore the method dataset.df_tags fetches the data set from LinkedInfo.co. It calls Web API of LinkedInfo.co to retrieve the article list, and then download and extract the full text of each article based on an article\u0026rsquo;s url. The tags of each article are encoded using MultiLabelBinarizer in scikit-learn. The implementation of the code could be found in dataset.py. We\u0026rsquo;ve set the parameter of content_length_threshold to 100 to screen out the articles with less than 100 for the description or full text.\nimport dataset ds = dataset.df_tags(content_length_threshold=100)  The dataset contains 3353 articles by the time retrieved the data. The dataset re returned as an object with the following attribute:\n  ds.data: pandas.DataFrame with cols of title, description, fulltext ds.target: encoding of tagsID ds.target_names: tagsID ds.target_decoded: the list of lists contains tagsID for each info   ds.data.head() ds.target[:5] ds.target_names[:5] ds.target_decoded[:5]  The following snippet is the actual process of getting the above dataset, by reading from file.\nimport json import pandas as pd from sklearn.preprocessing import MultiLabelBinarizer infos_file = 'data/infos/infos_0_3353_fulltext.json' with open(infos_file, 'r') as f: infos = json.load(f) content_length_threshold = 100 data_lst = [] tags_lst = [] for info in infos['content']: if len(info['fulltext']) \u0026lt; content_length_threshold: continue if len(info['description']) \u0026lt; content_length_threshold: continue data_lst.append({'title': info['title'], 'description': info['description'], 'fulltext': info['fulltext']}) tags_lst.append([tag['tagID'] for tag in info['tags']]) df_data = pd.DataFrame(data_lst) df_tags = pd.DataFrame(tags_lst) # fit and transform the binarizer mlb = MultiLabelBinarizer() Y = mlb.fit_transform(tags_lst) Y.shape  Now we\u0026rsquo;ve transformed the target (tags) but we cannot directly perform the algorithms on the text data, so we have to process and transform them into vectors. In order to do this, we will use TfidfVectorizer to preprocess, tokenize, filter stop words and transform the text data. The TfidfVectorizer implements the tf-idf (Term Frequency-Inverse Deocument Frequency) to reflect how important a word is to to a docuemnt in a collection of documents.\nfrom sklearn.feature_extraction.text import TfidfVectorizer # Use the default parameters for now, use_idf=True in default vectorizer = TfidfVectorizer() # Use the short descriptions for now for faster processing X = vectorizer.fit_transform(df_data.description) X.shape  As mentioned in the beginning, this is a multi-label classification problem, we will use OneVsRestClassifier to tackle our problem. And firstly we will use the SVM (Support Vector Machines) with linear kernel, implemented as LinearSVC in scikit-learn, to do the classification.\nfrom sklearn.multiclass import OneVsRestClassifier from sklearn.svm import LinearSVC from sklearn.model_selection import train_test_split # Use default parameters, and train and test with small set of samples. clf = OneVsRestClassifier(LinearSVC()) from sklearn.utils import resample X_sample, Y_sample = resample( X, Y, n_samples=1000, replace=False, random_state=7) # X_sample_test, Y_sample_test = resample( # X, Y, n_samples=10, replace=False, random_state=1) X_sample_train, X_sample_test, Y_sample_train, Y_sample_test = train_test_split( X_sample, Y_sample, test_size=0.01, random_state=42) clf.fit(X_sample, Y_sample) Y_sample_pred = clf.predict(X_sample_test) # Inverse transform the vectors back to tags pred_transformed = mlb.inverse_transform(Y_sample_pred) test_transformed = mlb.inverse_transform(Y_sample_test) for (t, p) in zip(test_transformed, pred_transformed): print(f'tags: {t} predicted as: {p}')  tags: ('javascript',) predicted as: ('javascript',) tags: ('erasure-code', 'storage') predicted as: () tags: ('mysql', 'network') predicted as: () tags: ('token',) predicted as: () tags: ('flask', 'python', 'web') predicted as: () tags: ('refactoring',) predicted as: () tags: ('emacs',) predicted as: () tags: ('async', 'javascript', 'promises') predicted as: ('async', 'javascript') tags: ('neural-networks',) predicted as: () tags: ('kubernetes',) predicted as: ('kubernetes',)  Though not very satisfied, this classifier predicted right a few tags. Next we\u0026rsquo;ll try to search for the best parameters for the classifier and train with fulltext of articles.\nSearch for best model parameters for SVM with linear kernel For the estimators TfidfVectorizer and LinearSVC, they both have many parameters could be tuned for better performance. We\u0026rsquo;ll the GridSearchCV to search for the best parameters with the help of Pipeline.\nfrom sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split, GridSearchCV # Split the dataset into training and test set, and use fulltext of articles: X_train, X_test, Y_train, Y_test = train_test_split( df_data.fulltext, Y, test_size=0.5, random_state=42) # Build vectorizer classifier pipeline clf = Pipeline([ ('vect', TfidfVectorizer()), ('clf', OneVsRestClassifier(LinearSVC())), ]) # Grid search parameters, I minimized the parameter set based on previous # experience to accelerate the processing speed. # And the combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True parameters = { 'vect__ngram_range': [(1, 2), (1, 3), (1, 4)], 'vect__max_df': [1, 0.9, 0.8, 0.7], 'vect__min_df': [1, 0.9, 0.8, 0.7, 0], 'vect__use_idf': [True, False], 'clf__estimator__penalty': ['l1', 'l2'], 'clf__estimator__C': [1, 10, 100, 1000], 'clf__estimator__dual': [False], } gs_clf = GridSearchCV(clf, parameters, cv=5, n_jobs=-1) gs_clf.fit(X_train, Y_train)  import datetime from sklearn import metrics # Predict the outcome on the testing set in a variable named y_predicted Y_predicted = gs_clf.predict(X_test) print(metrics.classification_report(Y_test, Y_predicted)) print(gs_clf.best_params_) print(gs_clf.best_score_) # Export some of the result cols cols = [ 'mean_test_score', 'mean_fit_time', 'param_vect__ngram_range', ] df_result = pd.DataFrame(gs_clf.cv_results_) df_result = df_result.sort_values(by='rank_test_score') df_result = df_result[cols] timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S') df_result.to_html( f'data/results/gridcv_results_{timestamp}_linearSVC.html')  Here we attach the top-5 performed classifiers with selected parameters.\n  rank_test_score mean_test_score mean_fit_time param_vect__max_df param_vect__ngram_range param_vect__use_idf param_clf__estimator__penalty param_clf__estimator__C     64 1 0.140811 96.127405 0.8 (1, 4) True l1 10   70 2 0.140215 103.252332 0.7 (1, 4) True l1 10   58 2 0.140215 98.990952 0.9 (1, 4) True l1 10   154 2 0.140215 1690.433151 0.9 (1, 4) True l1 1000   68 5 0.139618 70.778621 0.7 (1, 3) True l1 10    Based on the grid search results, we found the following parameters combined with the default parameters have the best performance. Now let\u0026rsquo;s see how it will perform.\nX_train, X_test, Y_train, Y_test = train_test_split( df_data, Y, test_size=0.2, random_state=42) clf = Pipeline([ ('vect', TfidfVectorizer(use_idf=True, max_df=0.8, ngram_range=[1, 4])), ('clf', OneVsRestClassifier(LinearSVC(penalty='l1', C=10, dual=False))), ]) clf.fit(X_train.fulltext, Y_train) Y_pred = clf.predict(X_test.fulltext) # Inverse transform the vectors back to tags pred_transformed = mlb.inverse_transform(Y_pred) test_transformed = mlb.inverse_transform(Y_test) for (title, t, p) in zip(X_test.title, test_transformed, pred_transformed): print(f'info title: {title} \\ntags: {t} \\npredicted as: {p}')  info title: Designing resilient systems: Circuit Breakers or Retries? (Part 1) tags: ('circuit-breaker', 'system-architecture') predicted as: ('golang', 'system-architecture') info title: Zuul 2 : The Netflix Journey to Asynchronous, Non-Blocking Systems tags: ('asynchronous', 'system-architecture') predicted as: () info title: 对称加密算法与非对称加密算法的优缺点 tags: ('crytography',) predicted as: () info title: Go coding in go way tags: ('golang',) predicted as: ('golang',) info title: Emacs Doom for Newbies tags: ('doom-emacs', 'emacs') predicted as: ('emacs',) info title: Basic testing patterns in Go tags: ('golang', 'testing') predicted as: ('golang', 'testing') info title: Dead Simple Python: Virtual Environments and pip tags: ('python', 'virtualenv') predicted as: ('python',) info title: Overcoming RESTlessness tags: ('api', 'restful', 'web') predicted as: ('api',) info title: Let’s Build A Simple Interpreter. Part 7. tags: ('compiler', 'interpreter', 'pascal', 'python') predicted as: ('compiler', 'interpreter', 'pascal', 'python') info title: Spring RestTemplate详解 tags: ('html', 'java', 'spring') predicted as: ('java',)  Here above is a fraction of the list that shows the manually input tags and the predicted tags. We can see that usually the more frequently appeared and more popular tags have better change to be correctly predicted.\n","date":1568201803,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568374603,"objectID":"1303b1c7696e3e70de5b8332ddf9c0ab","permalink":"https://pcx.linkedinfo.co/post/texttagspred/","publishdate":"2019-09-11T13:36:43+02:00","relpermalink":"/post/texttagspred/","section":"post","summary":"This code snippet is to predict topic tags based on the text of an article. Each article could have 1 or more tags (usually have at least 1 tag), and the tags are not mutually exclusive. So this is a multi-label classification problem. It\u0026rsquo;s different from multi-class classification, the classes in multi-class classification are mutually exclusive, i.e., each item belongs to 1 and only 1 class.\nIn this snippet, we will use OneVsRestClassifier (the One-Vs-the-Rest) in scikit-learn to process the multi-label classification.","tags":["Machine Learning","Multi-label classification","Text classification","LinkedInfo.co"],"title":"Multi-label classification to predict topic tags of technical articles from LinkedInfo.co","type":"post"},{"authors":["Cong Peng","Prashant Goswami"],"categories":null,"content":"","date":1554069600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554069600,"objectID":"60d09f5ebc761287248b9f096f0928f1","permalink":"https://pcx.linkedinfo.co/publication/pcx-2019-a/","publishdate":"2019-04-01T00:00:00+02:00","relpermalink":"/publication/pcx-2019-a/","section":"publication","summary":"The development of electronic health records, wearable devices, health applications and Internet of Things (IoT)-empowered smart homes is promoting various applications. It also makes health self-management much more feasible, which can partially mitigate one of the challenges that the current healthcare system is facing. Effective and convenient self-management of health requires the collaborative use of health data and home environment data from different services, devices, and even open data on the Web. Although health data interoperability standards including HL7 Fast Healthcare Interoperability Resources (FHIR) and IoT ontology including Semantic Sensor Network (SSN) have been developed and promoted, it is impossible for all the different categories of services to adopt the same standard in the near future. This study presents a method that applies Semantic Web technologies to integrate the health data and home environment data from heterogeneously built services and devices. We propose a Web Ontology Language (OWL)-based integration ontology that models health data from HL7 FHIR standard implemented services, normal Web services and Web of Things (WoT) services and Linked Data together with home environment data from formal ontology-described WoT services. It works on the resource integration layer of the layered integration architecture. An example use case with a prototype implementation shows that the proposed method successfully integrates the health data and home environment data into a resource graph. The integrated data are annotated with semantics and ontological links, which make them machine-understandable and cross-system reusable.","tags":["FHIR","REST","Semantic Web","Web service","WoT","eHealth","health data integration","ontology","smart homes"],"title":"Meaningful Integration of Data from Heterogeneous Health Services and Home Environment Based on Ontology","type":"publication"},{"authors":null,"categories":null,"content":"The Web should be an open web. All the informations published on the Web are meant to be shared, share through links by search engines, rss, social networks, etc. This site is yet another method that tries to link all the informations (but starts with only technical articles on LinkedInfo) and share them.\nThe original idea of this side project is to utilize Semantic Web technologies and Machine learning to link the informations. Noble ambition shall start from basic, it needs to be improved little by little.\n","date":1548584274,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548584274,"objectID":"6268ed86f6986d5dbdacded5a30b3972","permalink":"https://pcx.linkedinfo.co/project/linkedinfo/","publishdate":"2019-01-27T11:17:54+01:00","relpermalink":"/project/linkedinfo/","section":"project","summary":"The Web should be an open web, all the informations published on the Web are meant to be shared. Linkedinfo.co is a Web service uses Semantic Web technologies and Machine Learning to link and share technical articles on the Web.","tags":["Semantic Web","Web technologies","Machine Learning"],"title":"LinkedInfo.co","type":"project"},{"authors":["Cong Peng"],"categories":null,"content":"","date":1546297200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546297200,"objectID":"8ca333643ce6917c513fec55c6c10a0c","permalink":"https://pcx.linkedinfo.co/publication/peng-2019/","publishdate":"2019-01-01T00:00:00+01:00","relpermalink":"/publication/peng-2019/","section":"publication","summary":"Group work-based learning is encouraged in higher education on account of both ped-agogical benefits and industrial employers's requirements. However, although a plenty of studies have been performed, there are still various factors that will affect students' group work-based learning in practice. It is important for the teachers to understand which factors are influenceable and what can be done to influence. This paper performs a literature review to identify the factors that has been investigated and reported in journal articles. Fifteen journal articles were found relevant and fifteen factors were identified, which could be influenced by instructors directly or indirectly. However, more evidence is needed to support the conclusion of some studies since they were performed only in one single course. Therefore, more studies are required on this topic to investigate the factors in different subject areas.","tags":["Group work","TBL","collaborative learning","teamwork"],"title":"What Can Teachers Do to Make the Group Work Learning Effective - a Literature Review","type":"publication"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1538344800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538344800,"objectID":"9706bb40671e2c103cc24202b650b5b7","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-d/","publishdate":"2018-10-01T00:00:00+02:00","relpermalink":"/publication/pcx-2018-d/","section":"publication","summary":"Effective and convenient self-management of health requires collaborative utilization of health data from different services provided by healthcare providers, consumer-facing products and even open data on the Web. Although health data interoperability standards include Fast Healthcare Interoperability Resources (FHIR) have been developed and promoted, it is impossible for all the different categories of services to adopt in the near future. The objective of this study aims to apply Semantic Web technologies to integrate the health data from heterogeneously built services. We present an Web Ontology Language (OWL)-based ontology that models together health data from FHIR standard implemented services, normal Web services and Linked Data. It works on the resource integration layer of the presented layered integration architecture. An example use case that demonstrates how this method integrates the health data into a linked semantic health resource graph with the proposed ontology is presented.","tags":["FHIR","Health data integration","REST","Semantic Web","Web service","eHealth","ontology"],"title":"An Ontological Approach to Integrate Health Resources from Different Categories of Services","type":"publication"},{"authors":[],"categories":[],"content":"","date":1515554369,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516245569,"objectID":"a0b16b66c87372eb421e9e35ae610b5b","permalink":"https://pcx.linkedinfo.co/post/web-notes/","publishdate":"2018-01-10T05:19:29+02:00","relpermalink":"/post/web-notes/","section":"post","summary":"","tags":["Web","Semantic Web"],"title":"Web Notes","type":"post"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"2979f299debfb142891b565c7962abe8","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-c/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-c/","section":"publication","summary":"The vast amount of Web services brings the problem of discovering desired services for composition and orchestration. The syntactic service matching methods based on the classical set theory have a difficulty to capture the imprecise information. Therefore, an approximate service matching approach based on fuzzy control is explored in this paper. A service description matching model to the OpenAPI specification, which is the most widely used standard for describing the defacto REST Web services, is proposed to realize the fuzzy service matching with the fuzzy inference method developed by Mamdani and Assilian. An evaluation shows that the fuzzy service matching approach performed slightly better than the weighted approach in the setting context.","tags":null,"title":"Fuzzy Matching of OpenAPI Described REST Services","type":"publication"},{"authors":["Cong Peng","Prashant Goswami","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"07ecb044ca58895dc86215b3dd2d5afb","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-b/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-b/","section":"publication","summary":"Various health Web services host a huge amount of health data about patients. The heterogeneity of the services hinders the collaborative utilization of these health data, which can provide a valuable support for the self-management of chronic diseases. The combination of REST Web services and Semantic Web technologies has proven to be a viable approach to address the problem. This paper proposes a method to add semantic annotations to the REST Web services. The service descriptions and the resource representations with semantic annotations can be transformed into a resource graph. It integrates health data from different services, and can link to the health-domain ontologies and Linked Open Health Data to support health management and imaginative applications. The feasibility of our method is demonstrated by realizing with OpenAPI service description and JSON-LD representation in an example use case.","tags":null,"title":"Linking Health Web Services as Resource Graph by Semantic REST Resource Tagging","type":"publication"},{"authors":["Cong Peng","Guohua Bai"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514761200,"objectID":"bd953f4e0ce85f1fbf6d4b9aa345c53d","permalink":"https://pcx.linkedinfo.co/publication/pcx-2018-a/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2018-a/","section":"publication","summary":"The utilization of Web services is becoming a human labor consuming work as the rapid growth of Web. The semantic annotated service description can support more automatic ways on tasks such as service discovery, invocation and composition. But the adoption of existed Semantic Web Services solutions is hindering by their overly complexity and high expertise demand. In this paper we propose a highly lightweight and non-intrusive method to enrich the REST Web service resources with semantic annotations to support a more autonomous Web service utilization and generic client service interaction. It is achieved by turning the service description into a semantic resource graph represented in RDF, with the added tag based semantic annotation and a small vocabulary. The method is implemented with the popular OpenAPI service description format, and illustrated by a simple use case example.","tags":["OpenAPI","REST","Semantic Annotation","Semantic Web Services","Service Discovery","Web Service Description"],"title":"Using Tag based Semantic Annotation to Empower Client and REST Service Interaction","type":"publication"},{"authors":null,"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"139f78476b81d743eb7410727891ddd5","permalink":"https://pcx.linkedinfo.co/post-old/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/post-old/","section":"","summary":"Hello!","tags":null,"title":"Posts","type":"widget_page"},{"authors":["Cong Peng"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"bf2e6fb9722806fbbc16597bcfe691f8","permalink":"https://pcx.linkedinfo.co/publication/peng-2017/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/peng-2017/","section":"publication","summary":"","tags":["Record keeping","research ethics"],"title":"Good Record Keeping for Conducting Research Ethically Correct","type":"publication"},{"authors":["Cong Peng","Guohua Bai"],"categories":null,"content":"","date":1451602800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451602800,"objectID":"31e3a3b71b5ab915c6d140c2e759a6b2","permalink":"https://pcx.linkedinfo.co/publication/pcx-2016/","publishdate":"2016-01-01T00:00:00+01:00","relpermalink":"/publication/pcx-2016/","section":"publication","summary":"Health data sharing can benefit patients to self-manage the challenging chronic diseases out of hospital. The patient controlled electronic Personal Health Record (PHR), as a tool manages comprehensive health data, is absolutely a good entry point to share health data with multiple parties for mutual benefits in the long-term. However, sharing health data from PHR remains challenges. The sharing of health data has to be considered holistically together with the key issues such as privacy, compatibility, evolvement and so on. A PHR system should be flexible to aggregate health data of a patient from various sources to make it comprehensive and up-to-date, should be flexible to share different categories and levels of health data for various utilizations and should be flexible to embed emerging access control mechanisms to ensure privacy and security under different sceneries. Therefore, the flexibility of system architecture on the integration of existed and future diversifications is crucial for PHR's practical long-term usability. This paper discussed the necessity and some possible solution, based on the reviewed literatures and the experience from a previous study, of flexible PHR system architecture on the mentioned aspects.","tags":null,"title":"Flexible System Architecture of PHR to Support Sharing Health Data for Chronic Disease Self-Management","type":"publication"},{"authors":["Y. Hu","C. Peng","G. Bai"],"categories":null,"content":"","date":1420066800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420066800,"objectID":"4de5be7c4b3d9f4859435c10012402f6","permalink":"https://pcx.linkedinfo.co/publication/huyanpcx-2015/","publishdate":"2015-01-01T00:00:00+01:00","relpermalink":"/publication/huyanpcx-2015/","section":"publication","summary":"Nowadays, patient self-management is encouraged in home-based healthcare, especially for chronic disease care. Sharing health information could improve the quality of patient self-management. In this paper, we introduce cloud computing as a potential technology to provide a more sustainable long-term solution compared with other technologies. A hybrid cloud is identified as a suitable way to enable patients to share health information for promoting the treatment of chronic diseases. And then a prototype on the case of type 2 diabetes is implemented to prove the feasibility of the proposed solution.","tags":["Chronic disease","Hybrid cloud","cloud computing"],"title":"Sharing health data through hybrid cloud for self-management","type":"publication"},{"authors":[],"categories":[],"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"112a179f289faa12977d845909944856","permalink":"https://pcx.linkedinfo.co/post/swift-brief-intro/","publishdate":"2014-07-01T00:00:00Z","relpermalink":"/post/swift-brief-intro/","section":"post","summary":"","tags":["OpenStack Swift"],"title":"Swift Brief Intro","type":"post"}]