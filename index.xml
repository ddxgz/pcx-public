<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cong Peng</title>
    <link>https://pcx.linkedinfo.co/</link>
      <atom:link href="https://pcx.linkedinfo.co/index.xml" rel="self" type="application/rss+xml" />
    <description>Cong Peng</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©2014–2019 Cong Peng</copyright><lastBuildDate>Thu, 13 Feb 2020 11:20:19 +0200</lastBuildDate>
    <image>
      <url>https://pcx.linkedinfo.co/img/icon-192.png</url>
      <title>Cong Peng</title>
      <link>https://pcx.linkedinfo.co/</link>
    </image>
    
    <item>
      <title>Using BERT to perform Topic Tag Prediction for Technical Articles</title>
      <link>https://pcx.linkedinfo.co/post/text-tag-prediction-bert/</link>
      <pubDate>Thu, 13 Feb 2020 11:20:19 +0200</pubDate>
      <guid>https://pcx.linkedinfo.co/post/text-tag-prediction-bert/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This is a follow up post of &lt;a href=&#34;https://pcx.linkedinfo.co/post/text-tag-prediction/&#34; target=&#34;_blank&#34;&gt;Multi-label classification to predict topic tags of technical articles from LinkedInfo.co&lt;/a&gt;. We will continute the same task by using BERT.&lt;/p&gt;

&lt;p&gt;Firstly we&amp;rsquo;ll just use the embeddings from BERT, and then feed them to the same classification model used in the last post, SVM with linear kenel. The reason of keep using SVM is that the size of the dataset is quite small.&lt;/p&gt;

&lt;h1 id=&#34;experiments&#34;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&#34;classify-by-using-bert-mini-and-svm-with-linear-kernel&#34;&gt;Classify by using BERT-Mini and SVM with Linear Kernel&lt;/h2&gt;

&lt;p&gt;Due to the limited computation capacity, we&amp;rsquo;ll use a smaller BERT model - BERT-Mini. The first experiment we&amp;rsquo;ll try to train on only the titles of the articles.&lt;/p&gt;

&lt;p&gt;Now we firstly load the dataset. And then load the pretrained BERT tokenizer and model. Note that we only load the article samples that are in English since the BERT-Mini model here were pretrained in English.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
from collections import Counter

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
import nltk
import plotly.express as px
from transformers import (BertPreTrainedModel, AutoTokenizer, AutoModel,
                          BertForSequenceClassification, AdamW, BertModel,
                          BertTokenizer, BertConfig, get_linear_schedule_with_warmup)

import dataset
from mltb.bert import bert_tokenize, bert_transform, get_tokenizer_model, download_once_pretrained_transformers
from mltb.experiment import multilearn_iterative_train_test_split
from mltb.metrics import classification_report_avg


nltk.download(&#39;punkt&#39;)

RAND_STATE = 20200122
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;ds = dataset.ds_info_tags(from_batch_cache=&#39;info&#39;, lan=&#39;en&#39;,
                          concate_title=True,
                          filter_tags_threshold=0, partial_len=3000)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;c = Counter([tag for tags in ds.target_decoded for tag in tags])

dfc = pd.DataFrame.from_dict(c, orient=&#39;index&#39;, columns=[&#39;count&#39;]).sort_values(by=&#39;count&#39;, ascending=False)[:100]

fig_Y = px.bar(dfc, x=dfc.index, y=&#39;count&#39;,
               text=&#39;count&#39;,
               labels={&#39;count&#39;: &#39;Number of infos&#39;,
                       &#39;x&#39;: &#39;Tags&#39;})
fig_Y.update_traces(texttemplate=&#39;%{text}&#39;)
&lt;/code&gt;&lt;/pre&gt;



&lt;html&gt;
&lt;head&gt;&lt;meta charset=&#34;utf-8&#34; /&gt;&lt;/head&gt;
&lt;body&gt;
    &lt;div&gt;
            &lt;script src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&#34;&gt;&lt;/script&gt;&lt;script type=&#34;text/javascript&#34;&gt;if (window.MathJax) {MathJax.Hub.Config({SVG: {font: &#34;STIX-Web&#34;}});}&lt;/script&gt;
                &lt;script type=&#34;text/javascript&#34;&gt;window.PlotlyConfig = {MathJaxConfig: &#39;local&#39;};&lt;/script&gt;
        &lt;script src=&#34;https://cdn.plot.ly/plotly-latest.min.js&#34;&gt;&lt;/script&gt;    
            &lt;div id=&#34;d7db1ccb-6f97-4d35-a5b0-eda705ab1bdb&#34; class=&#34;plotly-graph-div&#34; style=&#34;height:525px; width:100%;&#34;&gt;&lt;/div&gt;
            &lt;script type=&#34;text/javascript&#34;&gt;
                
                    window.PLOTLYENV=window.PLOTLYENV || {};
                    
                if (document.getElementById(&#34;d7db1ccb-6f97-4d35-a5b0-eda705ab1bdb&#34;)) {
                    Plotly.newPlot(
                        &#39;d7db1ccb-6f97-4d35-a5b0-eda705ab1bdb&#39;,
                        [{&#34;alignmentgroup&#34;: &#34;True&#34;, &#34;hoverlabel&#34;: {&#34;namelength&#34;: 0}, &#34;hovertemplate&#34;: &#34;Tags=%{x}&lt;br&gt;Number of infos=%{text}&#34;, &#34;legendgroup&#34;: &#34;&#34;, &#34;marker&#34;: {&#34;color&#34;: &#34;#636efa&#34;}, &#34;name&#34;: &#34;&#34;, &#34;offsetgroup&#34;: &#34;&#34;, &#34;orientation&#34;: &#34;v&#34;, &#34;showlegend&#34;: false, &#34;text&#34;: [425.0, 272.0, 181.0, 155.0, 129.0, 89.0, 80.0, 77.0, 67.0, 60.0, 53.0, 50.0, 48.0, 47.0, 42.0, 39.0, 39.0, 39.0, 35.0, 35.0, 33.0, 32.0, 32.0, 32.0, 31.0, 31.0, 31.0, 31.0, 30.0, 29.0, 28.0, 27.0, 27.0, 26.0, 26.0, 25.0, 25.0, 25.0, 22.0, 22.0, 21.0, 20.0, 19.0, 19.0, 19.0, 18.0, 18.0, 17.0, 17.0, 17.0, 16.0, 16.0, 16.0, 16.0, 15.0, 15.0, 15.0, 15.0, 14.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 10.0, 10.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0], &#34;textposition&#34;: &#34;auto&#34;, &#34;texttemplate&#34;: &#34;%{text}&#34;, &#34;type&#34;: &#34;bar&#34;, &#34;x&#34;: [&#34;python&#34;, &#34;golang&#34;, &#34;web&#34;, &#34;javascript&#34;, &#34;machine-learning&#34;, &#34;microservices&#34;, &#34;deep-learning&#34;, &#34;neural-networks&#34;, &#34;api&#34;, &#34;data-science&#34;, &#34;java&#34;, &#34;node.js&#34;, &#34;testing&#34;, &#34;concurrency&#34;, &#34;vue.js&#34;, &#34;db&#34;, &#34;system-architecture&#34;, &#34;react&#34;, &#34;compiler&#34;, &#34;docker&#34;, &#34;http&#34;, &#34;kubernetes&#34;, &#34;rust&#34;, &#34;git&#34;, &#34;django&#34;, &#34;restful&#34;, &#34;cpp&#34;, &#34;data-visualization&#34;, &#34;nlp&#34;, &#34;oop&#34;, &#34;kafka&#34;, &#34;angular&#34;, &#34;graphql&#34;, &#34;linux&#34;, &#34;programming&#34;, &#34;css&#34;, &#34;frontend&#34;, &#34;security&#34;, &#34;interpreter&#34;, &#34;functional-programming&#34;, &#34;distributed-system&#34;, &#34;chatbot&#34;, &#34;c&#34;, &#34;r&#34;, &#34;ai&#34;, &#34;mysql&#34;, &#34;http2&#34;, &#34;debug&#34;, &#34;tensorflow&#34;, &#34;asyncio&#34;, &#34;tdd&#34;, &#34;pandas&#34;, &#34;error-handling&#34;, &#34;memory&#34;, &#34;performance&#34;, &#34;algorithm&#34;, &#34;semantic-web&#34;, &#34;emacs&#34;, &#34;auth&#34;, &#34;grpc&#34;, &#34;computer-vision&#34;, &#34;pascal&#34;, &#34;statistics&#34;, &#34;redis&#34;, &#34;big-data&#34;, &#34;numpy&#34;, &#34;postgres&#34;, &#34;refactoring&#34;, &#34;mongodb&#34;, &#34;programmer-development&#34;, &#34;data-structure&#34;, &#34;pwa&#34;, &#34;finance&#34;, &#34;graphdb&#34;, &#34;asynchronous&#34;, &#34;json&#34;, &#34;cloud-computing&#34;, &#34;web-scraping&#34;, &#34;html&#34;, &#34;spark&#34;, &#34;blockchain&#34;, &#34;webpack&#34;, &#34;vim&#34;, &#34;dotnet&#34;, &#34;jwt&#34;, &#34;sql&#34;, &#34;stock&#34;, &#34;csharp&#34;, &#34;log&#34;, &#34;lxc&#34;, &#34;kotlin&#34;, &#34;storage&#34;, &#34;multithreading&#34;, &#34;scala&#34;, &#34;interface&#34;, &#34;spring&#34;, &#34;jit&#34;, &#34;flask&#34;, &#34;design-pattern&#34;, &#34;websocket&#34;], &#34;xaxis&#34;: &#34;x&#34;, &#34;y&#34;: [425, 272, 181, 155, 129, 89, 80, 77, 67, 60, 53, 50, 48, 47, 42, 39, 39, 39, 35, 35, 33, 32, 32, 32, 31, 31, 31, 31, 30, 29, 28, 27, 27, 26, 26, 25, 25, 25, 22, 22, 21, 20, 19, 19, 19, 18, 18, 17, 17, 17, 16, 16, 16, 16, 15, 15, 15, 15, 14, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8], &#34;yaxis&#34;: &#34;y&#34;}],
                        {&#34;barmode&#34;: &#34;relative&#34;, &#34;legend&#34;: {&#34;tracegroupgap&#34;: 0}, &#34;margin&#34;: {&#34;t&#34;: 60}, &#34;template&#34;: {&#34;data&#34;: {&#34;bar&#34;: [{&#34;error_x&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;error_y&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;marker&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#E5ECF6&#34;, &#34;width&#34;: 0.5}}, &#34;type&#34;: &#34;bar&#34;}], &#34;barpolar&#34;: [{&#34;marker&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#E5ECF6&#34;, &#34;width&#34;: 0.5}}, &#34;type&#34;: &#34;barpolar&#34;}], &#34;carpet&#34;: [{&#34;aaxis&#34;: {&#34;endlinecolor&#34;: &#34;#2a3f5f&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;minorgridcolor&#34;: &#34;white&#34;, &#34;startlinecolor&#34;: &#34;#2a3f5f&#34;}, &#34;baxis&#34;: {&#34;endlinecolor&#34;: &#34;#2a3f5f&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;minorgridcolor&#34;: &#34;white&#34;, &#34;startlinecolor&#34;: &#34;#2a3f5f&#34;}, &#34;type&#34;: &#34;carpet&#34;}], &#34;choropleth&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;choropleth&#34;}], &#34;contour&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;contour&#34;}], &#34;contourcarpet&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;contourcarpet&#34;}], &#34;heatmap&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;heatmap&#34;}], &#34;heatmapgl&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;heatmapgl&#34;}], &#34;histogram&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;histogram&#34;}], &#34;histogram2d&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;histogram2d&#34;}], &#34;histogram2dcontour&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;histogram2dcontour&#34;}], &#34;mesh3d&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;mesh3d&#34;}], &#34;parcoords&#34;: [{&#34;line&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;parcoords&#34;}], &#34;pie&#34;: [{&#34;automargin&#34;: true, &#34;type&#34;: &#34;pie&#34;}], &#34;scatter&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatter&#34;}], &#34;scatter3d&#34;: [{&#34;line&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatter3d&#34;}], &#34;scattercarpet&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattercarpet&#34;}], &#34;scattergeo&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattergeo&#34;}], &#34;scattergl&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattergl&#34;}], &#34;scattermapbox&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattermapbox&#34;}], &#34;scatterpolar&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterpolar&#34;}], &#34;scatterpolargl&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterpolargl&#34;}], &#34;scatterternary&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterternary&#34;}], &#34;surface&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;surface&#34;}], &#34;table&#34;: [{&#34;cells&#34;: {&#34;fill&#34;: {&#34;color&#34;: &#34;#EBF0F8&#34;}, &#34;line&#34;: {&#34;color&#34;: &#34;white&#34;}}, &#34;header&#34;: {&#34;fill&#34;: {&#34;color&#34;: &#34;#C8D4E3&#34;}, &#34;line&#34;: {&#34;color&#34;: &#34;white&#34;}}, &#34;type&#34;: &#34;table&#34;}]}, &#34;layout&#34;: {&#34;annotationdefaults&#34;: {&#34;arrowcolor&#34;: &#34;#2a3f5f&#34;, &#34;arrowhead&#34;: 0, &#34;arrowwidth&#34;: 1}, &#34;coloraxis&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;colorscale&#34;: {&#34;diverging&#34;: [[0, &#34;#8e0152&#34;], [0.1, &#34;#c51b7d&#34;], [0.2, &#34;#de77ae&#34;], [0.3, &#34;#f1b6da&#34;], [0.4, &#34;#fde0ef&#34;], [0.5, &#34;#f7f7f7&#34;], [0.6, &#34;#e6f5d0&#34;], [0.7, &#34;#b8e186&#34;], [0.8, &#34;#7fbc41&#34;], [0.9, &#34;#4d9221&#34;], [1, &#34;#276419&#34;]], &#34;sequential&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;sequentialminus&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]]}, &#34;colorway&#34;: [&#34;#636efa&#34;, &#34;#EF553B&#34;, &#34;#00cc96&#34;, &#34;#ab63fa&#34;, &#34;#FFA15A&#34;, &#34;#19d3f3&#34;, &#34;#FF6692&#34;, &#34;#B6E880&#34;, &#34;#FF97FF&#34;, &#34;#FECB52&#34;], &#34;font&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;geo&#34;: {&#34;bgcolor&#34;: &#34;white&#34;, &#34;lakecolor&#34;: &#34;white&#34;, &#34;landcolor&#34;: &#34;#E5ECF6&#34;, &#34;showlakes&#34;: true, &#34;showland&#34;: true, &#34;subunitcolor&#34;: &#34;white&#34;}, &#34;hoverlabel&#34;: {&#34;align&#34;: &#34;left&#34;}, &#34;hovermode&#34;: &#34;closest&#34;, &#34;mapbox&#34;: {&#34;style&#34;: &#34;light&#34;}, &#34;paper_bgcolor&#34;: &#34;white&#34;, &#34;plot_bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;polar&#34;: {&#34;angularaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;radialaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}}, &#34;scene&#34;: {&#34;xaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}, &#34;yaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}, &#34;zaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}}, &#34;shapedefaults&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}}, &#34;ternary&#34;: {&#34;aaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;baxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;caxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}}, &#34;title&#34;: {&#34;x&#34;: 0.05}, &#34;xaxis&#34;: {&#34;automargin&#34;: true, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;, &#34;title&#34;: {&#34;standoff&#34;: 15}, &#34;zerolinecolor&#34;: &#34;white&#34;, &#34;zerolinewidth&#34;: 2}, &#34;yaxis&#34;: {&#34;automargin&#34;: true, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;, &#34;title&#34;: {&#34;standoff&#34;: 15}, &#34;zerolinecolor&#34;: &#34;white&#34;, &#34;zerolinewidth&#34;: 2}}}, &#34;xaxis&#34;: {&#34;anchor&#34;: &#34;y&#34;, &#34;domain&#34;: [0.0, 1.0], &#34;title&#34;: {&#34;text&#34;: &#34;Tags&#34;}}, &#34;yaxis&#34;: {&#34;anchor&#34;: &#34;x&#34;, &#34;domain&#34;: [0.0, 1.0], &#34;title&#34;: {&#34;text&#34;: &#34;Number of infos&#34;}}},
                        {&#34;responsive&#34;: true}
                    ).then(function(){
                            
var gd = document.getElementById(&#39;d7db1ccb-6f97-4d35-a5b0-eda705ab1bdb&#39;);
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === &#39;none&#39;) {{
            console.log([gd, &#39;removed!&#39;]);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest(&#39;#notebook-container&#39;);
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest(&#39;.output&#39;);
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })
                };
                
            &lt;/script&gt;
        &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;


&lt;pre&gt;&lt;code&gt;dfc_tail = pd.DataFrame.from_dict(c, orient=&#39;index&#39;, columns=[&#39;count&#39;]).sort_values(by=&#39;count&#39;, ascending=False)[-200:]

fig_Y = px.bar(dfc_tail, x=dfc_tail.index, y=&#39;count&#39;,
               text=&#39;count&#39;,
               labels={&#39;count&#39;: &#39;Number of infos&#39;,
                       &#39;x&#39;: &#39;Tags&#39;})
fig_Y.update_traces(texttemplate=&#39;%{text}&#39;)
&lt;/code&gt;&lt;/pre&gt;



&lt;html&gt;
&lt;head&gt;&lt;meta charset=&#34;utf-8&#34; /&gt;&lt;/head&gt;
&lt;body&gt;
    &lt;div&gt;
            &lt;script src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&#34;&gt;&lt;/script&gt;&lt;script type=&#34;text/javascript&#34;&gt;if (window.MathJax) {MathJax.Hub.Config({SVG: {font: &#34;STIX-Web&#34;}});}&lt;/script&gt;
                &lt;script type=&#34;text/javascript&#34;&gt;window.PlotlyConfig = {MathJaxConfig: &#39;local&#39;};&lt;/script&gt;
        &lt;script src=&#34;https://cdn.plot.ly/plotly-latest.min.js&#34;&gt;&lt;/script&gt;    
            &lt;div id=&#34;93a29e39-9a0c-49f1-9585-5d6ba50f38f5&#34; class=&#34;plotly-graph-div&#34; style=&#34;height:525px; width:100%;&#34;&gt;&lt;/div&gt;
            &lt;script type=&#34;text/javascript&#34;&gt;
                
                    window.PLOTLYENV=window.PLOTLYENV || {};
                    
                if (document.getElementById(&#34;93a29e39-9a0c-49f1-9585-5d6ba50f38f5&#34;)) {
                    Plotly.newPlot(
                        &#39;93a29e39-9a0c-49f1-9585-5d6ba50f38f5&#39;,
                        [{&#34;alignmentgroup&#34;: &#34;True&#34;, &#34;hoverlabel&#34;: {&#34;namelength&#34;: 0}, &#34;hovertemplate&#34;: &#34;Tags=%{x}&lt;br&gt;Number of infos=%{text}&#34;, &#34;legendgroup&#34;: &#34;&#34;, &#34;marker&#34;: {&#34;color&#34;: &#34;#636efa&#34;}, &#34;name&#34;: &#34;&#34;, &#34;offsetgroup&#34;: &#34;&#34;, &#34;orientation&#34;: &#34;v&#34;, &#34;showlegend&#34;: false, &#34;text&#34;: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], &#34;textposition&#34;: &#34;auto&#34;, &#34;texttemplate&#34;: &#34;%{text}&#34;, &#34;type&#34;: &#34;bar&#34;, &#34;x&#34;: [&#34;fusion.js&#34;, &#34;load-balance&#34;, &#34;ethic&#34;, &#34;web-services&#34;, &#34;aws&#34;, &#34;filesystem&#34;, &#34;iot&#34;, &#34;emacs-lisp&#34;, &#34;github&#34;, &#34;crawler&#34;, &#34;event-driven&#34;, &#34;pytorch&#34;, &#34;pdf&#34;, &#34;lambda&#34;, &#34;spacemacs&#34;, &#34;pdb&#34;, &#34;namespaces&#34;, &#34;mvc&#34;, &#34;unicode&#34;, &#34;sdk&#34;, &#34;makefile&#34;, &#34;webkit&#34;, &#34;cdn&#34;, &#34;object-detection&#34;, &#34;documentation&#34;, &#34;project-management&#34;, &#34;cloud&#34;, &#34;classification&#34;, &#34;jvm&#34;, &#34;circuit-breaker&#34;, &#34;integration-test&#34;, &#34;amqp&#34;, &#34;anomaly-detection&#34;, &#34;dom&#34;, &#34;cloud-storage&#34;, &#34;ddd&#34;, &#34;vue-router&#34;, &#34;activemq&#34;, &#34;excel&#34;, &#34;linked-list&#34;, &#34;linear-programming&#34;, &#34;electron&#34;, &#34;kernel&#34;, &#34;lock-free&#34;, &#34;data-mining&#34;, &#34;virtual-machine&#34;, &#34;d3&#34;, &#34;neo4j&#34;, &#34;text-editor&#34;, &#34;unity&#34;, &#34;baas&#34;, &#34;information-theory&#34;, &#34;messaging&#34;, &#34;crdt&#34;, &#34;os&#34;, &#34;webgl&#34;, &#34;anti-pattern&#34;, &#34;hadoop&#34;, &#34;development&#34;, &#34;orm&#34;, &#34;voltdb&#34;, &#34;linear-model&#34;, &#34;consul&#34;, &#34;ansible&#34;, &#34;pathfinding&#34;, &#34;linked-data&#34;, &#34;ajax&#34;, &#34;rendering-techniques&#34;, &#34;random-forests&#34;, &#34;photography&#34;, &#34;dns&#34;, &#34;code-review&#34;, &#34;rsa&#34;, &#34;bloom-filter&#34;, &#34;deployment&#34;, &#34;forward+&#34;, &#34;token&#34;, &#34;gcc&#34;, &#34;oracle&#34;, &#34;cap&#34;, &#34;severless&#34;, &#34;wpf&#34;, &#34;graphics&#34;, &#34;cors&#34;, &#34;locale&#34;, &#34;ipv6&#34;, &#34;apache&#34;, &#34;iots&#34;, &#34;bytecode&#34;, &#34;cookie&#34;, &#34;bot&#34;, &#34;time-series-analysis&#34;, &#34;assembly&#34;, &#34;arm&#34;, &#34;multiprocessing&#34;, &#34;cms&#34;, &#34;cassandra&#34;, &#34;ggplot&#34;, &#34;mesos&#34;, &#34;automl&#34;, &#34;auditing&#34;, &#34;service-architecture&#34;, &#34;goroutine&#34;, &#34;text-generator&#34;, &#34;option&#34;, &#34;travis-ci&#34;, &#34;gateway&#34;, &#34;productivity&#34;, &#34;angularjs&#34;, &#34;pyspark&#34;, &#34;data-pipeline&#34;, &#34;raspberry-pi&#34;, &#34;service-mesh&#34;, &#34;memcached&#34;, &#34;quantum-communication&#34;, &#34;soa&#34;, &#34;ssh&#34;, &#34;open-data&#34;, &#34;command-line-interface&#34;, &#34;yaml&#34;, &#34;server&#34;, &#34;mac&#34;, &#34;monitoring&#34;, &#34;translation&#34;, &#34;posix&#34;, &#34;data-analytics&#34;, &#34;hdfs&#34;, &#34;recurrent-neural-networks&#34;, &#34;word-embedding&#34;, &#34;ludwig&#34;, &#34;image-recognition&#34;, &#34;maven&#34;, &#34;wordpress&#34;, &#34;elm&#34;, &#34;monad&#34;, &#34;plotly&#34;, &#34;math&#34;, &#34;protobuf&#34;, &#34;face-recognition&#34;, &#34;paxos&#34;, &#34;academia&#34;, &#34;speech-recognition&#34;, &#34;feature-engineering&#34;, &#34;babel&#34;, &#34;tachyon&#34;, &#34;postgresql&#34;, &#34;relational-database&#34;, &#34;data-workflow&#34;, &#34;monkey-patch&#34;, &#34;udp&#34;, &#34;ehealth&#34;, &#34;optimization&#34;, &#34;webrtc&#34;, &#34;proxy&#34;, &#34;cross-validation&#34;, &#34;hypothesis-test&#34;, &#34;ivy&#34;, &#34;markov-chains&#34;, &#34;text-recognition&#34;, &#34;latex&#34;, &#34;openstack&#34;, &#34;sqlite&#34;, &#34;computer-system&#34;, &#34;eslint&#34;, &#34;obect-detection&#34;, &#34;bokeh&#34;, &#34;data-cleaning&#34;, &#34;color-topics&#34;, &#34;firebase&#34;, &#34;doom-emacs&#34;, &#34;speech--recognition&#34;, &#34;decentralized-web&#34;, &#34;tkinter&#34;, &#34;ops&#34;, &#34;epoll&#34;, &#34;index&#34;, &#34;lua&#34;, &#34;dgraph&#34;, &#34;desktop&#34;, &#34;reliability&#34;, &#34;laravel&#34;, &#34;mypy&#34;, &#34;access-control&#34;, &#34;virtualization&#34;, &#34;kvm&#34;, &#34;qemu&#34;, &#34;csrf&#34;, &#34;quantum-computing&#34;, &#34;nodejs&#34;, &#34;random-forest&#34;, &#34;quantum-theory&#34;, &#34;socket&#34;, &#34;redux&#34;, &#34;ide&#34;, &#34;postgre&#34;, &#34;text-classifier&#34;, &#34;etl&#34;, &#34;cloud-native&#34;, &#34;agile&#34;, &#34;sparql&#34;], &#34;xaxis&#34;: &#34;x&#34;, &#34;y&#34;: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], &#34;yaxis&#34;: &#34;y&#34;}],
                        {&#34;barmode&#34;: &#34;relative&#34;, &#34;legend&#34;: {&#34;tracegroupgap&#34;: 0}, &#34;margin&#34;: {&#34;t&#34;: 60}, &#34;template&#34;: {&#34;data&#34;: {&#34;bar&#34;: [{&#34;error_x&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;error_y&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;marker&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#E5ECF6&#34;, &#34;width&#34;: 0.5}}, &#34;type&#34;: &#34;bar&#34;}], &#34;barpolar&#34;: [{&#34;marker&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#E5ECF6&#34;, &#34;width&#34;: 0.5}}, &#34;type&#34;: &#34;barpolar&#34;}], &#34;carpet&#34;: [{&#34;aaxis&#34;: {&#34;endlinecolor&#34;: &#34;#2a3f5f&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;minorgridcolor&#34;: &#34;white&#34;, &#34;startlinecolor&#34;: &#34;#2a3f5f&#34;}, &#34;baxis&#34;: {&#34;endlinecolor&#34;: &#34;#2a3f5f&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;minorgridcolor&#34;: &#34;white&#34;, &#34;startlinecolor&#34;: &#34;#2a3f5f&#34;}, &#34;type&#34;: &#34;carpet&#34;}], &#34;choropleth&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;choropleth&#34;}], &#34;contour&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;contour&#34;}], &#34;contourcarpet&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;contourcarpet&#34;}], &#34;heatmap&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;heatmap&#34;}], &#34;heatmapgl&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;heatmapgl&#34;}], &#34;histogram&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;histogram&#34;}], &#34;histogram2d&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;histogram2d&#34;}], &#34;histogram2dcontour&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;histogram2dcontour&#34;}], &#34;mesh3d&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;mesh3d&#34;}], &#34;parcoords&#34;: [{&#34;line&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;parcoords&#34;}], &#34;pie&#34;: [{&#34;automargin&#34;: true, &#34;type&#34;: &#34;pie&#34;}], &#34;scatter&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatter&#34;}], &#34;scatter3d&#34;: [{&#34;line&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatter3d&#34;}], &#34;scattercarpet&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattercarpet&#34;}], &#34;scattergeo&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattergeo&#34;}], &#34;scattergl&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattergl&#34;}], &#34;scattermapbox&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattermapbox&#34;}], &#34;scatterpolar&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterpolar&#34;}], &#34;scatterpolargl&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterpolargl&#34;}], &#34;scatterternary&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterternary&#34;}], &#34;surface&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;surface&#34;}], &#34;table&#34;: [{&#34;cells&#34;: {&#34;fill&#34;: {&#34;color&#34;: &#34;#EBF0F8&#34;}, &#34;line&#34;: {&#34;color&#34;: &#34;white&#34;}}, &#34;header&#34;: {&#34;fill&#34;: {&#34;color&#34;: &#34;#C8D4E3&#34;}, &#34;line&#34;: {&#34;color&#34;: &#34;white&#34;}}, &#34;type&#34;: &#34;table&#34;}]}, &#34;layout&#34;: {&#34;annotationdefaults&#34;: {&#34;arrowcolor&#34;: &#34;#2a3f5f&#34;, &#34;arrowhead&#34;: 0, &#34;arrowwidth&#34;: 1}, &#34;coloraxis&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;colorscale&#34;: {&#34;diverging&#34;: [[0, &#34;#8e0152&#34;], [0.1, &#34;#c51b7d&#34;], [0.2, &#34;#de77ae&#34;], [0.3, &#34;#f1b6da&#34;], [0.4, &#34;#fde0ef&#34;], [0.5, &#34;#f7f7f7&#34;], [0.6, &#34;#e6f5d0&#34;], [0.7, &#34;#b8e186&#34;], [0.8, &#34;#7fbc41&#34;], [0.9, &#34;#4d9221&#34;], [1, &#34;#276419&#34;]], &#34;sequential&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;sequentialminus&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]]}, &#34;colorway&#34;: [&#34;#636efa&#34;, &#34;#EF553B&#34;, &#34;#00cc96&#34;, &#34;#ab63fa&#34;, &#34;#FFA15A&#34;, &#34;#19d3f3&#34;, &#34;#FF6692&#34;, &#34;#B6E880&#34;, &#34;#FF97FF&#34;, &#34;#FECB52&#34;], &#34;font&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;geo&#34;: {&#34;bgcolor&#34;: &#34;white&#34;, &#34;lakecolor&#34;: &#34;white&#34;, &#34;landcolor&#34;: &#34;#E5ECF6&#34;, &#34;showlakes&#34;: true, &#34;showland&#34;: true, &#34;subunitcolor&#34;: &#34;white&#34;}, &#34;hoverlabel&#34;: {&#34;align&#34;: &#34;left&#34;}, &#34;hovermode&#34;: &#34;closest&#34;, &#34;mapbox&#34;: {&#34;style&#34;: &#34;light&#34;}, &#34;paper_bgcolor&#34;: &#34;white&#34;, &#34;plot_bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;polar&#34;: {&#34;angularaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;radialaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}}, &#34;scene&#34;: {&#34;xaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}, &#34;yaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}, &#34;zaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}}, &#34;shapedefaults&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}}, &#34;ternary&#34;: {&#34;aaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;baxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;caxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}}, &#34;title&#34;: {&#34;x&#34;: 0.05}, &#34;xaxis&#34;: {&#34;automargin&#34;: true, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;, &#34;title&#34;: {&#34;standoff&#34;: 15}, &#34;zerolinecolor&#34;: &#34;white&#34;, &#34;zerolinewidth&#34;: 2}, &#34;yaxis&#34;: {&#34;automargin&#34;: true, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;, &#34;title&#34;: {&#34;standoff&#34;: 15}, &#34;zerolinecolor&#34;: &#34;white&#34;, &#34;zerolinewidth&#34;: 2}}}, &#34;xaxis&#34;: {&#34;anchor&#34;: &#34;y&#34;, &#34;domain&#34;: [0.0, 1.0], &#34;title&#34;: {&#34;text&#34;: &#34;Tags&#34;}}, &#34;yaxis&#34;: {&#34;anchor&#34;: &#34;x&#34;, &#34;domain&#34;: [0.0, 1.0], &#34;title&#34;: {&#34;text&#34;: &#34;Number of infos&#34;}}},
                        {&#34;responsive&#34;: true}
                    ).then(function(){
                            
var gd = document.getElementById(&#39;93a29e39-9a0c-49f1-9585-5d6ba50f38f5&#39;);
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === &#39;none&#39;) {{
            console.log([gd, &#39;removed!&#39;]);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest(&#39;#notebook-container&#39;);
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest(&#39;.output&#39;);
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })
                };
                
            &lt;/script&gt;
        &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;


&lt;p&gt;After we loaded the data, we checked how frequently are the tags being tagged to the articles. Here we only visualized the top-100 tags (you can select area of the figure to zoomin), we can see that there&amp;rsquo;s a big imbalancement of popularity among tags. We can try to mitigate this imbalancement by using different methods like sampling methods and augmentation. But now we&amp;rsquo;ll just pretend we don&amp;rsquo;t know that and leave this aside.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s load the BERT tokenizer and model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PRETRAINED_BERT_WEIGHTS = download_once_pretrained_transformers(
    &amp;quot;google/bert_uncased_L-4_H-256_A-4&amp;quot;)
tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BERT_WEIGHTS)
model = AutoModel.from_pretrained(PRETRAINED_BERT_WEIGHTS)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we encode all the titles by the BERT-Mini model. We&amp;rsquo;ll use only the 1st output vector from the model as it&amp;rsquo;s used for classification task.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;col_text = &#39;title&#39;
max_length = ds.data[col_text].apply(lambda x: len(nltk.word_tokenize(x))).max()

encoded = ds.data[col_text].apply(
    (lambda x: tokenizer.encode_plus(x, add_special_tokens=True,
                                     pad_to_max_length=True,
                                     return_attention_mask=True,
                                     max_length=max_length,
                                     return_tensors=&#39;pt&#39;)))

input_ids = torch.cat(tuple(encoded.apply(lambda x:x[&#39;input_ids&#39;])))
attention_mask = torch.cat(tuple(encoded.apply(lambda x:x[&#39;attention_mask&#39;])))

features = []
with torch.no_grad():
    last_hidden_states = model(input_ids, attention_mask=attention_mask)
    features = last_hidden_states[0][:, 0, :].numpy()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the features are changed from Tf-idf transformed to BERT transformed, so we&amp;rsquo;ll re-search for the hyper-parameters for the LinearSVC to use.&lt;/p&gt;

&lt;p&gt;The scorer we used in grid search is f-0.5 score since we want to weight higher precision over recall.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_features, test_features, train_labels, test_labels = train_test_split(
    features, ds.target, test_size=0.3, random_state=RAND_STATE)
    
clf = OneVsRestClassifier(LinearSVC())

C_OPTIONS = [0.01, 0.1, 0.5, 1, 10]

parameters = {
    &#39;estimator__penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;],
    &#39;estimator__dual&#39;: [True, False],
    &#39;estimator__C&#39;: C_OPTIONS,
}

micro_f05_sco = metrics.make_scorer(
    metrics.fbeta_score, beta=0.5, average=&#39;micro&#39;)

gs_clf = GridSearchCV(clf, parameters,
                      scoring=micro_f05_sco,
                      cv=3, n_jobs=-1)

gs_clf.fit(train_features, train_labels)
print(gs_clf.best_params_)
print(gs_clf.best_score_)

Y_predicted = gs_clf.predict(test_features)

report = metrics.classification_report(
    test_labels, Y_predicted, output_dict=True, zero_division=0)
df_report = pd.DataFrame(report).transpose()
cols_avg = [&#39;micro avg&#39;, &#39;macro avg&#39;, &#39;weighted avg&#39;, &#39;samples avg&#39;]
df_report.loc[cols_avg,]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;estimator__C&#39;: 0.1, &#39;estimator__dual&#39;: True, &#39;estimator__penalty&#39;: &#39;l2&#39;}
0.5793483937857783
&lt;/code&gt;&lt;/pre&gt;




&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;precision&lt;/th&gt;
      &lt;th&gt;recall&lt;/th&gt;
      &lt;th&gt;f1-score&lt;/th&gt;
      &lt;th&gt;support&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;micro avg&lt;/th&gt;
      &lt;td&gt;0.892857&lt;/td&gt;
      &lt;td&gt;0.242326&lt;/td&gt;
      &lt;td&gt;0.381194&lt;/td&gt;
      &lt;td&gt;1238.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;macro avg&lt;/th&gt;
      &lt;td&gt;0.173746&lt;/td&gt;
      &lt;td&gt;0.092542&lt;/td&gt;
      &lt;td&gt;0.111124&lt;/td&gt;
      &lt;td&gt;1238.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;weighted avg&lt;/th&gt;
      &lt;td&gt;0.608618&lt;/td&gt;
      &lt;td&gt;0.242326&lt;/td&gt;
      &lt;td&gt;0.324186&lt;/td&gt;
      &lt;td&gt;1238.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;samples avg&lt;/th&gt;
      &lt;td&gt;0.404088&lt;/td&gt;
      &lt;td&gt;0.274188&lt;/td&gt;
      &lt;td&gt;0.312305&lt;/td&gt;
      &lt;td&gt;1238.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;



&lt;p&gt;Though it&amp;rsquo;s not comparable, the result metrics are no better than the Tf-idf one when we use only the English samples with their titles here. The micro average precision is higher, the other averages of precision are about the same. The recalls got much lower.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s combine the titles and short descriptions to see if there&amp;rsquo;s any improvment.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;col_text = &#39;description&#39;
max_length = ds.data[col_text].apply(lambda x: len(nltk.word_tokenize(x))).max()
encoded = ds.data[col_text].apply(
    (lambda x: tokenizer.encode_plus(x, add_special_tokens=True,
                                     pad_to_max_length=True,
                                     return_attention_mask=True,
                                     max_length=max_length,
                                     return_tensors=&#39;pt&#39;)))

input_ids = torch.cat(tuple(encoded.apply(lambda x:x[&#39;input_ids&#39;])))
attention_mask = torch.cat(tuple(encoded.apply(lambda x:x[&#39;attention_mask&#39;])))

features = []
with torch.no_grad():
    last_hidden_states = model(input_ids, attention_mask=attention_mask)
    features = last_hidden_states[0][:, 0, :].numpy()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;train_features, test_features, train_labels, test_labels = train_test_split(
    features, ds.target, test_size=0.3, random_state=RAND_STATE)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;clf = OneVsRestClassifier(LinearSVC())

C_OPTIONS = [0.1, 1]

parameters = {
    &#39;estimator__penalty&#39;: [&#39;l2&#39;],
    &#39;estimator__dual&#39;: [True],
    &#39;estimator__C&#39;: C_OPTIONS,
}

micro_f05_sco = metrics.make_scorer(
    metrics.fbeta_score, beta=0.5, average=&#39;micro&#39;)

gs_clf = GridSearchCV(clf, parameters,
                      scoring=micro_f05_sco,
                      cv=3, n_jobs=-1)

gs_clf.fit(train_features, train_labels)

print(gs_clf.best_params_)
print(gs_clf.best_score_)

Y_predicted = gs_clf.predict(test_features)

classification_report_avg(test_labels, Y_predicted)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;estimator__C&#39;: 1, &#39;estimator__dual&#39;: True, &#39;estimator__penalty&#39;: &#39;l2&#39;}
0.4954311860243222
              precision    recall  f1-score  support
micro avg      0.684015  0.297254  0.414414   1238.0
macro avg      0.178030  0.109793  0.127622   1238.0
weighted avg   0.522266  0.297254  0.362237   1238.0
samples avg    0.401599  0.314649  0.337884   1238.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is no improvement, the precision averages even got a little bit worse. Let&amp;rsquo;s try to explore further.&lt;/p&gt;

&lt;h2 id=&#34;iterative-stratified-multilabel-data-sampling&#34;&gt;Iterative stratified multilabel data sampling&lt;/h2&gt;

&lt;p&gt;It would be a good idea to perform stratified sampling for spliting training and test sets since there&amp;rsquo;s a big imbalancement in the dataset for the labels. The problem is that the size of dataset is very small, which causes it that using normal stratified sampling method would fail since it&amp;rsquo;s likely that some labels may not appear in both training and testing sets. That&amp;rsquo;s why we have to use iterative stratified multilabel sampling. The explanation of this method can refer to &lt;a href=&#34;http://scikit.ml/stratification.html&#34; target=&#34;_blank&#34;&gt;document of scikit-multilearn&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the code below we have wrapped the split method for brevity.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;COL_TEXT = &#39;description&#39;

train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split(
    ds.data, ds.target, test_size=0.3, cols=ds.data.columns)

batch_size = 128
model_name = &amp;quot;google/bert_uncased_L-4_H-256_A-4&amp;quot;

train_features, test_features = bert_transform(
    train_features, test_features, COL_TEXT, model_name, batch_size)


clf = OneVsRestClassifier(LinearSVC())

C_OPTIONS = [0.1, 1]

parameters = {
    &#39;estimator__penalty&#39;: [&#39;l2&#39;],
    &#39;estimator__dual&#39;: [True],
    &#39;estimator__C&#39;: C_OPTIONS,
}

micro_f05_sco = metrics.make_scorer(
    metrics.fbeta_score, beta=0.5, average=&#39;micro&#39;)

gs_clf = GridSearchCV(clf, parameters,
                      scoring=micro_f05_sco,
                      cv=3, n_jobs=-1)

gs_clf.fit(train_features, train_labels)

print(gs_clf.best_params_)
print(gs_clf.best_score_)

Y_predicted = gs_clf.predict(test_features)

print(classification_report_avg(test_labels, Y_predicted))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;estimator__C&#39;: 0.1, &#39;estimator__dual&#39;: True, &#39;estimator__penalty&#39;: &#39;l2&#39;}
0.3292528001922235
              precision    recall  f1-score  support
micro avg      0.674086  0.356003  0.465934   1191.0
macro avg      0.230836  0.162106  0.181784   1191.0
weighted avg   0.551619  0.356003  0.420731   1191.0
samples avg    0.460420  0.377735  0.392599   1191.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There seems no improvement. But the cross validation F-0.5 score is lower than the testing score. It might be a sign that it&amp;rsquo;s under-fitting.&lt;/p&gt;

&lt;h2 id=&#34;training-set-augmentation&#34;&gt;Training set augmentation&lt;/h2&gt;

&lt;p&gt;As the dataset is quite small, now we&amp;rsquo;ll try to augment the trainig set to see if there&amp;rsquo;s any improvement.&lt;/p&gt;

&lt;p&gt;Here we set the augmentation level to 2, which means the dataset are concatenated by 2 times of the samples. And the added samples&amp;rsquo; content will be randomly chopped out as &lt;sup&gt;9&lt;/sup&gt;&amp;frasl;&lt;sub&gt;10&lt;/sub&gt; of its original content. Of course, both the actions only apply to the training set. The 30% test set is kept aside.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;COL_TEXT = &#39;description&#39;

train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split(
    ds.data, ds.target, test_size=0.3, cols=ds.data.columns)

train_features, train_labels = dataset.augmented_samples(
    train_features, train_labels, level=2, crop_ratio=0.1)

batch_size = 128
model_name = &amp;quot;google/bert_uncased_L-4_H-256_A-4&amp;quot;

train_features, test_features = bert_transform(
    train_features, test_features, COL_TEXT, model_name, batch_size)

clf = OneVsRestClassifier(LinearSVC())

C_OPTIONS = [0.1, 1]

parameters = {
    &#39;estimator__penalty&#39;: [&#39;l2&#39;],
    &#39;estimator__dual&#39;: [True],
    &#39;estimator__C&#39;: C_OPTIONS,
}

micro_f05_sco = metrics.make_scorer(
    metrics.fbeta_score, beta=0.5, average=&#39;micro&#39;)

gs_clf = GridSearchCV(clf, parameters,
                      scoring=micro_f05_sco,
                      cv=3, n_jobs=-1)

gs_clf.fit(train_features, train_labels)

print(gs_clf.best_params_)
print(gs_clf.best_score_)

Y_predicted = gs_clf.predict(test_features)

classification_report_avg(test_labels, Y_predicted)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;estimator__C&#39;: 0.1, &#39;estimator__dual&#39;: True, &#39;estimator__penalty&#39;: &#39;l2&#39;}
0.9249583214520737
              precision    recall  f1-score  support
micro avg      0.616296  0.348409  0.445158   1194.0
macro avg      0.224752  0.162945  0.180873   1194.0
weighted avg   0.520024  0.348409  0.406509   1194.0
samples avg    0.442572  0.373784  0.384738   1194.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can see that there&amp;rsquo;s still no improvement. It seems that we should change direction.&lt;/p&gt;

&lt;h2 id=&#34;filter-rare-tags&#34;&gt;Filter rare tags&lt;/h2&gt;

&lt;p&gt;If you remember that the first time we loaded the data we visualized the appearence frequency of the tags. It showed that most of the tags appeared only very few times, over 200 tags appeared only once or twice. This is quite a big problem for the model to classify for these tags.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s try to filter out the least appeared tags. Let&amp;rsquo;s start from a big number of 20, i.e., tags appeared in less than 20 articles will be removed.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;col_text = &#39;description&#39;
ds_param = dict(from_batch_cache=&#39;info&#39;, lan=&#39;en&#39;,
                concate_title=True,
                filter_tags_threshold=20)
ds = dataset.ds_info_tags(**ds_param)

c = Counter([tag for tags in ds.target_decoded for tag in tags])

dfc = pd.DataFrame.from_dict(c, orient=&#39;index&#39;, columns=[&#39;count&#39;]).sort_values(by=&#39;count&#39;, ascending=False)[:100]

fig_Y = px.bar(dfc, x=dfc.index, y=&#39;count&#39;,
               text=&#39;count&#39;,
               labels={&#39;count&#39;: &#39;Number of infos&#39;,
                       &#39;x&#39;: &#39;Tags&#39;})
fig_Y.update_traces(texttemplate=&#39;%{text}&#39;)
&lt;/code&gt;&lt;/pre&gt;



&lt;html&gt;
&lt;head&gt;&lt;meta charset=&#34;utf-8&#34; /&gt;&lt;/head&gt;
&lt;body&gt;
    &lt;div&gt;
            &lt;script src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&#34;&gt;&lt;/script&gt;&lt;script type=&#34;text/javascript&#34;&gt;if (window.MathJax) {MathJax.Hub.Config({SVG: {font: &#34;STIX-Web&#34;}});}&lt;/script&gt;
                &lt;script type=&#34;text/javascript&#34;&gt;window.PlotlyConfig = {MathJaxConfig: &#39;local&#39;};&lt;/script&gt;
        &lt;script src=&#34;https://cdn.plot.ly/plotly-latest.min.js&#34;&gt;&lt;/script&gt;    
            &lt;div id=&#34;1c689a44-bec8-4749-bd6f-2a3cc74c5cd9&#34; class=&#34;plotly-graph-div&#34; style=&#34;height:525px; width:100%;&#34;&gt;&lt;/div&gt;
            &lt;script type=&#34;text/javascript&#34;&gt;
                
                    window.PLOTLYENV=window.PLOTLYENV || {};
                    
                if (document.getElementById(&#34;1c689a44-bec8-4749-bd6f-2a3cc74c5cd9&#34;)) {
                    Plotly.newPlot(
                        &#39;1c689a44-bec8-4749-bd6f-2a3cc74c5cd9&#39;,
                        [{&#34;alignmentgroup&#34;: &#34;True&#34;, &#34;hoverlabel&#34;: {&#34;namelength&#34;: 0}, &#34;hovertemplate&#34;: &#34;Tags=%{x}&lt;br&gt;Number of infos=%{text}&#34;, &#34;legendgroup&#34;: &#34;&#34;, &#34;marker&#34;: {&#34;color&#34;: &#34;#636efa&#34;}, &#34;name&#34;: &#34;&#34;, &#34;offsetgroup&#34;: &#34;&#34;, &#34;orientation&#34;: &#34;v&#34;, &#34;showlegend&#34;: false, &#34;text&#34;: [425.0, 272.0, 181.0, 155.0, 129.0, 89.0, 80.0, 77.0, 67.0, 60.0, 53.0, 50.0, 48.0, 47.0, 42.0, 39.0, 39.0, 39.0, 35.0, 35.0, 33.0, 32.0, 32.0, 32.0, 31.0, 31.0, 31.0, 31.0, 30.0, 29.0, 28.0, 27.0, 27.0, 26.0, 26.0, 25.0, 25.0, 25.0, 22.0, 22.0, 21.0], &#34;textposition&#34;: &#34;auto&#34;, &#34;texttemplate&#34;: &#34;%{text}&#34;, &#34;type&#34;: &#34;bar&#34;, &#34;x&#34;: [&#34;python&#34;, &#34;golang&#34;, &#34;web&#34;, &#34;javascript&#34;, &#34;machine-learning&#34;, &#34;microservices&#34;, &#34;deep-learning&#34;, &#34;neural-networks&#34;, &#34;api&#34;, &#34;data-science&#34;, &#34;java&#34;, &#34;node.js&#34;, &#34;testing&#34;, &#34;concurrency&#34;, &#34;vue.js&#34;, &#34;system-architecture&#34;, &#34;react&#34;, &#34;db&#34;, &#34;compiler&#34;, &#34;docker&#34;, &#34;http&#34;, &#34;git&#34;, &#34;kubernetes&#34;, &#34;rust&#34;, &#34;restful&#34;, &#34;data-visualization&#34;, &#34;cpp&#34;, &#34;django&#34;, &#34;nlp&#34;, &#34;oop&#34;, &#34;kafka&#34;, &#34;graphql&#34;, &#34;angular&#34;, &#34;programming&#34;, &#34;linux&#34;, &#34;css&#34;, &#34;frontend&#34;, &#34;security&#34;, &#34;functional-programming&#34;, &#34;interpreter&#34;, &#34;distributed-system&#34;], &#34;xaxis&#34;: &#34;x&#34;, &#34;y&#34;: [425, 272, 181, 155, 129, 89, 80, 77, 67, 60, 53, 50, 48, 47, 42, 39, 39, 39, 35, 35, 33, 32, 32, 32, 31, 31, 31, 31, 30, 29, 28, 27, 27, 26, 26, 25, 25, 25, 22, 22, 21], &#34;yaxis&#34;: &#34;y&#34;}],
                        {&#34;barmode&#34;: &#34;relative&#34;, &#34;legend&#34;: {&#34;tracegroupgap&#34;: 0}, &#34;margin&#34;: {&#34;t&#34;: 60}, &#34;template&#34;: {&#34;data&#34;: {&#34;bar&#34;: [{&#34;error_x&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;error_y&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;marker&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#E5ECF6&#34;, &#34;width&#34;: 0.5}}, &#34;type&#34;: &#34;bar&#34;}], &#34;barpolar&#34;: [{&#34;marker&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#E5ECF6&#34;, &#34;width&#34;: 0.5}}, &#34;type&#34;: &#34;barpolar&#34;}], &#34;carpet&#34;: [{&#34;aaxis&#34;: {&#34;endlinecolor&#34;: &#34;#2a3f5f&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;minorgridcolor&#34;: &#34;white&#34;, &#34;startlinecolor&#34;: &#34;#2a3f5f&#34;}, &#34;baxis&#34;: {&#34;endlinecolor&#34;: &#34;#2a3f5f&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;minorgridcolor&#34;: &#34;white&#34;, &#34;startlinecolor&#34;: &#34;#2a3f5f&#34;}, &#34;type&#34;: &#34;carpet&#34;}], &#34;choropleth&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;choropleth&#34;}], &#34;contour&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;contour&#34;}], &#34;contourcarpet&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;contourcarpet&#34;}], &#34;heatmap&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;heatmap&#34;}], &#34;heatmapgl&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;heatmapgl&#34;}], &#34;histogram&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;histogram&#34;}], &#34;histogram2d&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;histogram2d&#34;}], &#34;histogram2dcontour&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;histogram2dcontour&#34;}], &#34;mesh3d&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;mesh3d&#34;}], &#34;parcoords&#34;: [{&#34;line&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;parcoords&#34;}], &#34;pie&#34;: [{&#34;automargin&#34;: true, &#34;type&#34;: &#34;pie&#34;}], &#34;scatter&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatter&#34;}], &#34;scatter3d&#34;: [{&#34;line&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatter3d&#34;}], &#34;scattercarpet&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattercarpet&#34;}], &#34;scattergeo&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattergeo&#34;}], &#34;scattergl&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattergl&#34;}], &#34;scattermapbox&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattermapbox&#34;}], &#34;scatterpolar&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterpolar&#34;}], &#34;scatterpolargl&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterpolargl&#34;}], &#34;scatterternary&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterternary&#34;}], &#34;surface&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;surface&#34;}], &#34;table&#34;: [{&#34;cells&#34;: {&#34;fill&#34;: {&#34;color&#34;: &#34;#EBF0F8&#34;}, &#34;line&#34;: {&#34;color&#34;: &#34;white&#34;}}, &#34;header&#34;: {&#34;fill&#34;: {&#34;color&#34;: &#34;#C8D4E3&#34;}, &#34;line&#34;: {&#34;color&#34;: &#34;white&#34;}}, &#34;type&#34;: &#34;table&#34;}]}, &#34;layout&#34;: {&#34;annotationdefaults&#34;: {&#34;arrowcolor&#34;: &#34;#2a3f5f&#34;, &#34;arrowhead&#34;: 0, &#34;arrowwidth&#34;: 1}, &#34;coloraxis&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;colorscale&#34;: {&#34;diverging&#34;: [[0, &#34;#8e0152&#34;], [0.1, &#34;#c51b7d&#34;], [0.2, &#34;#de77ae&#34;], [0.3, &#34;#f1b6da&#34;], [0.4, &#34;#fde0ef&#34;], [0.5, &#34;#f7f7f7&#34;], [0.6, &#34;#e6f5d0&#34;], [0.7, &#34;#b8e186&#34;], [0.8, &#34;#7fbc41&#34;], [0.9, &#34;#4d9221&#34;], [1, &#34;#276419&#34;]], &#34;sequential&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;sequentialminus&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]]}, &#34;colorway&#34;: [&#34;#636efa&#34;, &#34;#EF553B&#34;, &#34;#00cc96&#34;, &#34;#ab63fa&#34;, &#34;#FFA15A&#34;, &#34;#19d3f3&#34;, &#34;#FF6692&#34;, &#34;#B6E880&#34;, &#34;#FF97FF&#34;, &#34;#FECB52&#34;], &#34;font&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;geo&#34;: {&#34;bgcolor&#34;: &#34;white&#34;, &#34;lakecolor&#34;: &#34;white&#34;, &#34;landcolor&#34;: &#34;#E5ECF6&#34;, &#34;showlakes&#34;: true, &#34;showland&#34;: true, &#34;subunitcolor&#34;: &#34;white&#34;}, &#34;hoverlabel&#34;: {&#34;align&#34;: &#34;left&#34;}, &#34;hovermode&#34;: &#34;closest&#34;, &#34;mapbox&#34;: {&#34;style&#34;: &#34;light&#34;}, &#34;paper_bgcolor&#34;: &#34;white&#34;, &#34;plot_bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;polar&#34;: {&#34;angularaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;radialaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}}, &#34;scene&#34;: {&#34;xaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}, &#34;yaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}, &#34;zaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}}, &#34;shapedefaults&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}}, &#34;ternary&#34;: {&#34;aaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;baxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;caxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}}, &#34;title&#34;: {&#34;x&#34;: 0.05}, &#34;xaxis&#34;: {&#34;automargin&#34;: true, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;, &#34;title&#34;: {&#34;standoff&#34;: 15}, &#34;zerolinecolor&#34;: &#34;white&#34;, &#34;zerolinewidth&#34;: 2}, &#34;yaxis&#34;: {&#34;automargin&#34;: true, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;, &#34;title&#34;: {&#34;standoff&#34;: 15}, &#34;zerolinecolor&#34;: &#34;white&#34;, &#34;zerolinewidth&#34;: 2}}}, &#34;xaxis&#34;: {&#34;anchor&#34;: &#34;y&#34;, &#34;domain&#34;: [0.0, 1.0], &#34;title&#34;: {&#34;text&#34;: &#34;Tags&#34;}}, &#34;yaxis&#34;: {&#34;anchor&#34;: &#34;x&#34;, &#34;domain&#34;: [0.0, 1.0], &#34;title&#34;: {&#34;text&#34;: &#34;Number of infos&#34;}}},
                        {&#34;responsive&#34;: true}
                    ).then(function(){
                            
var gd = document.getElementById(&#39;1c689a44-bec8-4749-bd6f-2a3cc74c5cd9&#39;);
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === &#39;none&#39;) {{
            console.log([gd, &#39;removed!&#39;]);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest(&#39;#notebook-container&#39;);
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest(&#39;.output&#39;);
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })
                };
                
            &lt;/script&gt;
        &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;


&lt;pre&gt;&lt;code&gt;test_size = 0.3
train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split(
    ds.data, ds.target, test_size=test_size, cols=ds.data.columns)

train_features, train_labels = dataset.augmented_samples(
    train_features, train_labels, level=2, crop_ratio=0.1)

batch_size = 128
model_name = &amp;quot;google/bert_uncased_L-4_H-256_A-4&amp;quot;

train_features, test_features = bert_transform(
    train_features, test_features, col_text, model_name, batch_size)

clf = OneVsRestClassifier(LinearSVC())

C_OPTIONS = [0.1, 1]

parameters = {
    &#39;estimator__penalty&#39;: [&#39;l2&#39;],
    &#39;estimator__dual&#39;: [True],
    &#39;estimator__C&#39;: C_OPTIONS,
}

micro_f05_sco = metrics.make_scorer(
    metrics.fbeta_score, beta=0.5, average=&#39;micro&#39;)

gs_clf = GridSearchCV(clf, parameters,
                      scoring=micro_f05_sco,
                      cv=3, n_jobs=-1)

gs_clf.fit(train_features, train_labels)
print(f&#39;Best params in CV: {gs_clf.best_params_}&#39;)
print(f&#39;Best score in CV: {gs_clf.best_score_}&#39;)

Y_predicted = gs_clf.predict(test_features)

classification_report_avg(test_labels, Y_predicted)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Best params in CV: {&#39;estimator__C&#39;: 0.1, &#39;estimator__dual&#39;: True, &#39;estimator__penalty&#39;: &#39;l2&#39;}
Best score in CV: 0.8943719982878996
              precision    recall  f1-score  support
micro avg      0.593583  0.435294  0.502262    765.0
macro avg      0.523965  0.361293  0.416650    765.0
weighted avg   0.586632  0.435294  0.490803    765.0
samples avg    0.458254  0.472063  0.444127    765.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The filtering of tags made the averages of recall higher, but made the precision lower. The macro average goes up as there&amp;rsquo;re much fewer tags.&lt;/p&gt;

&lt;h2 id=&#34;fine-tuning-bert-model&#34;&gt;Fine-tuning BERT model&lt;/h2&gt;

&lt;p&gt;The next step is to see if we can make some progress by fine-tuning the BERT-Mini model. As for a comparable result, the fine-tuning training will be using the same dataset that filtered of tags appear at least in 20 infos. The final classifier model will also be the same of SVM with Linear kernel feeded by the embeddings from the fine-tuned BERT-Mini.&lt;/p&gt;

&lt;p&gt;The processing of fine-tuning refers much to &lt;a href=&#34;https://mccormickml.com/2019/07/22/BERT-fine-tuning/&#34; target=&#34;_blank&#34;&gt;Chris McCormick&amp;rsquo;s post&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;col_text = &#39;description&#39;
ds_param = dict(from_batch_cache=&#39;info&#39;, lan=&#39;en&#39;,
                concate_title=True,
                filter_tags_threshold=20)
ds = dataset.ds_info_tags(**ds_param)

test_size = 0.3
train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split(
    ds.data, ds.target, test_size=test_size, cols=ds.data.columns)

train_features, train_labels = dataset.augmented_samples(
    train_features, train_labels, level=2, crop_ratio=0.1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;BertForSequenceMultiLabelClassification&lt;/code&gt; class defined below is basically a copy of the &lt;code&gt;BertForSequenceClassification&lt;/code&gt; class in huggingface&amp;rsquo;s &lt;code&gt;Transformers&lt;/code&gt;, only with a small change of adding &lt;code&gt;sigmoid&lt;/code&gt; the logits from classification and adding &lt;code&gt;labels = torch.max(labels, 1)[1]&lt;/code&gt; in &lt;code&gt;forward&lt;/code&gt; for supporting multilabel.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class BertForSequenceMultiLabelClassification(BertPreTrainedModel):
    def __init__(self, config):
        super(BertForSequenceMultiLabelClassification, self).__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)

        self.init_weights()

    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,
                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):

        outputs = self.bert(input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids,
                            position_ids=position_ids,
                            head_mask=head_mask,
                            inputs_embeds=inputs_embeds)

        pooled_output = outputs[1]

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        logtis = torch.sigmoid(logits)
        # add hidden states and attention if they are here
        outputs = (logits,) + outputs[2:]

        if labels is not None:
            if self.num_labels == 1:
                #  We are doing regression
                loss_fct = nn.MSELoss()
                loss = loss_fct(logits.view(-1), labels.view(-1))
            else:
                loss_fct = nn.CrossEntropyLoss()

                labels = torch.max(labels, 1)[1]

                loss = loss_fct(
                    logits.view(-1, self.num_labels), labels.view(-1))
            outputs = (loss,) + outputs

        return outputs  # (loss), logits, (hidden_states), (attentions)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;DEVICE = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
n_classes = train_labels.shape[1]
batch_size: int = 16
epochs: int = 4

model_name = download_once_pretrained_transformers(
    &amp;quot;google/bert_uncased_L-4_H-256_A-4&amp;quot;)

model = BertForSequenceMultiLabelClassification.from_pretrained(
    model_name,
    num_labels=n_classes,
    output_attentions=False,  
    output_hidden_states=False,
)

model.to(DEVICE)

# Prepare optimizer and schedule (linear warmup and decay)
no_decay = [&amp;quot;bias&amp;quot;, &amp;quot;LayerNorm.weight&amp;quot;]
optimizer_grouped_parameters = [
    {&amp;quot;params&amp;quot;: [p for n, p in model.named_parameters() if not any(
        nd in n for nd in no_decay)], &amp;quot;weight_decay&amp;quot;: 0.1,
     },
    {&amp;quot;params&amp;quot;: [p for n, p in model.named_parameters() if any(
        nd in n for nd in no_decay)], &amp;quot;weight_decay&amp;quot;: 0.0},
]

optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5,  eps=1e-8  )

tokenizer, model_notuse = get_tokenizer_model(model_name)

input_ids, attention_mask = bert_tokenize(
    tokenizer, train_features, col_text=col_text)
input_ids_test, attention_mask_test = bert_tokenize(
    tokenizer, test_features, col_text=col_text)

train_set = torch.utils.data.TensorDataset(
    input_ids, attention_mask, torch.Tensor(train_labels))
test_set = torch.utils.data.TensorDataset(
    input_ids_test, attention_mask_test, torch.Tensor(test_labels))

train_loader = torch.utils.data.DataLoader(
    train_set, batch_size=batch_size, sampler=RandomSampler(train_set))
test_loader = torch.utils.data.DataLoader(
    test_set, sampler=SequentialSampler(test_set), batch_size=batch_size)


total_steps = len(train_loader) * epochs

scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=0,  # Default value in run_glue.py
                                            num_training_steps=total_steps)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;training_stats = []


def best_prec_score(true_labels, predictions):
    fbeta = 0
    thr_bst = 0
    for thr in range(0, 6):
        Y_predicted = (predictions &amp;gt; (thr * 0.1))

        f = metrics.average_precision_score(
            true_labels, Y_predicted, average=&#39;micro&#39;)
        if f &amp;gt; fbeta:
            fbeta = f
            thr_bst = thr * 0.1

    return fbeta, thr


def train():
    model.train()

    total_train_loss = 0

    for step, (input_ids, masks, labels) in enumerate(train_loader):
        input_ids, masks, labels = input_ids.to(
            DEVICE), masks.to(DEVICE), labels.to(DEVICE)

        model.zero_grad()
        loss, logits = model(input_ids, token_type_ids=None,
                             attention_mask=masks, labels=labels)

        total_train_loss += loss.item()
        loss.backward()

        # Clip the norm of the gradients to 1.0.
        # This is to help prevent the &amp;quot;exploding gradients&amp;quot; problem.
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()

    avg_train_loss = total_train_loss / len(train_loader)
    print(&amp;quot;Train loss: {0:.2f}&amp;quot;.format(avg_train_loss))


def val():
    model.eval()

    val_loss = 0

    y_pred, y_true = [], []
    # Evaluate data for one epoch
    for (input_ids, masks, labels) in test_loader:

        input_ids, masks, labels = input_ids.to(
            DEVICE), masks.to(DEVICE), labels.to(DEVICE)

        with torch.no_grad():
            (loss, logits) = model(input_ids,
                                   token_type_ids=None,
                                   attention_mask=masks,
                                   labels=labels)

        val_loss += loss.item()

        logits = logits.detach().cpu().numpy()
        label_ids = labels.to(&#39;cpu&#39;).numpy()

        y_pred += logits.tolist()
        y_true += label_ids.tolist()

    bes_val_prec, bes_val_prec_thr = best_prec_score(
        np.array(y_true), np.array(y_pred))
    y_predicted = (np.array(y_pred) &amp;gt; 0.5)

    avg_val_loss = val_loss / len(test_loader)

    print(&amp;quot;Val loss: {0:.2f}&amp;quot;.format(avg_val_loss))
    print(&amp;quot;best prec: {0:.4f}, thr: {1}&amp;quot;.format(
        bes_val_prec, bes_val_prec_thr))
    print(classification_report_avg(y_true, y_predicted))

for ep in range(epochs):
    print(f&#39;-------------- Epoch: {ep+1}/{epochs} --------------&#39;)
    train()
    val()

print(&#39;-------------- Completed --------------&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;-------------- Epoch: 1/4 --------------
Train loss: 2.94
Val loss: 2.35
best prec: 0.1540, thr: 5
              precision    recall  f1-score  support
micro avg      0.233079  0.599476  0.335654    764.0
macro avg      0.185025  0.294674  0.196651    764.0
weighted avg   0.225475  0.599476  0.292065    764.0
samples avg    0.252645  0.634959  0.342227    764.0
-------------- Epoch: 2/4 --------------
Train loss: 2.14
Val loss: 1.92
best prec: 0.1848, thr: 5
              precision    recall  f1-score  support
micro avg      0.255676  0.678010  0.371326    764.0
macro avg      0.381630  0.448064  0.303961    764.0
weighted avg   0.328057  0.678010  0.355185    764.0
samples avg    0.273901  0.735660  0.379705    764.0
-------------- Epoch: 3/4 --------------
Train loss: 1.78
Val loss: 1.74
best prec: 0.1881, thr: 5
              precision    recall  f1-score  support
micro avg      0.248974  0.714660  0.369293    764.0
macro avg      0.272232  0.524172  0.306814    764.0
weighted avg   0.275572  0.714660  0.364002    764.0
samples avg    0.273428  0.776291  0.383235    764.0
-------------- Epoch: 4/4 --------------
Train loss: 1.61
Val loss: 1.68
best prec: 0.1882, thr: 5
              precision    recall  f1-score  support
micro avg      0.244105  0.731675  0.366077    764.0
macro avg      0.288398  0.552318  0.310797    764.0
weighted avg   0.294521  0.731675  0.369942    764.0
samples avg    0.267708  0.795730  0.381341    764.0
-------------- Completed --------------
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the fine-tuned model for later encoding.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from transformers import WEIGHTS_NAME, CONFIG_NAME, BertTokenizer

output_dir = &amp;quot;./data/models/bert_finetuned_tagthr_20/&amp;quot;

if not os.path.exists(output_dir):
    os.makedirs(output_dir)
# Step 1: Save a model, configuration and vocabulary that you have fine-tuned

# If we have a distributed model, save only the encapsulated model
# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)
model_to_save = model.module if hasattr(model, &#39;module&#39;) else model

# If we save using the predefined names, we can load using `from_pretrained`
output_model_file = os.path.join(output_dir, WEIGHTS_NAME)
output_config_file = os.path.join(output_dir, CONFIG_NAME)

torch.save(model_to_save.state_dict(), output_model_file)
model_to_save.config.to_json_file(output_config_file)
tokenizer.save_vocabulary(output_dir)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(&#39;./data/models/bert_finetuned_tagthr_20/vocab.txt&#39;,)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s use the fine-tuned model to get the embeddings for the same SVM classification.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;col_text = &#39;description&#39;
ds_param = dict(from_batch_cache=&#39;info&#39;, lan=&#39;en&#39;,
                concate_title=True,
                filter_tags_threshold=20)
ds = dataset.ds_info_tags(**ds_param)

test_size = 0.3
train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split(
    ds.data, ds.target, test_size=test_size, cols=ds.data.columns)

train_features, train_labels = dataset.augmented_samples(
    train_features, train_labels, level=2, crop_ratio=0.1)

batch_size = 128
model_name = output_dir

train_features, test_features = bert_transform(
    train_features, test_features, col_text, model_name, batch_size)


clf = OneVsRestClassifier(LinearSVC())

C_OPTIONS = [0.1, 1, 10]

parameters = {
    &#39;estimator__penalty&#39;: [&#39;l2&#39;],
    &#39;estimator__dual&#39;: [True],
    &#39;estimator__C&#39;: C_OPTIONS,
}

micro_f05_sco = metrics.make_scorer(
    metrics.fbeta_score, beta=0.5, average=&#39;micro&#39;)

gs_clf = GridSearchCV(clf, parameters,
                      scoring=micro_f05_sco,
                      cv=3, n_jobs=-1)

gs_clf.fit(train_features, train_labels)

print(gs_clf.best_params_)
print(gs_clf.best_score_)

Y_predicted = gs_clf.predict(test_features)

report = metrics.classification_report(
    test_labels, Y_predicted, output_dict=True)
df_report = pd.DataFrame(report).transpose()
cols_avg = [&#39;micro avg&#39;, &#39;macro avg&#39;, &#39;weighted avg&#39;, &#39;samples avg&#39;]
df_report.loc[cols_avg]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;estimator__C&#39;: 0.1, &#39;estimator__dual&#39;: True, &#39;estimator__penalty&#39;: &#39;l2&#39;}
0.945576388765271
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;precision&lt;/th&gt;
      &lt;th&gt;recall&lt;/th&gt;
      &lt;th&gt;f1-score&lt;/th&gt;
      &lt;th&gt;support&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;micro avg&lt;/th&gt;
      &lt;td&gt;0.793605&lt;/td&gt;
      &lt;td&gt;0.714660&lt;/td&gt;
      &lt;td&gt;0.752066&lt;/td&gt;
      &lt;td&gt;764.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;macro avg&lt;/th&gt;
      &lt;td&gt;0.757259&lt;/td&gt;
      &lt;td&gt;0.642333&lt;/td&gt;
      &lt;td&gt;0.671768&lt;/td&gt;
      &lt;td&gt;764.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;weighted avg&lt;/th&gt;
      &lt;td&gt;0.782557&lt;/td&gt;
      &lt;td&gt;0.714660&lt;/td&gt;
      &lt;td&gt;0.732094&lt;/td&gt;
      &lt;td&gt;764.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;samples avg&lt;/th&gt;
      &lt;td&gt;0.798664&lt;/td&gt;
      &lt;td&gt;0.762087&lt;/td&gt;
      &lt;td&gt;0.754289&lt;/td&gt;
      &lt;td&gt;764.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;There&amp;rsquo;s quite a big improvement to both precision and recall after fine-tuning. This result makes the model quite usable.&lt;/p&gt;

&lt;h1 id=&#34;comeback-test-with-tf-idf&#34;&gt;Comeback test with tf-idf&lt;/h1&gt;

&lt;p&gt;Comparing to the early post that the model uses tf-idf to transform the text, we&amp;rsquo;ve made some changes to the dataset loading, spliting and augmentation. I&amp;rsquo;m curious to see if these changes would improve the performance when using tf-idf other than BERT-Mini.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start with samples only in English still.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;col_text = &#39;description&#39;

ds_param = dict(from_batch_cache=&#39;info&#39;, lan=&#39;en&#39;,
                concate_title=True,
                filter_tags_threshold=20)
ds = dataset.ds_info_tags(**ds_param)

train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split(
    ds.data, ds.target, test_size=0.3, cols=ds.data.columns)

train_features, train_labels = dataset.augmented_samples(
    train_features, train_labels, level=4, crop_ratio=0.2)

clf = Pipeline([
    (&#39;vect&#39;, TfidfVectorizer(use_idf=True, max_df=0.8)),
    (&#39;clf&#39;, OneVsRestClassifier(LinearSVC(penalty=&#39;l2&#39;, dual=True))),
])

C_OPTIONS = [0.1, 1, 10]

parameters = {
    &#39;vect__ngram_range&#39;: [(1, 4)],
    &#39;clf__estimator__C&#39;: C_OPTIONS,
}
gs_clf = GridSearchCV(clf, parameters, cv=3, n_jobs=-1)
gs_clf.fit(train_features[col_text], train_labels)

print(gs_clf.best_params_)
print(gs_clf.best_score_)

Y_predicted = gs_clf.predict(test_features[col_text])

classification_report_avg(test_labels, Y_predicted)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;clf__estimator__C&#39;: 10, &#39;vect__ngram_range&#39;: (1, 4)}
0.9986905637969986
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;precision&lt;/th&gt;
      &lt;th&gt;recall&lt;/th&gt;
      &lt;th&gt;f1-score&lt;/th&gt;
      &lt;th&gt;support&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;micro avg&lt;/th&gt;
      &lt;td&gt;0.955782&lt;/td&gt;
      &lt;td&gt;0.367801&lt;/td&gt;
      &lt;td&gt;0.531191&lt;/td&gt;
      &lt;td&gt;764.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;macro avg&lt;/th&gt;
      &lt;td&gt;0.740347&lt;/td&gt;
      &lt;td&gt;0.250587&lt;/td&gt;
      &lt;td&gt;0.353632&lt;/td&gt;
      &lt;td&gt;764.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;weighted avg&lt;/th&gt;
      &lt;td&gt;0.847419&lt;/td&gt;
      &lt;td&gt;0.367801&lt;/td&gt;
      &lt;td&gt;0.487887&lt;/td&gt;
      &lt;td&gt;764.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;samples avg&lt;/th&gt;
      &lt;td&gt;0.452608&lt;/td&gt;
      &lt;td&gt;0.396469&lt;/td&gt;
      &lt;td&gt;0.412913&lt;/td&gt;
      &lt;td&gt;764.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now let&amp;rsquo;s try samples in both English and Chinese.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;col_text = &#39;description&#39;

ds_param = dict(from_batch_cache=&#39;info&#39;, lan=None,
                concate_title=True,
                filter_tags_threshold=20)
ds = dataset.ds_info_tags(**ds_param)

train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split(
    ds.data, ds.target, test_size=0.3, cols=ds.data.columns)

train_features, train_labels = dataset.augmented_samples(
    train_features, train_labels, level=4, crop_ratio=0.2)

clf = Pipeline([
    (&#39;vect&#39;, TfidfVectorizer(use_idf=True, max_df=0.8)),
    (&#39;clf&#39;, OneVsRestClassifier(LinearSVC(penalty=&#39;l2&#39;, dual=True))),
])

C_OPTIONS = [0.1, 1, 10]

parameters = {
    &#39;vect__ngram_range&#39;: [(1, 4)],
    &#39;clf__estimator__C&#39;: C_OPTIONS,
}
gs_clf = GridSearchCV(clf, parameters, cv=3, n_jobs=-1)
gs_clf.fit(train_features[col_text], train_labels)

print(gs_clf.best_params_)
print(gs_clf.best_score_)

Y_predicted = gs_clf.predict(test_features[col_text])

classification_report_avg(test_labels, Y_predicted)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;clf__estimator__C&#39;: 10, &#39;vect__ngram_range&#39;: (1, 4)}
0.9962557077625571
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;precision&lt;/th&gt;
      &lt;th&gt;recall&lt;/th&gt;
      &lt;th&gt;f1-score&lt;/th&gt;
      &lt;th&gt;support&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;micro avg&lt;/th&gt;
      &lt;td&gt;0.884273&lt;/td&gt;
      &lt;td&gt;0.417952&lt;/td&gt;
      &lt;td&gt;0.567619&lt;/td&gt;
      &lt;td&gt;1426.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;macro avg&lt;/th&gt;
      &lt;td&gt;0.804614&lt;/td&gt;
      &lt;td&gt;0.311396&lt;/td&gt;
      &lt;td&gt;0.423867&lt;/td&gt;
      &lt;td&gt;1426.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;weighted avg&lt;/th&gt;
      &lt;td&gt;0.849494&lt;/td&gt;
      &lt;td&gt;0.417952&lt;/td&gt;
      &lt;td&gt;0.532041&lt;/td&gt;
      &lt;td&gt;1426.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;samples avg&lt;/th&gt;
      &lt;td&gt;0.487522&lt;/td&gt;
      &lt;td&gt;0.433421&lt;/td&gt;
      &lt;td&gt;0.446447&lt;/td&gt;
      &lt;td&gt;1426.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We can see that, for both the models, the micro average precision is quite high and the recalls are still low. However, the macro averages are much better since we filtered out minority tags. The model trained on samples with both languages has a lower precisions but higher recalls.&lt;/p&gt;

&lt;p&gt;Now lets see how would it perform training on the fulltext.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;col_text = &#39;fulltext&#39;

ds_param = dict(from_batch_cache=&#39;fulltext&#39;, lan=None,
                concate_title=True,
                filter_tags_threshold=20)
ds = dataset.ds_info_tags(**ds_param)

train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split(
    ds.data, ds.target, test_size=0.3, cols=ds.data.columns)

train_features, train_labels = dataset.augmented_samples(
    train_features, train_labels, col=col_text, level=4, crop_ratio=0.2)

clf = Pipeline([
    (&#39;vect&#39;, TfidfVectorizer(use_idf=True, max_df=0.8)),
    (&#39;clf&#39;, OneVsRestClassifier(LinearSVC(penalty=&#39;l2&#39;, dual=True))),
])

C_OPTIONS = [0.1, 1, 10]

parameters = {
    &#39;vect__ngram_range&#39;: [(1, 4)],
    &#39;clf__estimator__C&#39;: C_OPTIONS,
}
gs_clf = GridSearchCV(clf, parameters, cv=3, n_jobs=-1)
gs_clf.fit(train_features[col_text], train_labels)

print(gs_clf.best_params_)
print(gs_clf.best_score_)

Y_predicted = gs_clf.predict(test_features[col_text])

classification_report_avg(test_labels, Y_predicted)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;clf__estimator__C&#39;: 10, &#39;vect__ngram_range&#39;: (1, 4)}
0.9719756244169426
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;precision&lt;/th&gt;
      &lt;th&gt;recall&lt;/th&gt;
      &lt;th&gt;f1-score&lt;/th&gt;
      &lt;th&gt;support&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;micro avg&lt;/th&gt;
      &lt;td&gt;0.891927&lt;/td&gt;
      &lt;td&gt;0.479692&lt;/td&gt;
      &lt;td&gt;0.623862&lt;/td&gt;
      &lt;td&gt;1428.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;macro avg&lt;/th&gt;
      &lt;td&gt;0.858733&lt;/td&gt;
      &lt;td&gt;0.385982&lt;/td&gt;
      &lt;td&gt;0.502745&lt;/td&gt;
      &lt;td&gt;1428.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;weighted avg&lt;/th&gt;
      &lt;td&gt;0.864123&lt;/td&gt;
      &lt;td&gt;0.479692&lt;/td&gt;
      &lt;td&gt;0.584996&lt;/td&gt;
      &lt;td&gt;1428.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;samples avg&lt;/th&gt;
      &lt;td&gt;0.559050&lt;/td&gt;
      &lt;td&gt;0.494357&lt;/td&gt;
      &lt;td&gt;0.510951&lt;/td&gt;
      &lt;td&gt;1428.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The model trained on the fulltext is slightly better, but the training time is way much longer. A tuning on the length of the partial text could be explored with both tf-idf and BERT.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Walk Through of the IEEE-CIS Fraud Detection Challenge</title>
      <link>https://pcx.linkedinfo.co/post/fraud-detection/</link>
      <pubDate>Mon, 10 Feb 2020 10:07:11 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/post/fraud-detection/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;This is a brief walk through of the Kaggle challenge IEEE-CIS Fraud Detection. The process in this post is not meant to compete the top solution by performing an extre feature engineering and a greedy search for the best model with hyper-parameters. This is just to walk through the problem and demonstrate a relatively good solution, by doing feature analysis and a few experiments with reference to other&amp;rsquo;s methods.&lt;/p&gt;

&lt;p&gt;The problem of this challenge is to detect payment frauds by using the data of the transactions and identities. The performance of the prediction is evaluated on &lt;em&gt;ROC AUC&lt;/em&gt;. The reason why this measure is suitable for this problem (rather than Precision-Recall) can refer to the discussion &lt;a href=&#34;https://www.kaggle.com/c/ieee-fraud-detection/discussion/99982&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;look-into-the-data&#34;&gt;Look into the data&lt;/h1&gt;

&lt;p&gt;The provided dataset is broken into two files named &lt;code&gt;identity&lt;/code&gt; and &lt;code&gt;transaction&lt;/code&gt;, which are joined by &lt;code&gt;TransactionID&lt;/code&gt; (note that NOT all the transactions have corresponding identity information).&lt;/p&gt;

&lt;h3 id=&#34;transaction-table&#34;&gt;Transaction Table&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;TransactionDT: timedelta from a given reference datetime (not an actual
timestamp),  the number of seconds in a day (60 * 60 * 24 = 86400)&lt;/li&gt;
&lt;li&gt;TransactionAMT: transaction payment amount in USD&lt;/li&gt;
&lt;li&gt;ProductCD: product code, the product for each transaction&lt;/li&gt;
&lt;li&gt;card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.&lt;/li&gt;
&lt;li&gt;addr: address&lt;/li&gt;
&lt;li&gt;dist: distance&lt;/li&gt;
&lt;li&gt;P_ and (R__) emaildomain: purchaser and recipient email domain&lt;/li&gt;
&lt;li&gt;C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.&lt;/li&gt;
&lt;li&gt;D1-D15: timedelta, such as days between previous transaction, etc.&lt;/li&gt;
&lt;li&gt;M1-M9: match, such as names on card and address, etc.&lt;/li&gt;
&lt;li&gt;Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Among these variables, categorical variables are:
   - ProductCD
   - card1 - card6
   - addr1, addr2
   - Pemaildomain Remaildomain
   - M1 - M9&lt;/p&gt;

&lt;h3 id=&#34;identity-table&#34;&gt;Identity Table&lt;/h3&gt;

&lt;p&gt;All the variable in this table are categorical:
   - DeviceType
   - DeviceInfo
   - id12 - id38&lt;/p&gt;

&lt;p&gt;A more detailed explanation of the data can be found in the reply of &lt;a href=&#34;https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203&#34; target=&#34;_blank&#34;&gt;this discussion&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s have a close look at the data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = &amp;quot;all&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import pandas as pd
import plotly.express as px


DATA_DIR = &#39;/content/drive/My Drive/colab-data/fraud detect/data&#39;

tran_train = reduce_mem_usage(pd.read_csv(f&#39;{DATA_DIR}/train_transaction.csv&#39;))
id_train = reduce_mem_usage(pd.read_csv(f&#39;{DATA_DIR}/train_identity.csv&#39;))

tran_train.info()
tran_train.head()
id_train.info()
id_train.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Mem. usage decreased to 542.35 Mb (69.4% reduction)
Mem. usage decreased to 25.86 Mb (42.7% reduction)
&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 590540 entries, 0 to 590539
Columns: 394 entries, TransactionID to V339
dtypes: float16(332), float32(44), int16(1), int32(2), int8(1), object(14)
memory usage: 542.3+ MB
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TransactionID&lt;/th&gt;
      &lt;th&gt;isFraud&lt;/th&gt;
      &lt;th&gt;TransactionDT&lt;/th&gt;
      &lt;th&gt;TransactionAmt&lt;/th&gt;
      &lt;th&gt;ProductCD&lt;/th&gt;
      &lt;th&gt;card1&lt;/th&gt;
      &lt;th&gt;card2&lt;/th&gt;
      &lt;th&gt;card3&lt;/th&gt;
      &lt;th&gt;card4&lt;/th&gt;
      &lt;th&gt;card5&lt;/th&gt;
      &lt;th&gt;card6&lt;/th&gt;
      &lt;th&gt;addr1&lt;/th&gt;
      &lt;th&gt;addr2&lt;/th&gt;
      &lt;th&gt;dist1&lt;/th&gt;
      &lt;th&gt;dist2&lt;/th&gt;
      &lt;th&gt;P_emaildomain&lt;/th&gt;
      &lt;th&gt;R_emaildomain&lt;/th&gt;
      &lt;th&gt;C1&lt;/th&gt;
      &lt;th&gt;C2&lt;/th&gt;
      &lt;th&gt;C3&lt;/th&gt;
      &lt;th&gt;C4&lt;/th&gt;
      &lt;th&gt;C5&lt;/th&gt;
      &lt;th&gt;C6&lt;/th&gt;
      &lt;th&gt;C7&lt;/th&gt;
      &lt;th&gt;C8&lt;/th&gt;
      &lt;th&gt;C9&lt;/th&gt;
      &lt;th&gt;C10&lt;/th&gt;
      &lt;th&gt;C11&lt;/th&gt;
      &lt;th&gt;C12&lt;/th&gt;
      &lt;th&gt;C13&lt;/th&gt;
      &lt;th&gt;C14&lt;/th&gt;
      &lt;th&gt;D1&lt;/th&gt;
      &lt;th&gt;D2&lt;/th&gt;
      &lt;th&gt;D3&lt;/th&gt;
      &lt;th&gt;D4&lt;/th&gt;
      &lt;th&gt;D5&lt;/th&gt;
      &lt;th&gt;D6&lt;/th&gt;
      &lt;th&gt;D7&lt;/th&gt;
      &lt;th&gt;D8&lt;/th&gt;
      &lt;th&gt;D9&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;V300&lt;/th&gt;
      &lt;th&gt;V301&lt;/th&gt;
      &lt;th&gt;V302&lt;/th&gt;
      &lt;th&gt;V303&lt;/th&gt;
      &lt;th&gt;V304&lt;/th&gt;
      &lt;th&gt;V305&lt;/th&gt;
      &lt;th&gt;V306&lt;/th&gt;
      &lt;th&gt;V307&lt;/th&gt;
      &lt;th&gt;V308&lt;/th&gt;
      &lt;th&gt;V309&lt;/th&gt;
      &lt;th&gt;V310&lt;/th&gt;
      &lt;th&gt;V311&lt;/th&gt;
      &lt;th&gt;V312&lt;/th&gt;
      &lt;th&gt;V313&lt;/th&gt;
      &lt;th&gt;V314&lt;/th&gt;
      &lt;th&gt;V315&lt;/th&gt;
      &lt;th&gt;V316&lt;/th&gt;
      &lt;th&gt;V317&lt;/th&gt;
      &lt;th&gt;V318&lt;/th&gt;
      &lt;th&gt;V319&lt;/th&gt;
      &lt;th&gt;V320&lt;/th&gt;
      &lt;th&gt;V321&lt;/th&gt;
      &lt;th&gt;V322&lt;/th&gt;
      &lt;th&gt;V323&lt;/th&gt;
      &lt;th&gt;V324&lt;/th&gt;
      &lt;th&gt;V325&lt;/th&gt;
      &lt;th&gt;V326&lt;/th&gt;
      &lt;th&gt;V327&lt;/th&gt;
      &lt;th&gt;V328&lt;/th&gt;
      &lt;th&gt;V329&lt;/th&gt;
      &lt;th&gt;V330&lt;/th&gt;
      &lt;th&gt;V331&lt;/th&gt;
      &lt;th&gt;V332&lt;/th&gt;
      &lt;th&gt;V333&lt;/th&gt;
      &lt;th&gt;V334&lt;/th&gt;
      &lt;th&gt;V335&lt;/th&gt;
      &lt;th&gt;V336&lt;/th&gt;
      &lt;th&gt;V337&lt;/th&gt;
      &lt;th&gt;V338&lt;/th&gt;
      &lt;th&gt;V339&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2987000&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;86400&lt;/td&gt;
      &lt;td&gt;68.5&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;13926&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;150.0&lt;/td&gt;
      &lt;td&gt;discover&lt;/td&gt;
      &lt;td&gt;142.0&lt;/td&gt;
      &lt;td&gt;credit&lt;/td&gt;
      &lt;td&gt;315.0&lt;/td&gt;
      &lt;td&gt;87.0&lt;/td&gt;
      &lt;td&gt;19.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;14.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;13.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;117.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;117.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2987001&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;86401&lt;/td&gt;
      &lt;td&gt;29.0&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;2755&lt;/td&gt;
      &lt;td&gt;404.0&lt;/td&gt;
      &lt;td&gt;150.0&lt;/td&gt;
      &lt;td&gt;mastercard&lt;/td&gt;
      &lt;td&gt;102.0&lt;/td&gt;
      &lt;td&gt;credit&lt;/td&gt;
      &lt;td&gt;325.0&lt;/td&gt;
      &lt;td&gt;87.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;gmail.com&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2987002&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;86469&lt;/td&gt;
      &lt;td&gt;59.0&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;4663&lt;/td&gt;
      &lt;td&gt;490.0&lt;/td&gt;
      &lt;td&gt;150.0&lt;/td&gt;
      &lt;td&gt;visa&lt;/td&gt;
      &lt;td&gt;166.0&lt;/td&gt;
      &lt;td&gt;debit&lt;/td&gt;
      &lt;td&gt;330.0&lt;/td&gt;
      &lt;td&gt;87.0&lt;/td&gt;
      &lt;td&gt;287.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;outlook.com&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2987003&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;86499&lt;/td&gt;
      &lt;td&gt;50.0&lt;/td&gt;
      &lt;td&gt;W&lt;/td&gt;
      &lt;td&gt;18132&lt;/td&gt;
      &lt;td&gt;567.0&lt;/td&gt;
      &lt;td&gt;150.0&lt;/td&gt;
      &lt;td&gt;mastercard&lt;/td&gt;
      &lt;td&gt;117.0&lt;/td&gt;
      &lt;td&gt;debit&lt;/td&gt;
      &lt;td&gt;476.0&lt;/td&gt;
      &lt;td&gt;87.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;yahoo.com&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;25.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;112.0&lt;/td&gt;
      &lt;td&gt;112.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;94.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;50.0&lt;/td&gt;
      &lt;td&gt;1758.0&lt;/td&gt;
      &lt;td&gt;925.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;354.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;135.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;50.0&lt;/td&gt;
      &lt;td&gt;1404.0&lt;/td&gt;
      &lt;td&gt;790.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2987004&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;86506&lt;/td&gt;
      &lt;td&gt;50.0&lt;/td&gt;
      &lt;td&gt;H&lt;/td&gt;
      &lt;td&gt;4497&lt;/td&gt;
      &lt;td&gt;514.0&lt;/td&gt;
      &lt;td&gt;150.0&lt;/td&gt;
      &lt;td&gt;mastercard&lt;/td&gt;
      &lt;td&gt;102.0&lt;/td&gt;
      &lt;td&gt;credit&lt;/td&gt;
      &lt;td&gt;420.0&lt;/td&gt;
      &lt;td&gt;87.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;gmail.com&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 394 columns&lt;/p&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 144233 entries, 0 to 144232
Data columns (total 41 columns):
TransactionID    144233 non-null int32
id_01            144233 non-null float16
id_02            140872 non-null float32
id_03            66324 non-null float16
id_04            66324 non-null float16
id_05            136865 non-null float16
id_06            136865 non-null float16
id_07            5155 non-null float16
id_08            5155 non-null float16
id_09            74926 non-null float16
id_10            74926 non-null float16
id_11            140978 non-null float16
id_12            144233 non-null object
id_13            127320 non-null float16
id_14            80044 non-null float16
id_15            140985 non-null object
id_16            129340 non-null object
id_17            139369 non-null float16
id_18            45113 non-null float16
id_19            139318 non-null float16
id_20            139261 non-null float16
id_21            5159 non-null float16
id_22            5169 non-null float16
id_23            5169 non-null object
id_24            4747 non-null float16
id_25            5132 non-null float16
id_26            5163 non-null float16
id_27            5169 non-null object
id_28            140978 non-null object
id_29            140978 non-null object
id_30            77565 non-null object
id_31            140282 non-null object
id_32            77586 non-null float16
id_33            73289 non-null object
id_34            77805 non-null object
id_35            140985 non-null object
id_36            140985 non-null object
id_37            140985 non-null object
id_38            140985 non-null object
DeviceType       140810 non-null object
DeviceInfo       118666 non-null object
dtypes: float16(22), float32(1), int32(1), object(17)
memory usage: 25.9+ MB
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;TransactionID&lt;/th&gt;
      &lt;th&gt;id_01&lt;/th&gt;
      &lt;th&gt;id_02&lt;/th&gt;
      &lt;th&gt;id_03&lt;/th&gt;
      &lt;th&gt;id_04&lt;/th&gt;
      &lt;th&gt;id_05&lt;/th&gt;
      &lt;th&gt;id_06&lt;/th&gt;
      &lt;th&gt;id_07&lt;/th&gt;
      &lt;th&gt;id_08&lt;/th&gt;
      &lt;th&gt;id_09&lt;/th&gt;
      &lt;th&gt;id_10&lt;/th&gt;
      &lt;th&gt;id_11&lt;/th&gt;
      &lt;th&gt;id_12&lt;/th&gt;
      &lt;th&gt;id_13&lt;/th&gt;
      &lt;th&gt;id_14&lt;/th&gt;
      &lt;th&gt;id_15&lt;/th&gt;
      &lt;th&gt;id_16&lt;/th&gt;
      &lt;th&gt;id_17&lt;/th&gt;
      &lt;th&gt;id_18&lt;/th&gt;
      &lt;th&gt;id_19&lt;/th&gt;
      &lt;th&gt;id_20&lt;/th&gt;
      &lt;th&gt;id_21&lt;/th&gt;
      &lt;th&gt;id_22&lt;/th&gt;
      &lt;th&gt;id_23&lt;/th&gt;
      &lt;th&gt;id_24&lt;/th&gt;
      &lt;th&gt;id_25&lt;/th&gt;
      &lt;th&gt;id_26&lt;/th&gt;
      &lt;th&gt;id_27&lt;/th&gt;
      &lt;th&gt;id_28&lt;/th&gt;
      &lt;th&gt;id_29&lt;/th&gt;
      &lt;th&gt;id_30&lt;/th&gt;
      &lt;th&gt;id_31&lt;/th&gt;
      &lt;th&gt;id_32&lt;/th&gt;
      &lt;th&gt;id_33&lt;/th&gt;
      &lt;th&gt;id_34&lt;/th&gt;
      &lt;th&gt;id_35&lt;/th&gt;
      &lt;th&gt;id_36&lt;/th&gt;
      &lt;th&gt;id_37&lt;/th&gt;
      &lt;th&gt;id_38&lt;/th&gt;
      &lt;th&gt;DeviceType&lt;/th&gt;
      &lt;th&gt;DeviceInfo&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2987004&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;70787.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;NotFound&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;-480.0&lt;/td&gt;
      &lt;td&gt;New&lt;/td&gt;
      &lt;td&gt;NotFound&lt;/td&gt;
      &lt;td&gt;166.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;542.0&lt;/td&gt;
      &lt;td&gt;144.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;New&lt;/td&gt;
      &lt;td&gt;NotFound&lt;/td&gt;
      &lt;td&gt;Android 7.0&lt;/td&gt;
      &lt;td&gt;samsung browser 6.2&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;2220x1080&lt;/td&gt;
      &lt;td&gt;match_status:2&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;F&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;SAMSUNG SM-G892A Build/NRD90M&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2987008&lt;/td&gt;
      &lt;td&gt;-5.0&lt;/td&gt;
      &lt;td&gt;98945.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;-5.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;NotFound&lt;/td&gt;
      &lt;td&gt;49.0&lt;/td&gt;
      &lt;td&gt;-300.0&lt;/td&gt;
      &lt;td&gt;New&lt;/td&gt;
      &lt;td&gt;NotFound&lt;/td&gt;
      &lt;td&gt;166.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;621.0&lt;/td&gt;
      &lt;td&gt;500.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;New&lt;/td&gt;
      &lt;td&gt;NotFound&lt;/td&gt;
      &lt;td&gt;iOS 11.1.2&lt;/td&gt;
      &lt;td&gt;mobile safari 11.0&lt;/td&gt;
      &lt;td&gt;32.0&lt;/td&gt;
      &lt;td&gt;1334x750&lt;/td&gt;
      &lt;td&gt;match_status:1&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;F&lt;/td&gt;
      &lt;td&gt;F&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;mobile&lt;/td&gt;
      &lt;td&gt;iOS Device&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2987010&lt;/td&gt;
      &lt;td&gt;-5.0&lt;/td&gt;
      &lt;td&gt;191631.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;NotFound&lt;/td&gt;
      &lt;td&gt;52.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Found&lt;/td&gt;
      &lt;td&gt;Found&lt;/td&gt;
      &lt;td&gt;121.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;410.0&lt;/td&gt;
      &lt;td&gt;142.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Found&lt;/td&gt;
      &lt;td&gt;Found&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;chrome 62.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;F&lt;/td&gt;
      &lt;td&gt;F&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;desktop&lt;/td&gt;
      &lt;td&gt;Windows&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2987011&lt;/td&gt;
      &lt;td&gt;-5.0&lt;/td&gt;
      &lt;td&gt;221832.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;-6.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;NotFound&lt;/td&gt;
      &lt;td&gt;52.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;New&lt;/td&gt;
      &lt;td&gt;NotFound&lt;/td&gt;
      &lt;td&gt;225.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;176.0&lt;/td&gt;
      &lt;td&gt;507.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;New&lt;/td&gt;
      &lt;td&gt;NotFound&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;chrome 62.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;F&lt;/td&gt;
      &lt;td&gt;F&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;desktop&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2987016&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;7460.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;0.0&lt;/td&gt;
      &lt;td&gt;100.0&lt;/td&gt;
      &lt;td&gt;NotFound&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;-300.0&lt;/td&gt;
      &lt;td&gt;Found&lt;/td&gt;
      &lt;td&gt;Found&lt;/td&gt;
      &lt;td&gt;166.0&lt;/td&gt;
      &lt;td&gt;15.0&lt;/td&gt;
      &lt;td&gt;529.0&lt;/td&gt;
      &lt;td&gt;575.0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Found&lt;/td&gt;
      &lt;td&gt;Found&lt;/td&gt;
      &lt;td&gt;Mac OS X 10_11_6&lt;/td&gt;
      &lt;td&gt;chrome 62.0&lt;/td&gt;
      &lt;td&gt;24.0&lt;/td&gt;
      &lt;td&gt;1280x800&lt;/td&gt;
      &lt;td&gt;match_status:2&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;F&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;T&lt;/td&gt;
      &lt;td&gt;desktop&lt;/td&gt;
      &lt;td&gt;MacOS&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code&gt;is_fraud = tran_train[[&#39;isFraud&#39;, &#39;TransactionID&#39;]].groupby(&#39;isFraud&#39;).count()

is_fraud[&#39;ratio&#39;] = is_fraud[&#39;TransactionID&#39;] / is_fraud[&#39;TransactionID&#39;].sum()
fig_Y = px.bar(is_fraud, x=is_fraud.index, y=&#39;TransactionID&#39;,
               text=&#39;ratio&#39;,
               labels={&#39;TransactionID&#39;: &#39;Number of transactions&#39;,
                       &#39;x&#39;: &#39;is fraud&#39;})
fig_Y.update_traces(texttemplate=&#39;%{text:.6p}&#39;)
&lt;/code&gt;&lt;/pre&gt;



&lt;html&gt;
&lt;head&gt;&lt;meta charset=&#34;utf-8&#34; /&gt;&lt;/head&gt;
&lt;body&gt;
    &lt;div&gt;
            &lt;script src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG&#34;&gt;&lt;/script&gt;&lt;script type=&#34;text/javascript&#34;&gt;if (window.MathJax) {MathJax.Hub.Config({SVG: {font: &#34;STIX-Web&#34;}});}&lt;/script&gt;
                &lt;script type=&#34;text/javascript&#34;&gt;window.PlotlyConfig = {MathJaxConfig: &#39;local&#39;};&lt;/script&gt;
        &lt;script src=&#34;https://cdn.plot.ly/plotly-latest.min.js&#34;&gt;&lt;/script&gt;    
            &lt;div id=&#34;ac80ada0-952e-4492-9d69-bdab360ef9d6&#34; class=&#34;plotly-graph-div&#34; style=&#34;height:525px; width:100%;&#34;&gt;&lt;/div&gt;
            &lt;script type=&#34;text/javascript&#34;&gt;
                
                    window.PLOTLYENV=window.PLOTLYENV || {};
                    
                if (document.getElementById(&#34;ac80ada0-952e-4492-9d69-bdab360ef9d6&#34;)) {
                    Plotly.newPlot(
                        &#39;ac80ada0-952e-4492-9d69-bdab360ef9d6&#39;,
                        [{&#34;alignmentgroup&#34;: &#34;True&#34;, &#34;hoverlabel&#34;: {&#34;namelength&#34;: 0}, &#34;hovertemplate&#34;: &#34;is fraud=%{x}&lt;br&gt;Number of transactions=%{y}&lt;br&gt;ratio=%{text}&#34;, &#34;legendgroup&#34;: &#34;&#34;, &#34;marker&#34;: {&#34;color&#34;: &#34;#636efa&#34;}, &#34;name&#34;: &#34;&#34;, &#34;offsetgroup&#34;: &#34;&#34;, &#34;orientation&#34;: &#34;v&#34;, &#34;showlegend&#34;: false, &#34;text&#34;: [0.9650099908558268, 0.03499000914417313], &#34;textposition&#34;: &#34;auto&#34;, &#34;texttemplate&#34;: &#34;%{text:.6p}&#34;, &#34;type&#34;: &#34;bar&#34;, &#34;x&#34;: [0, 1], &#34;xaxis&#34;: &#34;x&#34;, &#34;y&#34;: [569877, 20663], &#34;yaxis&#34;: &#34;y&#34;}],
                        {&#34;barmode&#34;: &#34;relative&#34;, &#34;legend&#34;: {&#34;tracegroupgap&#34;: 0}, &#34;margin&#34;: {&#34;t&#34;: 60}, &#34;template&#34;: {&#34;data&#34;: {&#34;bar&#34;: [{&#34;error_x&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;error_y&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;marker&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#E5ECF6&#34;, &#34;width&#34;: 0.5}}, &#34;type&#34;: &#34;bar&#34;}], &#34;barpolar&#34;: [{&#34;marker&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#E5ECF6&#34;, &#34;width&#34;: 0.5}}, &#34;type&#34;: &#34;barpolar&#34;}], &#34;carpet&#34;: [{&#34;aaxis&#34;: {&#34;endlinecolor&#34;: &#34;#2a3f5f&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;minorgridcolor&#34;: &#34;white&#34;, &#34;startlinecolor&#34;: &#34;#2a3f5f&#34;}, &#34;baxis&#34;: {&#34;endlinecolor&#34;: &#34;#2a3f5f&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;minorgridcolor&#34;: &#34;white&#34;, &#34;startlinecolor&#34;: &#34;#2a3f5f&#34;}, &#34;type&#34;: &#34;carpet&#34;}], &#34;choropleth&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;choropleth&#34;}], &#34;contour&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;contour&#34;}], &#34;contourcarpet&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;contourcarpet&#34;}], &#34;heatmap&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;heatmap&#34;}], &#34;heatmapgl&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;heatmapgl&#34;}], &#34;histogram&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;histogram&#34;}], &#34;histogram2d&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;histogram2d&#34;}], &#34;histogram2dcontour&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;histogram2dcontour&#34;}], &#34;mesh3d&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;type&#34;: &#34;mesh3d&#34;}], &#34;parcoords&#34;: [{&#34;line&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;parcoords&#34;}], &#34;pie&#34;: [{&#34;automargin&#34;: true, &#34;type&#34;: &#34;pie&#34;}], &#34;scatter&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatter&#34;}], &#34;scatter3d&#34;: [{&#34;line&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatter3d&#34;}], &#34;scattercarpet&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattercarpet&#34;}], &#34;scattergeo&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattergeo&#34;}], &#34;scattergl&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattergl&#34;}], &#34;scattermapbox&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scattermapbox&#34;}], &#34;scatterpolar&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterpolar&#34;}], &#34;scatterpolargl&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterpolargl&#34;}], &#34;scatterternary&#34;: [{&#34;marker&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;type&#34;: &#34;scatterternary&#34;}], &#34;surface&#34;: [{&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}, &#34;colorscale&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;type&#34;: &#34;surface&#34;}], &#34;table&#34;: [{&#34;cells&#34;: {&#34;fill&#34;: {&#34;color&#34;: &#34;#EBF0F8&#34;}, &#34;line&#34;: {&#34;color&#34;: &#34;white&#34;}}, &#34;header&#34;: {&#34;fill&#34;: {&#34;color&#34;: &#34;#C8D4E3&#34;}, &#34;line&#34;: {&#34;color&#34;: &#34;white&#34;}}, &#34;type&#34;: &#34;table&#34;}]}, &#34;layout&#34;: {&#34;annotationdefaults&#34;: {&#34;arrowcolor&#34;: &#34;#2a3f5f&#34;, &#34;arrowhead&#34;: 0, &#34;arrowwidth&#34;: 1}, &#34;coloraxis&#34;: {&#34;colorbar&#34;: {&#34;outlinewidth&#34;: 0, &#34;ticks&#34;: &#34;&#34;}}, &#34;colorscale&#34;: {&#34;diverging&#34;: [[0, &#34;#8e0152&#34;], [0.1, &#34;#c51b7d&#34;], [0.2, &#34;#de77ae&#34;], [0.3, &#34;#f1b6da&#34;], [0.4, &#34;#fde0ef&#34;], [0.5, &#34;#f7f7f7&#34;], [0.6, &#34;#e6f5d0&#34;], [0.7, &#34;#b8e186&#34;], [0.8, &#34;#7fbc41&#34;], [0.9, &#34;#4d9221&#34;], [1, &#34;#276419&#34;]], &#34;sequential&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]], &#34;sequentialminus&#34;: [[0.0, &#34;#0d0887&#34;], [0.1111111111111111, &#34;#46039f&#34;], [0.2222222222222222, &#34;#7201a8&#34;], [0.3333333333333333, &#34;#9c179e&#34;], [0.4444444444444444, &#34;#bd3786&#34;], [0.5555555555555556, &#34;#d8576b&#34;], [0.6666666666666666, &#34;#ed7953&#34;], [0.7777777777777778, &#34;#fb9f3a&#34;], [0.8888888888888888, &#34;#fdca26&#34;], [1.0, &#34;#f0f921&#34;]]}, &#34;colorway&#34;: [&#34;#636efa&#34;, &#34;#EF553B&#34;, &#34;#00cc96&#34;, &#34;#ab63fa&#34;, &#34;#FFA15A&#34;, &#34;#19d3f3&#34;, &#34;#FF6692&#34;, &#34;#B6E880&#34;, &#34;#FF97FF&#34;, &#34;#FECB52&#34;], &#34;font&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}, &#34;geo&#34;: {&#34;bgcolor&#34;: &#34;white&#34;, &#34;lakecolor&#34;: &#34;white&#34;, &#34;landcolor&#34;: &#34;#E5ECF6&#34;, &#34;showlakes&#34;: true, &#34;showland&#34;: true, &#34;subunitcolor&#34;: &#34;white&#34;}, &#34;hoverlabel&#34;: {&#34;align&#34;: &#34;left&#34;}, &#34;hovermode&#34;: &#34;closest&#34;, &#34;mapbox&#34;: {&#34;style&#34;: &#34;light&#34;}, &#34;paper_bgcolor&#34;: &#34;white&#34;, &#34;plot_bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;polar&#34;: {&#34;angularaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;radialaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}}, &#34;scene&#34;: {&#34;xaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}, &#34;yaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}, &#34;zaxis&#34;: {&#34;backgroundcolor&#34;: &#34;#E5ECF6&#34;, &#34;gridcolor&#34;: &#34;white&#34;, &#34;gridwidth&#34;: 2, &#34;linecolor&#34;: &#34;white&#34;, &#34;showbackground&#34;: true, &#34;ticks&#34;: &#34;&#34;, &#34;zerolinecolor&#34;: &#34;white&#34;}}, &#34;shapedefaults&#34;: {&#34;line&#34;: {&#34;color&#34;: &#34;#2a3f5f&#34;}}, &#34;ternary&#34;: {&#34;aaxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;baxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}, &#34;bgcolor&#34;: &#34;#E5ECF6&#34;, &#34;caxis&#34;: {&#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;}}, &#34;title&#34;: {&#34;x&#34;: 0.05}, &#34;xaxis&#34;: {&#34;automargin&#34;: true, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;, &#34;title&#34;: {&#34;standoff&#34;: 15}, &#34;zerolinecolor&#34;: &#34;white&#34;, &#34;zerolinewidth&#34;: 2}, &#34;yaxis&#34;: {&#34;automargin&#34;: true, &#34;gridcolor&#34;: &#34;white&#34;, &#34;linecolor&#34;: &#34;white&#34;, &#34;ticks&#34;: &#34;&#34;, &#34;title&#34;: {&#34;standoff&#34;: 15}, &#34;zerolinecolor&#34;: &#34;white&#34;, &#34;zerolinewidth&#34;: 2}}}, &#34;xaxis&#34;: {&#34;anchor&#34;: &#34;y&#34;, &#34;domain&#34;: [0.0, 1.0], &#34;title&#34;: {&#34;text&#34;: &#34;is fraud&#34;}}, &#34;yaxis&#34;: {&#34;anchor&#34;: &#34;x&#34;, &#34;domain&#34;: [0.0, 1.0], &#34;title&#34;: {&#34;text&#34;: &#34;Number of transactions&#34;}}},
                        {&#34;responsive&#34;: true}
                    ).then(function(){
                            
var gd = document.getElementById(&#39;ac80ada0-952e-4492-9d69-bdab360ef9d6&#39;);
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === &#39;none&#39;) {{
            console.log([gd, &#39;removed!&#39;]);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest(&#39;#notebook-container&#39;);
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest(&#39;.output&#39;);
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })
                };
                
            &lt;/script&gt;
        &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;


&lt;h2 id=&#34;very-imbalanced-target-varible&#34;&gt;Very imbalanced target varible&lt;/h2&gt;

&lt;p&gt;Positives of &lt;code&gt;isFraud&lt;/code&gt; is very low of 3.5% in the entire dataset. For this classification problem, it&amp;rsquo;s very important to have high true positive rate. That is, how good can the model identify the fraud cases from all the fraud cases. So recall is in a sense more important than precision in this problem. Macro average of recall would be a good side metric for this problem. Of cource, in reality we need to consider the belance between the cost of a few frauds and the cost of handling cases.&lt;/p&gt;

&lt;p&gt;In addition, we need to put some effort on the sampling and train-val split method, to ensure that the minority class samples have enough impact to the model while training. Class weights of the model could be set to see if there&amp;rsquo;s difference in performance.&lt;/p&gt;

&lt;h2 id=&#34;check-missing-values&#34;&gt;Check missing values&lt;/h2&gt;

&lt;p&gt;Now let&amp;rsquo;s have a look at if there&amp;rsquo;s any missing value in the dataset. We can see from the table below that there&amp;rsquo;re quite a lot of missing values.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s hard to tell how we should handle with them before we look into each variable. Sometimes a missing value stands for something. It also depends on what kind of model we are going to use. We can leave them as missing value when using a tree model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def missing_ratio_col(df):
    df_na = (df.isna().sum() / len(df)) * 100
    if isinstance(df, pd.DataFrame):
        df_na = df_na.drop(
            df_na[df_na == 0].index).sort_values(ascending=False)
        missing_data = pd.DataFrame(
            {&#39;Missing Ratio %&#39;: df_na})
    else:
        missing_data = pd.DataFrame(
            {&#39;Missing Ratio %&#39;: df_na}, index=[0])
            
    return missing_data

missing_ratio_col(tran_train)
missing_ratio_col(id_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Missing Ratio %&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;dist2&lt;/th&gt;
      &lt;td&gt;93.628374&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;D7&lt;/th&gt;
      &lt;td&gt;93.409930&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;D13&lt;/th&gt;
      &lt;td&gt;89.509263&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;D14&lt;/th&gt;
      &lt;td&gt;89.469469&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;D12&lt;/th&gt;
      &lt;td&gt;89.041047&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;V307&lt;/th&gt;
      &lt;td&gt;0.002032&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;V299&lt;/th&gt;
      &lt;td&gt;0.002032&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;V309&lt;/th&gt;
      &lt;td&gt;0.002032&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;V310&lt;/th&gt;
      &lt;td&gt;0.002032&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;V308&lt;/th&gt;
      &lt;td&gt;0.002032&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;374 rows × 1 columns&lt;/p&gt;
&lt;/div&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Missing Ratio %&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;id_24&lt;/th&gt;
      &lt;td&gt;96.708798&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_25&lt;/th&gt;
      &lt;td&gt;96.441868&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_07&lt;/th&gt;
      &lt;td&gt;96.425922&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_08&lt;/th&gt;
      &lt;td&gt;96.425922&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_21&lt;/th&gt;
      &lt;td&gt;96.423149&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_26&lt;/th&gt;
      &lt;td&gt;96.420375&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_27&lt;/th&gt;
      &lt;td&gt;96.416215&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_23&lt;/th&gt;
      &lt;td&gt;96.416215&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_22&lt;/th&gt;
      &lt;td&gt;96.416215&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_18&lt;/th&gt;
      &lt;td&gt;68.722137&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_04&lt;/th&gt;
      &lt;td&gt;54.016071&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_03&lt;/th&gt;
      &lt;td&gt;54.016071&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_33&lt;/th&gt;
      &lt;td&gt;49.187079&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_10&lt;/th&gt;
      &lt;td&gt;48.052110&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_09&lt;/th&gt;
      &lt;td&gt;48.052110&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_30&lt;/th&gt;
      &lt;td&gt;46.222432&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_32&lt;/th&gt;
      &lt;td&gt;46.207872&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_34&lt;/th&gt;
      &lt;td&gt;46.056034&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_14&lt;/th&gt;
      &lt;td&gt;44.503685&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;DeviceInfo&lt;/th&gt;
      &lt;td&gt;17.726179&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_13&lt;/th&gt;
      &lt;td&gt;11.726165&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_16&lt;/th&gt;
      &lt;td&gt;10.325654&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_05&lt;/th&gt;
      &lt;td&gt;5.108401&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_06&lt;/th&gt;
      &lt;td&gt;5.108401&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_20&lt;/th&gt;
      &lt;td&gt;3.447200&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_19&lt;/th&gt;
      &lt;td&gt;3.407681&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_17&lt;/th&gt;
      &lt;td&gt;3.372321&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_31&lt;/th&gt;
      &lt;td&gt;2.739318&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;DeviceType&lt;/th&gt;
      &lt;td&gt;2.373243&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_02&lt;/th&gt;
      &lt;td&gt;2.330257&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_11&lt;/th&gt;
      &lt;td&gt;2.256765&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_28&lt;/th&gt;
      &lt;td&gt;2.256765&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_29&lt;/th&gt;
      &lt;td&gt;2.256765&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_35&lt;/th&gt;
      &lt;td&gt;2.251912&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_36&lt;/th&gt;
      &lt;td&gt;2.251912&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_15&lt;/th&gt;
      &lt;td&gt;2.251912&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_37&lt;/th&gt;
      &lt;td&gt;2.251912&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;id_38&lt;/th&gt;
      &lt;td&gt;2.251912&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h2 id=&#34;detailed-look-at-each-variable&#34;&gt;Detailed look at each variable&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;re very good references of EDA and feature engineering on the dataset, so it&amp;rsquo;s meaningless to repeat here. Please check the list here if you&amp;rsquo;re interested:
- &lt;a href=&#34;https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575&#34; target=&#34;_blank&#34;&gt;Feature Engineering Techniques&lt;/a&gt;
- &lt;a href=&#34;https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id#EDA-for-Columns-V-and-ID&#34; target=&#34;_blank&#34;&gt;EDA for Columns V and ID&lt;/a&gt;
- &lt;a href=&#34;https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600#Feature-Engineering&#34; target=&#34;_blank&#34;&gt;XGB Fraud with Magic scores LB 0.96&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;data-transformation-pipeline&#34;&gt;Data transformation pipeline&lt;/h1&gt;

&lt;p&gt;Based on the references and my own analysis, here we have a pipeline of the transformations to perform on the dataset. It can be adjusted for experimenting. Explanation of the transformations see in code comments.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import pandas as pd
import plotly.express as px
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from typing import List, Callable


DATA_DIR = &#39;/content/drive/My Drive/colab-data/fraud detect/data&#39;


def reduce_mem_usage(df, verbose=True):
    numerics = [&#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;float16&#39;, &#39;float32&#39;, &#39;float64&#39;]
    start_mem = df.memory_usage().sum() / 1024**2    
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == &#39;int&#39;:
                if c_min &amp;gt; np.iinfo(np.int8).min and c_max &amp;lt; np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min &amp;gt; np.iinfo(np.int16).min and c_max &amp;lt; np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min &amp;gt; np.iinfo(np.int32).min and c_max &amp;lt; np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min &amp;gt; np.iinfo(np.int64).min and c_max &amp;lt; np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min &amp;gt; np.finfo(np.float16).min and c_max &amp;lt; np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min &amp;gt; np.finfo(np.float32).min and c_max &amp;lt; np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)    
    end_mem = df.memory_usage().sum() / 1024**2
    if verbose: print(&#39;Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)&#39;.format(end_mem, 100 * (start_mem - end_mem) / start_mem))
    return df

    
def load_df(test_set: bool = False, nrows: int = None, sample_ratio: float = None, reduce_mem: bool = True) -&amp;gt; pd.DataFrame:
    if test_set:
        tran = pd.read_csv(f&#39;{DATA_DIR}/test_transaction.csv&#39;, nrows=nrows)
        ids = pd.read_csv(f&#39;{DATA_DIR}/test_identity.csv&#39;, nrows=nrows)
    else:
        tran = pd.read_csv(f&#39;{DATA_DIR}/train_transaction.csv&#39;, nrows=nrows)
        ids = pd.read_csv(f&#39;{DATA_DIR}/train_identity.csv&#39;, nrows=nrows)

    if sample_ratio:
        size = int(len(tran) * sample_ratio)
        tran = tran.sample(n=size, random_state=RAND_STATE)
        ids = ids.sample(n=size, random_state=RAND_STATE)
    df = tran.merge(ids, how=&#39;left&#39;, on=&#39;TransactionID&#39;)
    if reduce_mem:
        reduce_mem_usage(df)
    return df


def cat_cols(df: pd.DataFrame) -&amp;gt; List[str]:
    cols: List[str] = []

    cols.append(&#39;ProductCD&#39;)

    cols_card = [c for c in df.columns if &#39;card&#39; in c]
    cols.extend(cols_card)

    cols_addr = [&#39;addr1&#39;, &#39;addr2&#39;]
    cols.extend(cols_addr)

    cols_emaildomain = [c for c in df if &#39;email&#39; in c]
    cols.extend(cols_emaildomain)

    cols_M = [c for c in df if c.startswith(&#39;M&#39;)]
    cols.extend(cols_M)

    cols.extend([&#39;DeviceType&#39;, &#39;DeviceInfo&#39;])

    cols_id = [c for c in df if c.startswith(&#39;id&#39;)]
    cols.extend(cols_id)

    return cols


def num_cols(df: pd.DataFrame, target_col=&#39;isFraud&#39;) -&amp;gt; List[str]:
    cols_cat = cat_cols(df)
    cats = df[cols_cat]
    cols_num = list(set(df.columns) - set(cols_cat))

    if target_col in cols_num:
        cols_num.remove(target_col)

    return cols_num


def missing_ratio_col(df):
    df_na = (df.isna().sum() / len(df)) * 100
    if isinstance(df, pd.DataFrame):
        df_na = df_na.drop(
            df_na[df_na == 0].index).sort_values(ascending=False)
        missing_data = pd.DataFrame({&#39;Missing Ratio %&#39;: df_na})
    else:
        missing_data = pd.DataFrame({&#39;Missing Ratio %&#39;: df_na}, index=[0])

    return missing_data


class NumColsNaMedianFiller(TransformerMixin, BaseEstimator):

    def fit(self, X, y=None):
        return self

    def transform(self, df):
        cols_cat = cat_cols(df)
        cols_num = list(set(df.columns) - set(cols_cat))

        for col in cols_num:
            median = df[col].median()
            df[col].fillna(median, inplace=True)

        return df


class NumColsNegFiller(TransformerMixin, BaseEstimator):

    def fit(self, X, y=None):
        return self

    def transform(self, df):
        cols_num = num_cols(df)

        for col in cols_num:
            df[col].fillna(-999, inplace=True)

        return df


class NumColsRatioDropper(TransformerMixin, BaseEstimator):
    def __init__(self, ratio: float = 0.5):
        self.ratio = ratio

    def fit(self, X, y=None):
        return self

    def transform(self, df):
        # print(X[self.attribute_names].columns)

        cols_cat = cat_cols(df)
        cats = df[cols_cat]
        # nums = df.drop(columns=cols_cat)
        # cols_num = df[~df[cols_cat]].columns
        cols_num = list(set(df.columns) - set(cols_cat))
        nums = df[cols_num]

        ratio = self.ratio * 100
        missings = missing_ratio_col(nums)
        # print(missings)
        inds = missings[missings[&#39;Missing Ratio %&#39;] &amp;gt; ratio].index
        df = df.drop(columns=inds)
        return df


class ColsDropper(TransformerMixin, BaseEstimator):
    def __init__(self, cols: List[str]):
        self.cols = cols

    def fit(self, X, y=None):
        return self

    def transform(self, df):
        return df.drop(columns=self.cols)


class DataFrameSelector(TransformerMixin, BaseEstimator):
    def __init__(self, col_names):
        self.attribute_names = col_names

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        print(X[self.attribute_names].columns)

        return X[self.attribute_names].values


class DummyEncoder(TransformerMixin, BaseEstimator):

    def fit(self, X, y=None):
        return self

    def transform(self, df):
        cols_cat = cat_cols(df)

        cats = df[cols_cat]
        noncats = df.drop(columns=cols_cat)

        cats = cats.astype(&#39;category&#39;)
        cats_enc = pd.get_dummies(cats, prefix=cols_cat, dummy_na=True)

        return noncats.join(cats_enc)


# Label encoding is OK when we&#39;re using tree models
class MyLabelEncoder(TransformerMixin, BaseEstimator):

    def fit(self, X, y=None):
        return self

    def transform(self, df):
        cols_cat = cat_cols(df)

        for col in cols_cat:
            df[col] = df[col].astype(&#39;category&#39;).cat.add_categories(
                &#39;missing&#39;).fillna(&#39;missing&#39;)
            le = preprocessing.LabelEncoder()
            # TODO add test set together to encoding
            # le.fit(df[col].astype(str).values)
            df[col] = le.fit_transform(df[col].astype(str).values)
        return df


class FrequencyEncoder(TransformerMixin, BaseEstimator):
    def __init__(self, cols):
        self.cols = cols

    def fit(self, X, y=None):
        return self

    def transform(self, df):
        for col in self.cols:
            vc = df[col].value_counts(dropna=True, normalize=True).to_dict()
            vc[-1] = -1
            nm = col + &#39;_FE&#39;
            df[nm] = df[col].map(vc)
            df[nm] = df[nm].astype(&#39;float32&#39;)
        return df


class CombineEncoder(TransformerMixin, BaseEstimator):
    def __init__(self, cols_pairs: List[List[str]]):
        self.cols_pairs = cols_pairs

    def fit(self, X, y=None):
        return self

    def transform(self, df):
        for pair in self.cols_pairs:
            col1 = pair[0]
            col2 = pair[1]
            nm = col1 + &#39;_&#39; + col2
            df[nm] = df[col1].astype(str) + &#39;_&#39; + df[col2].astype(str)
            df[nm] = df[nm].astype(&#39;category&#39;)
            # print(nm, &#39;, &#39;, end=&#39;&#39;)
        return df


class AggregateEncoder(TransformerMixin, BaseEstimator):
    def __init__(self, main_cols: List[str], uids: List[str], aggr_types: List[str],
                 fill_na: bool = True, use_na: bool = False):
        self.main_cols = main_cols
        self.uids = uids
        self.aggr_types = aggr_types
        self.use_na = use_na
        self.fill_na = fill_na

    def fit(self, X, y=None):
        return self

    def transform(self, df):
        for col in self.main_cols:
            for uid in self.uids:
                for aggr_type in self.aggr_types:
                    col_new = f&#39;{col}_{uid}_{aggr_type}&#39;
                    tmp = df.groupby([uid])[col].agg([aggr_type]).reset_index().rename(
                        columns={aggr_type: col_new})
                    tmp.index = list(tmp[uid])
                    tmp = tmp[col_new].to_dict()
                    df[col_new] = df[uid].map(tmp).astype(&#39;float32&#39;)
                    if self.fill_na:
                        df[col_new].fillna(-1, inplace=True)
        return df
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;from sklearn.pipeline import Pipeline

pipe = Pipeline(steps=[
    # Based on feature engineering from 
    # https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600#Encoding-Functions
    (&#39;combine_enc&#39;, CombineEncoder(
        [[&#39;card1&#39;, &#39;addr1&#39;], [&#39;card1_addr1&#39;, &#39;P_emaildomain&#39;]])),
    (&#39;freq_enc&#39;, FrequencyEncoder(
        [&#39;addr1&#39;, &#39;card1&#39;, &#39;card2&#39;, &#39;card3&#39;, &#39;P_emaildomain&#39;])),
    (&#39;aggr_enc&#39;, AggregateEncoder([&#39;TransactionAmt&#39;, &#39;D9&#39;, &#39;D11&#39;], [
        &#39;card1&#39;, &#39;card1_addr1&#39;, &#39;card1_addr1_P_emaildomain&#39;], [&#39;mean&#39;, &#39;std&#39;])),

    # Drop columns that have certain high ratio of missing values, and then fill
    # in values e.g. median value. May not be used if using a tree model.
    (&#39;reduce_missing&#39;, NumColsRatioDropper(0.5)),
    (&#39;fillna_median&#39;, NumColsNaMedianFiller()),

    # Drop some columns that will not be used
    (&#39;drop_cols_basic&#39;, ColsDropper([&#39;TransactionID&#39;, &#39;TransactionDT&#39;, &#39;D6&#39;, 
                                     &#39;D7&#39;, &#39;D8&#39;, &#39;D9&#39;, &#39;D12&#39;, &#39;D13&#39;, &#39;D14&#39;, &#39;C3&#39;,
                                     &#39;M5&#39;, &#39;id_08&#39;, &#39;id_33&#39;, &#39;card4&#39;, &#39;id_07&#39;, 
                                     &#39;id_14&#39;, &#39;id_21&#39;, &#39;id_30&#39;, &#39;id_32&#39;, &#39;id_34&#39;])),

    # Drop some columns based on feature importance got from a model.
    (&#39;drop_cols_feat_importance&#39;, ColsDropper(
        [&#39;v107&#39;, &#39;v117&#39;, &#39;v119&#39;, &#39;v120&#39;, &#39;v27&#39;, &#39;v28&#39;, &#39;v305&#39;])),

    (&#39;fillna_negative&#39;, NumColsNegFiller()),

    # Encode categorical variables. Depending on the kind of model we use, 
    # we can choose between label encoding and onehot encoding.
    # (&#39;onehot_enc&#39;, DummyEncoder()),
    (&#39;label_enc&#39;, MyLabelEncoder()),
])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;split-dataset&#34;&gt;Split dataset&lt;/h3&gt;

&lt;p&gt;And as we want to predict future payment fraud based on the past data, so we should not shuffle the dataset when split training and testing sets, but just time-based split.&lt;/p&gt;

&lt;p&gt;As this is a imbalanced dataset with 1 class of the target variable have only about 3.5%, so we may want to try sampling methods like over-sampling or SMOTE sampling on the training dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;RAND_STATE = 20200119

def data_split_v1(X: pd.DataFrame, y: pd.Series):
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.25, shuffle=False, random_state=RAND_STATE)

    return X_train, X_val, y_train, y_val


def data_split_oversample_v1(X: pd.DataFrame, y: pd.Series):
    from imblearn.over_sampling import RandomOverSampler

    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.25, shuffle=False, random_state=RAND_STATE)

    ros = RandomOverSampler(random_state=RAND_STATE)
    X_train, y_train = ros.fit_resample(X_train, y_train)

    return X_train, X_val, y_train, y_val


def data_split_smoteenn_v1(X: pd.DataFrame, y: pd.Series):
    from imblearn.combine import SMOTEENN

    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, shuffle=False, random_state=RAND_STATE)

    ros = SMOTEENN(random_state=RAND_STATE)
    X_train, y_train = ros.fit_resample(X_train, y_train)

    return X_train, X_val, y_train, y_val
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;experiments&#34;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;Now let&amp;rsquo;s start play with experimenting with simple models like Logistic Regression, or complex models like Gradient Boosting.&lt;/p&gt;

&lt;p&gt;Here below is a scaffold for performing experiments.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
from datetime import datetime
import json
import pprint

from sklearn import metrics
from sklearn.pipeline import Pipeline
from typing import List, Callable
        
EXP_DIR = &#39;exp&#39;

class Experiment:
    def __init__(self, df_nrows: int = None, transform_pipe: Pipeline = None,
                 data_split: Callable = None, model=None, model_class=None,
                 model_param: dict = None):
        self.df_nrows = df_nrows
        self.pipe = transform_pipe

        if data_split is None:
            self.data_split = data_split_v1
        else:
            self.data_split = data_split

        if model_class:
            self.model = model_class(**model_param)
        else:
            self.model = model

        self.model_param = model_param

    def transform(self, X):
        return self.pipe.fit_transform(X)

    def run(self, df_train: pd.DataFrame, save_exp: bool = True) -&amp;gt; float:
        # self.df = load_df(nrows=self.df_nrows)

        y = df_train[&#39;isFraud&#39;]
        X = df_train.drop(columns=[&#39;isFraud&#39;])

        X = self.transform(X)

        X_train, X_val, Y_train, Y_val = self.data_split(X, y)

        # del X
        # gc.collect()

        self.model.fit(X_train, Y_train)

        Y_pred = self.model.predict(X_val)
        self.last_roc_auc = metrics.roc_auc_score(Y_val, Y_pred)

        if save_exp:
            self.save_result()

        return self.last_roc_auc
    
    def save_result(self, feature_importance:bool=False):
        save_time = datetime.now().strftime(&#39;%Y-%m-%d_%H-%M-%S&#39;)
        result = {}
        result[&#39;roc_auc&#39;] = self.last_roc_auc
        result[&#39;transform&#39;] = list(self.pipe.named_steps.keys())
        result[&#39;model&#39;] = self.model.__class__.__name__
        result[&#39;model_param&#39;] = self.model_param
        result[&#39;data_split&#39;] = self.data_split.__name__
        result[&#39;num_sample_rows&#39;] = self.df_nrows
        result[&#39;save_time&#39;] = save_time
        if feature_importance:
            if hasattr(self.model, &#39;feature_importances_&#39;):
                result[&#39;feature_importances_&#39;] = dict(
                    zip(self.X.columns, self.model.feature_importances_.tolist()))
            if hasattr(self.model, &#39;feature_importance&#39;):
                result[&#39;feature_importances_&#39;] = dict(
                    zip(self.df.columns, self.model.feature_importance.tolist()))

        pprint.pprint(result, indent=4)

        if not os.path.exists(EXP_DIR):
            os.makedirs(EXP_DIR)
        with open(f&#39;{EXP_DIR}/exp_{save_time}_{self.last_roc_auc:.4f}.json&#39;, &#39;w&#39;) as f:
            json.dump(result, f, indent=4)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;import gc


del tran_train, id_train
gc.collect()

df_train = load_df()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;df_train = load_df()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Mem. usage decreased to 650.48 Mb (66.8% reduction)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;logistic-regression-as-baseline&#34;&gt;Logistic Regression as baseline&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;def exp1():
    from sklearn.linear_model import LogisticRegression

    pipe = Pipeline(steps=[
        (&#39;combine_enc&#39;, CombineEncoder(
            [[&#39;card1&#39;, &#39;addr1&#39;], [&#39;card1_addr1&#39;, &#39;P_emaildomain&#39;]])),
        (&#39;freq_enc&#39;, FrequencyEncoder(
            [&#39;addr1&#39;, &#39;card1&#39;, &#39;card2&#39;, &#39;card3&#39;, &#39;P_emaildomain&#39;])),
        (&#39;aggr_enc&#39;, AggregateEncoder([&#39;TransactionAmt&#39;, &#39;D9&#39;, &#39;D11&#39;], [
         &#39;card1&#39;, &#39;card1_addr1&#39;, &#39;card1_addr1_P_emaildomain&#39;], [&#39;mean&#39;, &#39;std&#39;])),

        (&#39;reduce_missing&#39;, NumColsRatioDropper(0.3)),
        (&#39;fillna_median&#39;, NumColsNaMedianFiller()),

        (&#39;drop_cols_basic&#39;, ColsDropper([&#39;TransactionID&#39;, &#39;TransactionDT&#39;, &#39;C3&#39;, &#39;M5&#39;, &#39;id_08&#39;, &#39;id_33&#39;, &#39;card4&#39;, &#39;id_07&#39;, &#39;id_14&#39;, &#39;id_21&#39;, &#39;id_30&#39;, &#39;id_32&#39;, &#39;id_34&#39;])),

        # Though onehot encoding is more appropriate for logistic regression, we
        # don&#39;t have enough memory to encode that many variables. So we take a 
        # step back using label encoding.
        # (&#39;onehot_enc&#39;, DummyEncoder()),
        (&#39;label_enc&#39;, MyLabelEncoder()),
    ])

    exp = Experiment(transform_pipe=pipe,
                      data_split=data_split_v1,
                      model_class=LogisticRegression,
                      # just use the default hyper paramenters
                      model_param={},
                     )
    exp.run(df_train=df_train)

exp1()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning:

lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression



{   &#39;data_split&#39;: &#39;data_split_v1&#39;,
    &#39;model&#39;: &#39;LogisticRegression&#39;,
    &#39;model_param&#39;: {},
    &#39;num_sample_rows&#39;: None,
    &#39;roc_auc&#39;: 0.4956463187232307,
    &#39;save_time&#39;: &#39;2020-03-26_20-27-08&#39;,
    &#39;transform&#39;: [   &#39;combine_enc&#39;,
                     &#39;freq_enc&#39;,
                     &#39;aggr_enc&#39;,
                     &#39;reduce_missing&#39;,
                     &#39;fillna_median&#39;,
                     &#39;drop_cols_basic&#39;,
                     &#39;label_enc&#39;]}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;gradient-boosting-with-lightgbm&#34;&gt;Gradient Boosting with LightGBM&lt;/h2&gt;

&lt;p&gt;Now let&amp;rsquo;s try a Gradient Boosting tree model using the LightGBM implementation, and tune a little on the hyper-parameters to make it a more complex model.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import lightgbm as lgb


class LgbmWrapper:
    def __init__(self, **param):
        self.param = param
        self.trained = None

    def fit(self, X_train, y_train):
        train = lgb.Dataset(X_train, label=y_train)
        self.trained = lgb.train(self.param, train)
        self.feature_importances_ = self.trained.feature_importance()
        return self.trained

    def predict(self, X_val):
        return self.trained.predict(X_val, num_iteration=self.trained.best_iteration)


def exp2():
    pipe = Pipeline(steps=[
        # Based on feature engineering from 
        # https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600#Encoding-Functions
        (&#39;combine_enc&#39;, CombineEncoder(
            [[&#39;card1&#39;, &#39;addr1&#39;], [&#39;card1_addr1&#39;, &#39;P_emaildomain&#39;]])),
        (&#39;freq_enc&#39;, FrequencyEncoder(
            [&#39;addr1&#39;, &#39;card1&#39;, &#39;card2&#39;, &#39;card3&#39;, &#39;P_emaildomain&#39;])),
        (&#39;aggr_enc&#39;, AggregateEncoder([&#39;TransactionAmt&#39;, &#39;D9&#39;, &#39;D11&#39;], [
            &#39;card1&#39;, &#39;card1_addr1&#39;, &#39;card1_addr1_P_emaildomain&#39;], [&#39;mean&#39;, &#39;std&#39;])),
    
        # Drop some columns that will not be used
        (&#39;drop_cols_basic&#39;, ColsDropper([&#39;TransactionID&#39;, &#39;TransactionDT&#39;, &#39;D6&#39;, 
                                        &#39;D7&#39;, &#39;D8&#39;, &#39;D9&#39;, &#39;D12&#39;, &#39;D13&#39;, &#39;D14&#39;, &#39;C3&#39;,
                                        &#39;M5&#39;, &#39;id_08&#39;, &#39;id_33&#39;, &#39;card4&#39;, &#39;id_07&#39;, 
                                        &#39;id_14&#39;, &#39;id_21&#39;, &#39;id_30&#39;, &#39;id_32&#39;, &#39;id_34&#39;])),
    
        # Drop some columns based on feature importance got from a model.
        # (&#39;drop_cols_feat_importance&#39;, ColsDropper(
        #     [&#39;v107&#39;, &#39;v117&#39;, &#39;v119&#39;, &#39;v120&#39;, &#39;v27&#39;, &#39;v28&#39;, &#39;v305&#39;])),
    
        (&#39;fillna_negative&#39;, NumColsNegFiller()),
    
        # Label encoding used for tree models.
        # (&#39;onehot_enc&#39;, DummyEncoder()),
        (&#39;label_enc&#39;, MyLabelEncoder()),
    ])

    param_lgbm = {&#39;objective&#39;: &#39;binary&#39;,
                  &#39;boosting_type&#39;: &#39;gbdt&#39;,
                  &#39;metric&#39;: &#39;auc&#39;,
                  &#39;learning_rate&#39;: 0.01,
                  &#39;num_leaves&#39;: 2**8,
                  &#39;max_depth&#39;: -1,
                  &#39;tree_learner&#39;: &#39;serial&#39;,
                  &#39;colsample_bytree&#39;: 0.7,
                  &#39;subsample_freq&#39;: 1,
                  &#39;subsample&#39;: 0.7,
                  &#39;n_estimators&#39;: 10000,
                  #  &#39;n_estimators&#39;: 80000,
                  &#39;max_bin&#39;: 255,
                  &#39;n_jobs&#39;: -1,
                  &#39;verbose&#39;: -1,
                  &#39;seed&#39;: RAND_STATE,
                  # &#39;early_stopping_rounds&#39;: 100,
                  }


    exp = Experiment(transform_pipe=pipe,
                    data_split=data_split_v1,
                     model_class=LgbmWrapper,
                     model_param=param_lgbm,
                     )
    exp.run(df_train=df_train)


exp2()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py:118: UserWarning:

Found `n_estimators` in params. Will use it instead of argument



{   &#39;data_split&#39;: &#39;data_split_v1&#39;,
    &#39;model&#39;: &#39;LgbmWrapper&#39;,
    &#39;model_param&#39;: {   &#39;boosting_type&#39;: &#39;gbdt&#39;,
                       &#39;colsample_bytree&#39;: 0.7,
                       &#39;learning_rate&#39;: 0.01,
                       &#39;max_bin&#39;: 255,
                       &#39;max_depth&#39;: -1,
                       &#39;metric&#39;: &#39;auc&#39;,
                       &#39;n_estimators&#39;: 10000,
                       &#39;n_jobs&#39;: -1,
                       &#39;num_leaves&#39;: 256,
                       &#39;objective&#39;: &#39;binary&#39;,
                       &#39;seed&#39;: 20200119,
                       &#39;subsample&#39;: 0.7,
                       &#39;subsample_freq&#39;: 1,
                       &#39;tree_learner&#39;: &#39;serial&#39;,
                       &#39;verbose&#39;: -1},
    &#39;num_sample_rows&#39;: None,
    &#39;roc_auc&#39;: 0.919589853747652,
    &#39;save_time&#39;: &#39;2020-03-27_09-55-43&#39;,
    &#39;transform&#39;: [   &#39;combine_enc&#39;,
                     &#39;freq_enc&#39;,
                     &#39;aggr_enc&#39;,
                     &#39;drop_cols_basic&#39;,
                     &#39;fillna_negative&#39;,
                     &#39;label_enc&#39;]}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So we got local validation ROC AUC of about 0.9196, this is a looks OK score.&lt;/p&gt;

&lt;p&gt;This model&amp;rsquo;s prediction on the test dataset got 0.9398 on publica leader board, and 0.9058 on private leader board. These scores have a somehow big gap to the top scores, but still good enough as there&amp;rsquo;re potentially many ways for improvement. For example, more different ways of transformations and engineering could be performed on the features, try model implementation like CatBoost and XGB, and search for better hyper-parameters. But it assumes you have plenty of computation resource and time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Skin Lesion Classifier</title>
      <link>https://pcx.linkedinfo.co/project/skin-lesion-classifier/</link>
      <pubDate>Fri, 07 Feb 2020 19:51:26 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/project/skin-lesion-classifier/</guid>
      <description>&lt;p&gt;A skin lesion classifier that uses a deep neural network trained on the HAM10000 dataset. An implementation of the ISIC challenge 2018 task 3.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Topic Tag Predictor</title>
      <link>https://pcx.linkedinfo.co/project/topic-tag-predictor/</link>
      <pubDate>Sat, 04 Jan 2020 19:22:26 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/project/topic-tag-predictor/</guid>
      <description>&lt;p&gt;A topic tag prediction service for technical articles. The model uses a pre-trained BERT and fine-tuned on the dataset of LinkedInfo.co.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Skin Lesion Image Classification with Deep Convolutional Neural Networks</title>
      <link>https://pcx.linkedinfo.co/post/skin-lesion-cls/</link>
      <pubDate>Mon, 02 Dec 2019 21:26:33 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/post/skin-lesion-cls/</guid>
      <description>

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In this post we will show how to do skin lesion image classification with deep neural networks. It is an image classifier trained on the HAM10000 dataset, the same problem in the International Skin Imaging Collaboration (ISIC) 2018 challenge task3.&lt;/p&gt;

&lt;p&gt;The solution in this post is mainly based on some web posts and methods from the &lt;a href=&#34;https://challenge2018.isic-archive.com/leaderboards/&#34; target=&#34;_blank&#34;&gt;ISIC2018 leadboard&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The classification neural network model is tested with pretrained ResNet and DenseNet and implemented with PyTOrch. The model with the highest mean of recalls (0.9369 on 20% test set) is a ensemble of ImageNet pretrained and fine-tuned DenseNet161 + ResNet152.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Confusion matrix of the mdoel with the best recall
from IPython.display import Image

Image(&#39;test_results/ensemble_dense161_6_res152_4_lesion/confusion_matrix.png&#39;, width=900)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_1_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here below we go through the process how I worked on this problem.&lt;/p&gt;

&lt;h1 id=&#34;a-look-at-the-data&#34;&gt;A look at the data&lt;/h1&gt;

&lt;p&gt;Before diving into the models and metrics, we need to firstly have a look at the dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd

df = pd.read_csv(&#39;data/HAM10000_metadata.csv&#39;, index_col=&#39;image_id&#39;)
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;lesion_id&lt;/th&gt;
      &lt;th&gt;dx&lt;/th&gt;
      &lt;th&gt;dx_type&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;sex&lt;/th&gt;
      &lt;th&gt;localization&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;image_id&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;ISIC_0027419&lt;/th&gt;
      &lt;td&gt;HAM_0000118&lt;/td&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;80.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;scalp&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ISIC_0025030&lt;/th&gt;
      &lt;td&gt;HAM_0000118&lt;/td&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;80.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;scalp&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ISIC_0026769&lt;/th&gt;
      &lt;td&gt;HAM_0002730&lt;/td&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;80.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;scalp&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ISIC_0025661&lt;/th&gt;
      &lt;td&gt;HAM_0002730&lt;/td&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;80.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;scalp&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ISIC_0031633&lt;/th&gt;
      &lt;td&gt;HAM_0001466&lt;/td&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;75.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;ear&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import seaborn as sns

sns.countplot(df[&#39;dx&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x126f39eb8&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_4_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pd.DataFrame({&#39;counts&#39;:df[&#39;dx&#39;].value_counts(), &#39;percent&#39;: df[&#39;dx&#39;].value_counts() / len(df)})
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;counts&lt;/th&gt;
      &lt;th&gt;percent&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;nv&lt;/th&gt;
      &lt;td&gt;6705&lt;/td&gt;
      &lt;td&gt;0.669496&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mel&lt;/th&gt;
      &lt;td&gt;1113&lt;/td&gt;
      &lt;td&gt;0.111133&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;bkl&lt;/th&gt;
      &lt;td&gt;1099&lt;/td&gt;
      &lt;td&gt;0.109735&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;bcc&lt;/th&gt;
      &lt;td&gt;514&lt;/td&gt;
      &lt;td&gt;0.051323&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;akiec&lt;/th&gt;
      &lt;td&gt;327&lt;/td&gt;
      &lt;td&gt;0.032651&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;vasc&lt;/th&gt;
      &lt;td&gt;142&lt;/td&gt;
      &lt;td&gt;0.014179&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;df&lt;/th&gt;
      &lt;td&gt;115&lt;/td&gt;
      &lt;td&gt;0.011483&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;We can see that the samples for each class are very imbalanced.
The class &lt;em&gt;melanocytic nevi (nv)&lt;/em&gt; has about 67% of the dataset. The most minority class has only about 1% of the dataset.&lt;/p&gt;

&lt;p&gt;When we organize the rows by lesion_id, we can see that many lesions have more than 1 images. The description of ham10000 says the dataset includes lesions with multiple images, which can be tracked by the lesion_id column.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dfr=df.reset_index(col_level=&#39;lesion_id&#39;).set_index([&#39;lesion_id&#39;,&#39;image_id&#39;])
dfr.head(10)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;dx&lt;/th&gt;
      &lt;th&gt;dx_type&lt;/th&gt;
      &lt;th&gt;age&lt;/th&gt;
      &lt;th&gt;sex&lt;/th&gt;
      &lt;th&gt;localization&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;lesion_id&lt;/th&gt;
      &lt;th&gt;image_id&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th rowspan=&#34;2&#34; valign=&#34;top&#34;&gt;HAM_0000118&lt;/th&gt;
      &lt;th&gt;ISIC_0027419&lt;/th&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;80.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;scalp&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ISIC_0025030&lt;/th&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;80.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;scalp&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan=&#34;2&#34; valign=&#34;top&#34;&gt;HAM_0002730&lt;/th&gt;
      &lt;th&gt;ISIC_0026769&lt;/th&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;80.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;scalp&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ISIC_0025661&lt;/th&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;80.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;scalp&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan=&#34;2&#34; valign=&#34;top&#34;&gt;HAM_0001466&lt;/th&gt;
      &lt;th&gt;ISIC_0031633&lt;/th&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;75.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;ear&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ISIC_0027850&lt;/th&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;75.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;ear&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan=&#34;2&#34; valign=&#34;top&#34;&gt;HAM_0002761&lt;/th&gt;
      &lt;th&gt;ISIC_0029176&lt;/th&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;60.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;face&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ISIC_0029068&lt;/th&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;60.0&lt;/td&gt;
      &lt;td&gt;male&lt;/td&gt;
      &lt;td&gt;face&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th rowspan=&#34;2&#34; valign=&#34;top&#34;&gt;HAM_0005132&lt;/th&gt;
      &lt;th&gt;ISIC_0025837&lt;/th&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;70.0&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;back&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ISIC_0025209&lt;/th&gt;
      &lt;td&gt;bkl&lt;/td&gt;
      &lt;td&gt;histo&lt;/td&gt;
      &lt;td&gt;70.0&lt;/td&gt;
      &lt;td&gt;female&lt;/td&gt;
      &lt;td&gt;back&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from matplotlib import rcParams

%matplotlib inline

# figure size in inches optional
rcParams[&#39;figure.figsize&#39;] = 10 ,5

def plot_by_lesion():
    grouped = df.groupby([&#39;lesion_id&#39;])
    lesions = []
    for i, lesion in enumerate(grouped):
        cnt = len(lesion[1].index)
        if cnt &amp;gt; 1:
            fig, axes = plt.subplots(1, cnt)
            for ax, name in zip(axes, lesion[1].index):
                img = mpimg.imread(f&#39;data/{name}.jpg&#39;)
                ax.imshow(img)
        if i &amp;gt; 4:
            break

plot_by_lesion()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_8_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;output_8_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;output_8_2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can seee that the multiple images capture the same lesion with differences in color, scaling, orientation.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s count the images by lesion_id.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;lesion_id&#39;].nunique()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;7470
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cnt = df.groupby(&#39;dx&#39;)[&#39;lesion_id&#39;].nunique()
per = df.groupby(&#39;dx&#39;)[&#39;lesion_id&#39;].nunique().div(df[&#39;lesion_id&#39;].nunique())
pd.DataFrame({&#39;counts&#39;:cnt, &#39;percent&#39;: per})
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;counts&lt;/th&gt;
      &lt;th&gt;percent&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;dx&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;akiec&lt;/th&gt;
      &lt;td&gt;228&lt;/td&gt;
      &lt;td&gt;0.030522&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;bcc&lt;/th&gt;
      &lt;td&gt;327&lt;/td&gt;
      &lt;td&gt;0.043775&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;bkl&lt;/th&gt;
      &lt;td&gt;727&lt;/td&gt;
      &lt;td&gt;0.097323&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;df&lt;/th&gt;
      &lt;td&gt;73&lt;/td&gt;
      &lt;td&gt;0.009772&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;mel&lt;/th&gt;
      &lt;td&gt;614&lt;/td&gt;
      &lt;td&gt;0.082195&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;nv&lt;/th&gt;
      &lt;td&gt;5403&lt;/td&gt;
      &lt;td&gt;0.723293&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;vasc&lt;/th&gt;
      &lt;td&gt;98&lt;/td&gt;
      &lt;td&gt;0.013119&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now we see there&amp;rsquo;re 7470 unique lesions there, and the class imbalance got evern worse when we group them by lesion_id. We need to keep this in mind for the later actions like sampling and choosing evaluation metrics.&lt;/p&gt;

&lt;h1 id=&#34;metrics&#34;&gt;Metrics&lt;/h1&gt;

&lt;p&gt;For classfication problem, the commonly used metrics are &lt;em&gt;Precision/Recall/F-measures&lt;/em&gt;, &lt;em&gt;ROC_AUC&lt;/em&gt;, &lt;em&gt;Accuracy Score (ACC)&lt;/em&gt; and so on. But for this imbalanced dataset, we need to think more on the choice of metrics.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&#34;https://arxiv.org/abs/1902.03368&#34; target=&#34;_blank&#34;&gt;ISIC 2018 challenge report&lt;/a&gt;, it mentioned that
&amp;gt; Use of &lt;em&gt;balanced accuracy&lt;/em&gt; is critical to select the best unbiased classifier, rather than one that overfits to arbitrary dataset prevalence, as is the case with accuracy.&lt;/p&gt;

&lt;p&gt;Based on the description, it&amp;rsquo;s the class-wise mean of recall. The &lt;code&gt;recall_score(average=&#39;macro&#39;)&lt;/code&gt; in scikit-learn just calculates this score:&lt;/p&gt;

&lt;p&gt;$$ \frac{1}{|L|} \sum_{l \in L} R \left( y_{l}, \hat{y}_{l} \right) $$&lt;/p&gt;

&lt;p&gt;The more details of the Balanced Multiclass Accuracy can refer to&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;description from the tooltip on the &lt;a href=&#34;https://challenge2018.isic-archive.com/leaderboards/&#34; target=&#34;_blank&#34;&gt;ISIC 2018 leaderboard webpage&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://forum.isic-archive.com/t/metric-for-the-task-3-lesion-diagnosis/356/9&#34; target=&#34;_blank&#34;&gt;an explanation on the ISIC discussion forum&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;description on &lt;a href=&#34;https://challenge2019.isic-archive.com/evaluation.html&#34; target=&#34;_blank&#34;&gt;ISIC 2019 introduction&lt;/a&gt;.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So we&amp;rsquo;ll use the &lt;em&gt;balanced accuracy (BACC)&lt;/em&gt; or &lt;em&gt;mean of recalls of the 7 classes&lt;/em&gt; as the main metric for this assignment.&lt;/p&gt;

&lt;p&gt;The mean reason is that this is a very imbalanced dataset, it is a big problem we need to handel carefully. For this multiclass classification with very imbalanced dataset:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s important for the model to have good performance on all the classes, other than a few majority classes. The different classes have equal importance.&lt;/li&gt;
&lt;li&gt;Mean recall is good because it counts the model&amp;rsquo;s classification performance on all the classes equally, no matter how many samples belong to a class.&lt;/li&gt;
&lt;li&gt;So global accuracy score, micro average of recalls or so are not good metrics to measure the performance in this case.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And this is a medical diagnosis, it&amp;rsquo;s important to have a high true positive rate (to minimize the false negatives), so it&amp;rsquo;s better to focus more on recall over precision.&lt;/p&gt;

&lt;p&gt;But we&amp;rsquo;ll also use other metrics togher to have more insights. A confusion matrix plot is also a good way to present how does the model performed for each class. One of the metrics that is also good for a imbalanced classification is Matthews correlation coefficient (MCC), it ranges between &lt;em&gt;−1 to 1&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1 score shows a perfect prediction&lt;/li&gt;
&lt;li&gt;0 equals to the random prediction&lt;/li&gt;
&lt;li&gt;−1 indicates total disagreement between predicted scores and true labels’ values
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$m c c=\frac{t p \cdot t n-f p \cdot f n}{\sqrt{(t p+f p) \cdot(t p+f n) \cdot(t n+f p) \cdot(t n+f n)}}
$$&lt;/p&gt;

&lt;h1 id=&#34;preprocess-dataset&#34;&gt;Preprocess dataset&lt;/h1&gt;

&lt;h2 id=&#34;sampling&#34;&gt;Sampling&lt;/h2&gt;

&lt;p&gt;Since the dataset is very imbalanced, so even though we could use the mean recall and loss function with class weights, it would be still troublesome to train the model for the under-represented minority classes. And the under-represented classes are likely to be missing or very few samples in a subsample or split, especially when the fraction is small. So we need to do something for the train-validation-test set sampling and split.&lt;/p&gt;

&lt;p&gt;2 methods were applied to deal with the problem, with the assumption that new data follow a close imbalanced distribution as the labelled dataset.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Subsampling based on the classes distribution of all the samples. So a small fraction train, validation or set will still have the same distribution of different classes.&lt;/li&gt;
&lt;li&gt;Oversampling training set for the under-represented classess (with random transformations) to equalize the distribution. Since the dataset is considered small so we will use oversampling on the minority classes other than undersampling on the majority classes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For simplicity, I&amp;rsquo;ll just use the first image of each lesion_id. The code snippet below processes the dataset with oversampling. The parameter &lt;code&gt;over_rate&lt;/code&gt; controls how much to over sample the minority classes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import functools

exclude_set = [] 
weighted = True
imbalance_eq = True
remove_dup_img = True
over_rate = 4
train_fraction = 0.8
val_fraction = 0.2

meta_data = pd.read_csv(&#39;data/HAM10000_metadata.csv&#39;, index_col=&#39;image_id&#39;)

# for reproducibility, just keep 1st image of each lesion_id
if remove_dup_img:
    lesion_ids = []
    for index, row in meta_data.iterrows():
        if row[&#39;lesion_id&#39;] not in lesion_ids:
            lesion_ids.append(row[&#39;lesion_id&#39;])
        else:
            meta_data = meta_data.drop(index=index)

if len(exclude_set) &amp;gt; 0:
    meta_data = meta_data.drop(index=exclude_set)

image_ids = meta_data.index.tolist()
num_images = len(image_ids)
num_train_ids = int(num_images * train_fraction)
num_val_ids = int(num_images * val_fraction)

# sampling based on the distribution of classees
if weighted:
    size_total = num_train_ids + num_val_ids
    df_c = meta_data[&#39;dx&#39;].astype(&#39;category&#39;).value_counts()
    weights = df_c / len(meta_data)

    def sampling(df, replace=False, total=size_total):
        return df.sample(n=int(weights[df.name] * total), replace=replace)

    train_val = meta_data.groupby(&#39;dx&#39;, as_index=False).apply(
        sampling).reset_index(0, drop=True)

    train_sampling = functools.partial(sampling, total=num_train_ids)

    train = train_val.groupby(&#39;dx&#39;, as_index=False).apply(
        train_sampling).reset_index(0, drop=True)
    val = train_val.drop(index=train.index)

    if imbalance_eq:
        bal_rate = 1 / weights / over_rate 
        for k, v in bal_rate.to_dict().items():
            if v &amp;gt; 2:
                train = train.append(
                    [train.loc[train[&#39;dx&#39;] == k, :]] * int(v),
                    ignore_index=False)


sns.countplot(train[&#39;dx&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x12738aa58&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_15_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Fractions of the entire set is splitted as below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;training 0.64 (0.8*0.8)&lt;/li&gt;
&lt;li&gt;validation 0.16 (0.8*0.2)&lt;/li&gt;
&lt;li&gt;testing 0.2 (the same set for all the experiments)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;loss-function&#34;&gt;Loss function&lt;/h2&gt;

&lt;p&gt;Cross Entropy Loss function will be used. As for this multiclass classification problem, I don&amp;rsquo;t have a good reason to use other loss functions over cross entropy.&lt;/p&gt;

&lt;p&gt;On whether or not the loss criterion should also be weighted according to the imbalanced classes, I think it needs to be based on how we sample the training and validation set.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If we sample the subsets as the sample distribution as the entire dataset, then we could use the weighted loss criterion so that it gives penalty to the majority classes.&lt;/li&gt;
&lt;li&gt;If we are to perform some sampling method like oversampling, it already gives some penalty to the majority classes, then I think we should use a loss criterion without weighted.
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;data-transformation&#34;&gt;Data transformation&lt;/h2&gt;

&lt;p&gt;Some data transformations were performed to all the input images. It is performed according to the description in &lt;a href=&#34;https://pytorch.org/docs/stable/torchvision/models.html&#34; target=&#34;_blank&#34;&gt;torchvision documentation for pre-trained models&lt;/a&gt; (as we will use these pre-trained models). It says Height and Width are expected to be at least 224, so we will resize all the input images into 224x224 to save some computation. We also normalize the iamges by the same mean and std as mentioned in the documentation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transformation for validation and testing sets
transforms.Compose([
     transforms.Resize(224),
     transforms.CenterCrop(224),
     transforms.ToTensor(),
     transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                          std=[0.229, 0.224, 0.225])
])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we oversampled minority classes in the training set, we should perform some random transformations, such as random horizontal-vertical flip, rotation and color jitter (saturation not used since I thought it might affect the preciseness of lesion area).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Transformation for training set
transforms_train = transforms.Compose([
     transforms.Resize(300),
     transforms.RandomHorizontalFlip(0.5),
     transforms.RandomVerticalFlip(0.5),
     transforms.RandomRotation(20),
     transforms.RandomResizedCrop(224),
     transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),
     transforms.ToTensor(),
     transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                          std=[0.229, 0.224, 0.225])
])
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;models&#34;&gt;Models&lt;/h1&gt;

&lt;p&gt;When choosing a neural netowrk model for a certain task, we need to consider about several factors, e.g., the performance (accuracy) of a model, the appropriation of a model architecture for the task (in terms of pretrained model, the pretrained dataset should be more similar to the new task&amp;rsquo;s dataset), and computation efficiency.&lt;/p&gt;

&lt;p&gt;For this assignment, we need to make a tradeoff between the performance and computation efficiency since I don&amp;rsquo;t have much of the computation resources.&lt;/p&gt;

&lt;h2 id=&#34;pre-trained-models&#34;&gt;Pre-trained models&lt;/h2&gt;

&lt;p&gt;By glancing through the methods in the &lt;a href=&#34;https://challenge2018.isic-archive.com/leaderboards/&#34; target=&#34;_blank&#34;&gt;ISIC2018 leadboard&lt;/a&gt;, most of the top methods used ensemble models of pre-trained models such as ResNet, Densenet, Inception and so on. There&amp;rsquo;re also methods used a single pre-trained model achieved a high rank.&lt;/p&gt;

&lt;p&gt;And also as quoted from &lt;a href=&#34;https://cs231n.github.io/transfer-learning/&#34; target=&#34;_blank&#34;&gt;cs231n notes&lt;/a&gt; states that it&amp;rsquo;s now rarely people will train a network from scratch due to insufficient data size and expensiveness of training. It&amp;rsquo;s common to start from a model pre-trained on a very large dataset and use it as an initialization or a fixed feature extractor for a new task.&lt;/p&gt;

&lt;p&gt;Therefore, we&amp;rsquo;ll start from a pre-trained model. As suggested in &lt;a href=&#34;https://towardsdatascience.com/deep-learning-for-diagnosis-of-skin-images-with-fastai-792160ab5495&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;, ResNet-34 would be a good choice to start. So the initial plan was use the pre-trained ResNet model as a fixed feature extractor to see how it performs, and then try to &amp;ldquo;fine-tune&amp;rdquo; the weights of some layers.&lt;/p&gt;

&lt;p&gt;However, after a few preliminary short-run experiments, I found it&amp;rsquo;s slow to train and the metrics didn&amp;rsquo;t show a potential improvement.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;https://keras.io/applications/&#34; target=&#34;_blank&#34;&gt;Keres documentation&lt;/a&gt; there&amp;rsquo;s a table lists some stats like accuracy and number of parameters for some widely used models. As a tradeoff between accuracy and trainability (number of parameters), I started to focus more on the &lt;em&gt;DenseNet161&lt;/em&gt; and &lt;em&gt;ResNet152&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;After some preliminary experiments on using the networks as a feature extractor, which didn&amp;rsquo;t give a encouraging result, I decided to fine-tune the whole network.&lt;/p&gt;

&lt;h1 id=&#34;experiments&#34;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;The experiments were early stopped when I think it might stop improving. Though it will possibly improve as the training continues, the time is precious.&lt;/p&gt;

&lt;h2 id=&#34;training-and-validation&#34;&gt;Training and validation&lt;/h2&gt;

&lt;h3 id=&#34;densenet161&#34;&gt;DenseNet161&lt;/h3&gt;

&lt;p&gt;For the DenseNet161, the best validation mean of recalls is about 0.6845.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch

best_dense161_lesion = torch.load(
    &#39;experiments/dense161_eq3_exclutest_lesion_v1/model_best.pth.tar&#39;, 
    map_location=torch.device(&#39;cpu&#39;))
recall_val_dense161_lesion = best_dense161_lesion[&#39;metrics&#39;][&#39;recall&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recall_val_dense161_lesion 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.684509306993473
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Image(&#39;experiments/dense161_eq3_exclutest_lesion_v1/confusion_matrix.png&#39;, width=900)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_24_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;resnet152&#34;&gt;ResNet152&lt;/h3&gt;

&lt;p&gt;For the ResNet152, the best validation mean of recalls is about 0.7202.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;best_res152_lesion = torch.load(
    &#39;experiments/res152_eq3_exclutest_lesion_v1/model_best.pth.tar&#39;, 
    map_location=torch.device(&#39;cpu&#39;))
recall_val_res152_lesion = best_res152_lesion[&#39;metrics&#39;][&#39;recall&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recall_val_res152_lesion 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.7202260074093291
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Image(&#39;experiments/res152_eq3_exclutest_lesion_v1/recall.png&#39;, width=700)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_28_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np 
import matplotlib.pyplot as plt

def plot_metric(train_loss, test_loss, name, plot_type=&#39;loss&#39;):
    epochs = range(len(train_losses))

    f = plt.figure()
    plt.title(f&amp;quot;{name} {plot_type} plot&amp;quot;)
    plt.xlabel(&amp;quot;epoch&amp;quot;)
    plt.ylabel(f&amp;quot;{plot_type}&amp;quot;)
    plt.grid(True)
    plt.plot(epochs, train_loss, &#39;b&#39;, marker=&#39;o&#39;, label=f&#39;train {plot_type}&#39;)
    plt.plot(epochs, test_loss, &#39;r&#39;, marker=&#39;o&#39;, label=f&#39;val {plot_type}&#39;)
    plt.legend()

train_losses, test_losses = np.load(
    &#39;experiments/res152_eq3_exclutest_lesion_v1/final_results.npy&#39;)
plot_metric(train_losses, test_losses, &#39;ResNet152_lesion&#39;, &#39;loss&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_29_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# ResNet152 val
Image(&#39;experiments/res152_eq3_exclutest_lesion_v1/confusion_matrix.png&#39;, width=900)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_30_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see from the results, it&amp;rsquo;s not a satisfactory performance for both the DenseNet161 and ResNet152 as they have only around 0.7 mean of recalls. No matter what, let&amp;rsquo;s have a look at how they perform on the test set.&lt;/p&gt;

&lt;h2 id=&#34;metrics-on-the-test-set&#34;&gt;Metrics on the test set&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import json

import numpy as np
import pandas as pd
from sklearn import metrics

test_results_path = &#39;test_results&#39;
# model_ids =[&#39;dense161&#39;,&#39;res101&#39;,&#39;res152&#39;]
result_paths = [d for d in os.listdir(
    test_results_path) if not d.startswith(&#39;.&#39;)]
result_paths = [d for d in result_paths if &#39;lesion&#39; in d]
# print(result_paths)

model_metrics = {}
for i in result_paths:
    fp = os.path.join(test_results_path, i, &#39;metrics_results.json&#39;)
    y_true = np.load(os.path.join(test_results_path, i, &#39;val_true.npy&#39;))
    y_pred = np.load(os.path.join(test_results_path, i, &#39;val_pred.npy&#39;))
    with open(fp) as f:
        rec = json.load(f)
        rec[&#39;f1&#39;] = metrics.f1_score(y_true, y_pred, average=&#39;macro&#39;)
        rec[&#39;mcc&#39;] = metrics.matthews_corrcoef(y_true, y_pred)
    model_metrics[i] = rec

df_results_lesion = pd.read_json(json.dumps(model_metrics), orient=&#39;index&#39;).drop(
    columns=[&#39;bacc&#39;]).sort_values(by=&#39;recall&#39;, ascending=False)
df_results_lesion[&#39;acc&#39;] = df_results_lesion[&#39;acc&#39;] / 100
df_results_lesion = df_results_lesion[[&#39;recall&#39;, &#39;prec&#39;, &#39;f1&#39;, &#39;mcc&#39;, &#39;acc&#39;]]
df_results_lesion.columns = [&#39;Recall&#39;, &#39;Precision&#39;, &#39;F1&#39;, &#39;MCC&#39;, &#39;ACC&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_results_lesion.loc[[&#39;dense161_lesion&#39;,&#39;res152_lesion&#39;]].round(4)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Recall&lt;/th&gt;
      &lt;th&gt;Precision&lt;/th&gt;
      &lt;th&gt;F1&lt;/th&gt;
      &lt;th&gt;MCC&lt;/th&gt;
      &lt;th&gt;ACC&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;dense161_lesion&lt;/th&gt;
      &lt;td&gt;0.9105&lt;/td&gt;
      &lt;td&gt;0.8085&lt;/td&gt;
      &lt;td&gt;0.8504&lt;/td&gt;
      &lt;td&gt;0.8210&lt;/td&gt;
      &lt;td&gt;0.9111&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;res152_lesion&lt;/th&gt;
      &lt;td&gt;0.8594&lt;/td&gt;
      &lt;td&gt;0.7542&lt;/td&gt;
      &lt;td&gt;0.7971&lt;/td&gt;
      &lt;td&gt;0.7072&lt;/td&gt;
      &lt;td&gt;0.8465&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;The surperising result shows a much higher mean of recalls for both of the models on the test dataset, from around 0.7 to 0.9105 and 0.8594.&lt;/p&gt;

&lt;p&gt;I also tested ensembles of the the trained models with different weights on each (though without grid search).&lt;/p&gt;

&lt;p&gt;Besides pick the model when with the highest mean of recalls, I also used the DenseNet161 model with the highest MCC score during validation.&lt;/p&gt;

&lt;p&gt;The results are also surprisingly good.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_results_lesion.round(4)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Recall&lt;/th&gt;
      &lt;th&gt;Precision&lt;/th&gt;
      &lt;th&gt;F1&lt;/th&gt;
      &lt;th&gt;MCC&lt;/th&gt;
      &lt;th&gt;ACC&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;ensemble_dense161_6_res152_4_lesion&lt;/th&gt;
      &lt;td&gt;0.9369&lt;/td&gt;
      &lt;td&gt;0.7939&lt;/td&gt;
      &lt;td&gt;0.8558&lt;/td&gt;
      &lt;td&gt;0.8067&lt;/td&gt;
      &lt;td&gt;0.9018&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ensemble_dense161_4_res152_6_lesion&lt;/th&gt;
      &lt;td&gt;0.9206&lt;/td&gt;
      &lt;td&gt;0.8132&lt;/td&gt;
      &lt;td&gt;0.8610&lt;/td&gt;
      &lt;td&gt;0.7808&lt;/td&gt;
      &lt;td&gt;0.8884&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;dense161_lesion&lt;/th&gt;
      &lt;td&gt;0.9105&lt;/td&gt;
      &lt;td&gt;0.8085&lt;/td&gt;
      &lt;td&gt;0.8504&lt;/td&gt;
      &lt;td&gt;0.8210&lt;/td&gt;
      &lt;td&gt;0.9111&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;dense161_lesion_mcc&lt;/th&gt;
      &lt;td&gt;0.9095&lt;/td&gt;
      &lt;td&gt;0.8540&lt;/td&gt;
      &lt;td&gt;0.8789&lt;/td&gt;
      &lt;td&gt;0.8236&lt;/td&gt;
      &lt;td&gt;0.9144&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;ensemble_dense161_res152_lesion&lt;/th&gt;
      &lt;td&gt;0.9055&lt;/td&gt;
      &lt;td&gt;0.8052&lt;/td&gt;
      &lt;td&gt;0.8491&lt;/td&gt;
      &lt;td&gt;0.7931&lt;/td&gt;
      &lt;td&gt;0.8960&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;res152_lesion&lt;/th&gt;
      &lt;td&gt;0.8594&lt;/td&gt;
      &lt;td&gt;0.7542&lt;/td&gt;
      &lt;td&gt;0.7971&lt;/td&gt;
      &lt;td&gt;0.7072&lt;/td&gt;
      &lt;td&gt;0.8465&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Image(&#39;test_results/dense161_lesion/confusion_matrix.png&#39;, width=900)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_37_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Image(&#39;test_results/dense161_lesion_mcc/confusion_matrix.png&#39;, width=900)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_38_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Image(&#39;test_results/ensemble_dense161_6_res152_4_lesion/confusion_matrix.png&#39;, width=900)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_39_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Image(&#39;test_results/res152_lesion/confusion_matrix.png&#39;, width=900)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_40_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;However, I still doubt why the test metrics are much higher than when in validation (even higher than those deeply hacked top ranks in the ISIC 2018 challenge).&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve thought about and also discussed with others on the possible flaws in my solution. However, I couldn&amp;rsquo;t find a likely problem caused the very high mean of recalls on the tes tset. There&amp;rsquo;s no leakage of information from training set to test set, as I groupped and splitted the datasets according to the lesion_id.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;re very welcomed to contact me if you have any idea.&lt;/p&gt;

&lt;h1 id=&#34;discussion-of-limitations-and-possible-improvements&#34;&gt;Discussion of limitations and possible improvements&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Should use all the images for each lesion_id&lt;/li&gt;
&lt;li&gt;Could&amp;rsquo;ve train longer&lt;/li&gt;
&lt;li&gt;Sacrifice majority classes for the performance on the minority classes, &lt;code&gt;nv&lt;/code&gt; could be better as the given data&lt;/li&gt;
&lt;li&gt;The experiments were not well controled, no comparison on the performance when a single variable changed, such as

&lt;ul&gt;
&lt;li&gt;the use of oversampling, fine-tune to get a balance&lt;/li&gt;
&lt;li&gt;different ways of training set transformations&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Fine-tune hyper parameters&lt;/li&gt;
&lt;li&gt;Look into other variables in meta_data, if could be combined to improve the classification performance&lt;/li&gt;
&lt;li&gt;Input images of only lesion region, as semantic segmentation (the task 1 of ISIC 2018)&lt;/li&gt;
&lt;li&gt;Color constancy (mentioned in &lt;a href=&#34;https://www.mdpi.com/1424-8220/18/2/556&#34; target=&#34;_blank&#34;&gt;leadboard high-rank manuscript&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Focal loss function (1 report mentioned smaller variance on accuracy)&lt;/li&gt;
&lt;li&gt;Get extra data&lt;/li&gt;
&lt;li&gt;Exploration of other models&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;references-not-mentioned-yet&#34;&gt;References not mentioned yet&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/deep-learning-for-diagnosis-of-skin-images-with-fastai-792160ab5495&#34; target=&#34;_blank&#34;&gt;Deep Learning for Diagnosis of Skin Images with fastai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@rzuazua/skin-cancer-detection-with-deep-learning-4a7e3fce7ef9&#34; target=&#34;_blank&#34;&gt;Improving Skin Cancer Detection with Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A literature review of current technologies on health data integration for patient-centered health management</title>
      <link>https://pcx.linkedinfo.co/publication/pcx-2019-b/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/publication/pcx-2019-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-label classification to predict topic tags of technical articles from LinkedInfo.co</title>
      <link>https://pcx.linkedinfo.co/post/text-tag-prediction/</link>
      <pubDate>Wed, 11 Sep 2019 13:36:43 +0200</pubDate>
      <guid>https://pcx.linkedinfo.co/post/text-tag-prediction/</guid>
      <description>
&lt;p&gt;
This code snippet is to predict topic tags based on the text of an article. Each article could have 1 or more tags (usually have at least 1 tag), and the tags are not mutually exclusive. So this is a multi-label classification problem. It&amp;#39;s different from multi-class classification, the classes in multi-class classification are mutually exclusive, i.e., each item belongs to 1 and only 1 class.
&lt;/p&gt;
&lt;p&gt;
In this snippet, we will use &lt;code class=&#34;verbatim&#34;&gt;OneVsRestClassifier&lt;/code&gt; (the One-Vs-the-Rest) in scikit-learn to process the multi-label classification. The article data will be retrieved from &lt;a href=&#34;https://linkedinfo.co&#34;&gt;LinkedInfo.co&lt;/a&gt; via Web API. The methods in this snippet should give credits to &lt;a href=&#34;https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html&#34;&gt;Working With Text Data - scikit-learn&lt;/a&gt; and &lt;a href=&#34;https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff&#34;&gt;this post&lt;/a&gt;.
&lt;/p&gt;
    
  &lt;h2&gt;Table of Contents&lt;/h2&gt;
  HAHAHUGOSHORTCODE-TOC0-HBHB
&lt;h2 id=&#34;preprocessing-data-and-explore-the-method&#34;&gt;
Preprocessing data and explore the method
&lt;/h2&gt;
&lt;p&gt;
&lt;code class=&#34;verbatim&#34;&gt;dataset.df_tags&lt;/code&gt; fetches the data set from &lt;a href=&#34;https://linkedinfo.co&#34;&gt;LinkedInfo.co&lt;/a&gt;. It calls Web API of LinkedInfo.co to retrieve the article list, and then download and extract the full text of each article based on an article&amp;#39;s url. The tags of each article are encoded using &lt;code class=&#34;verbatim&#34;&gt;MultiLabelBinarizer&lt;/code&gt; in scikit-learn. The implementation of the code could be found in &lt;a href=&#34;https://github.com/ddxgz/linkedinfo-ml-models/blob/master/dataset.py&#34;&gt;dataset.py&lt;/a&gt;. We&amp;#39;ve set the parameter of &lt;code class=&#34;verbatim&#34;&gt;content_length_threshold&lt;/code&gt; to 100 to screen out the articles with less than 100 for the description or full text.
&lt;/p&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; dataset

ds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dataset&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;df_tags(content_length_threshold&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
The dataset contains 3353 articles by the time retrieved the data. The
dataset re returned as an object with the following attribute:
&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
ds.data: pandas.DataFrame with cols of title, description, fulltext
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
ds.target: encoding of tagsID
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
ds.target_names: tagsID
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
ds.target_decoded: the list of lists contains tagsID for each info
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ds&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&#34;align-right&#34;&gt;&lt;/td&gt;
&lt;td&gt;description&lt;/td&gt;
&lt;td&gt;fulltext&lt;/td&gt;
&lt;td&gt;title&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;align-right&#34;&gt;0&lt;/td&gt;
&lt;td&gt;Both HTTP 1.x and HTTP/2 rely on lower level c…&lt;/td&gt;
&lt;td&gt;[Stressgrid](&lt;em&gt;)\n\n__\n\n[](&lt;/em&gt; &amp;#34;home&amp;#34;)\n\n * […&lt;/td&gt;
&lt;td&gt;Achieving 100k connections per second with Elixir&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;align-right&#34;&gt;1&lt;/td&gt;
&lt;td&gt;At Phusion we run a simple multithreaded HTTP …&lt;/td&gt;
&lt;td&gt;[![Hongli Lai](&lt;em&gt;images/avatar-b64f1ad5.png)](&lt;/em&gt;…&lt;/td&gt;
&lt;td&gt;What causes Ruby memory bloat?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;align-right&#34;&gt;2&lt;/td&gt;
&lt;td&gt;Have you ever wanted to contribute to a projec…&lt;/td&gt;
&lt;td&gt;[ ![Real Python](/static/real-python-logo.ab1a…&lt;/td&gt;
&lt;td&gt;Managing Multiple Python Versions With pyenv&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;align-right&#34;&gt;3&lt;/td&gt;
&lt;td&gt;安卓在版本Pie中第一次引入了ART优化配置文件，这个新特性利用发送到Play Cloud的…&lt;/td&gt;
&lt;td&gt;安卓在版本Pie中第一次引入了[ART优化配置文件](&lt;a href=&#34;https://youtu.be/Yi...&#34;&gt;https://youtu.be/Yi...&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ART云配置文件，提高安卓应用的性能&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;align-right&#34;&gt;4&lt;/td&gt;
&lt;td&gt;I work at Red Hat on GCC, the GNU Compiler Col…&lt;/td&gt;
&lt;td&gt;[ ![Red Hat\nLogo](&lt;a href=&#34;https://developers.redhat.c...&#34;&gt;https://developers.redhat.c...&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Usability improvements in GCC 9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ds&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;target[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
array([[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;],
       [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;],
       [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;],
       [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;],
       [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ds&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;target_names[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
array([&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;academia&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;access-control&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;activemq&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;aes&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;agile&amp;#39;&lt;/span&gt;],
      dtype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;object)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ds&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;target_decoded[:&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;]
[[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;concurrency&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;elixir&amp;#39;&lt;/span&gt;],
 [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ruby&amp;#39;&lt;/span&gt;],
 [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;python&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;virtualenv&amp;#39;&lt;/span&gt;],
 [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;android&amp;#39;&lt;/span&gt;],
 [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;gcc&amp;#39;&lt;/span&gt;]]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
The following snippet is the actual process of getting the above
dataset, by reading from file.
&lt;/p&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; json
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; pd
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.preprocessing &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; MultiLabelBinarizer

infos_file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data/infos/infos_0_3353_fulltext.json&amp;#39;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(infos_file, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:
    infos &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; json&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(f)

content_length_threshold &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;

data_lst &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
tags_lst &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; info &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; infos[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;content&amp;#39;&lt;/span&gt;]:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(info[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;fulltext&amp;#39;&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; content_length_threshold:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len(info[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;description&amp;#39;&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; content_length_threshold:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;
    data_lst&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append({&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;title&amp;#39;&lt;/span&gt;: info[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;title&amp;#39;&lt;/span&gt;],
                     &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;description&amp;#39;&lt;/span&gt;: info[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;description&amp;#39;&lt;/span&gt;],
                     &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;fulltext&amp;#39;&lt;/span&gt;: info[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;fulltext&amp;#39;&lt;/span&gt;]})
    tags_lst&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append([tag[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tagID&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; tag &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; info[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tags&amp;#39;&lt;/span&gt;]])

df_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(data_lst)
df_tags &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(tags_lst)

&lt;span style=&#34;color:#75715e&#34;&gt;# fit and transform the binarizer&lt;/span&gt;
mlb &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MultiLabelBinarizer()
Y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mlb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_transform(tags_lst)
Y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;3221&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;560&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
Now we&amp;#39;ve transformed the target (tags) but we cannot directly perform
the algorithms on the text data, so we have to process and transform
them into vectors. In order to do this, we will use &lt;code class=&#34;verbatim&#34;&gt;TfidfVectorizer&lt;/code&gt; to
preprocess, tokenize, filter stop words and transform the text data. The
&lt;code class=&#34;verbatim&#34;&gt;TfidfVectorizer&lt;/code&gt; implements the
&lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34;&gt;&lt;em&gt;tf-idf&lt;/em&gt;&lt;/a&gt; (Term
Frequency-Inverse Document Frequency) to reflect how important a word
is to to a document in a collection of documents.
&lt;/p&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.feature_extraction.text &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; TfidfVectorizer

&lt;span style=&#34;color:#75715e&#34;&gt;# Use the default parameters for now, use_idf=True in default&lt;/span&gt;
vectorizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TfidfVectorizer()
&lt;span style=&#34;color:#75715e&#34;&gt;# Use the short descriptions for now for faster processing&lt;/span&gt;
X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; vectorizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_transform(df_data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;description)
X&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;3221&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;35506&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
As mentioned in the beginning, this is a multi-label classification
problem, we will use &lt;code class=&#34;verbatim&#34;&gt;OneVsRestClassifier&lt;/code&gt; to tackle our problem. And
firstly we will use the SVM (Support Vector Machines) with linear
kernel, implemented as &lt;code class=&#34;verbatim&#34;&gt;LinearSVC&lt;/code&gt; in scikit-learn, to do the
classification.
&lt;/p&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.multiclass &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; OneVsRestClassifier
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.svm &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LinearSVC
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; train_test_split

&lt;span style=&#34;color:#75715e&#34;&gt;# Use default parameters, and train and test with small set of samples.&lt;/span&gt;
clf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; OneVsRestClassifier(LinearSVC())

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.utils &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; resample

X_sample, Y_sample &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; resample(
    X, Y, n_samples&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;, replace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False, random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# X_sample_test, Y_sample_test = resample(&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#     X, Y, n_samples=10, replace=False, random_state=1)&lt;/span&gt;

X_sample_train, X_sample_test, Y_sample_train, Y_sample_test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_test_split(
    X_sample, Y_sample, test_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;, random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;)

clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_sample, Y_sample)
Y_sample_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_sample_test)

&lt;span style=&#34;color:#75715e&#34;&gt;# Inverse transform the vectors back to tags&lt;/span&gt;
pred_transformed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mlb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;inverse_transform(Y_sample_pred)
test_transformed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mlb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;inverse_transform(Y_sample_test)

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (t, p) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(test_transformed, pred_transformed):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tags: {t} predicted as: {p}&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;pre class=&#34;example&#34;&gt;
tags: (&#39;javascript&#39;,) predicted as: (&#39;javascript&#39;,)
tags: (&#39;erasure-code&#39;, &#39;storage&#39;) predicted as: ()
tags: (&#39;mysql&#39;, &#39;network&#39;) predicted as: ()
tags: (&#39;token&#39;,) predicted as: ()
tags: (&#39;flask&#39;, &#39;python&#39;, &#39;web&#39;) predicted as: ()
tags: (&#39;refactoring&#39;,) predicted as: ()
tags: (&#39;emacs&#39;,) predicted as: ()
tags: (&#39;async&#39;, &#39;javascript&#39;, &#39;promises&#39;) predicted as: (&#39;async&#39;, &#39;javascript&#39;)
tags: (&#39;neural-networks&#39;,) predicted as: ()
tags: (&#39;kubernetes&#39;,) predicted as: (&#39;kubernetes&#39;,)
&lt;/pre&gt;
&lt;p&gt;
Though not very satisfied, this classifier predicted right a few tags.
Next we&amp;#39;ll try to search for the best parameters for the classifier and
train with fulltext of articles.
&lt;/p&gt;
&lt;h2 id=&#34;search-for-best-model-parameters-for-svm-with-linear-kernel&#34;&gt;
Search for best model parameters for SVM with linear kernel
&lt;/h2&gt;
&lt;p&gt;
For the estimators &lt;code class=&#34;verbatim&#34;&gt;TfidfVectorizer&lt;/code&gt; and &lt;code class=&#34;verbatim&#34;&gt;LinearSVC&lt;/code&gt;, they both have
many parameters could be tuned for better performance. We&amp;#39;ll the
&lt;code class=&#34;verbatim&#34;&gt;GridSearchCV&lt;/code&gt; to search for the best parameters with the help of
&lt;code class=&#34;verbatim&#34;&gt;Pipeline&lt;/code&gt;.
&lt;/p&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.pipeline &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Pipeline
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn.model_selection &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; train_test_split, GridSearchCV


&lt;span style=&#34;color:#75715e&#34;&gt;# Split the dataset into training and test set, and use fulltext of articles:&lt;/span&gt;
X_train, X_test, Y_train, Y_test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_test_split(
    df_data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fulltext, Y, test_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# Build vectorizer classifier pipeline&lt;/span&gt;
clf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Pipeline([
    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;vect&amp;#39;&lt;/span&gt;, TfidfVectorizer()),
    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;clf&amp;#39;&lt;/span&gt;, OneVsRestClassifier(LinearSVC())),
])

&lt;span style=&#34;color:#75715e&#34;&gt;# Grid search parameters, I minimized the parameter set based on previous&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# experience to accelerate the processing speed.&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# And the combination of penalty=&amp;#39;l1&amp;#39; and loss=&amp;#39;squared_hinge&amp;#39; are not supported when dual=True&lt;/span&gt;
parameters &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;vect__ngram_range&amp;#39;&lt;/span&gt;: [(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;), (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)],
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;vect__max_df&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.7&lt;/span&gt;],
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;vect__min_df&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;],
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;vect__use_idf&amp;#39;&lt;/span&gt;: [True, False],
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;clf__estimator__penalty&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;l1&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;l2&amp;#39;&lt;/span&gt;],
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;clf__estimator__C&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;],
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;clf__estimator__dual&amp;#39;&lt;/span&gt;: [False],
}

gs_clf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; GridSearchCV(clf, parameters, cv&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, n_jobs&lt;span style=&#34;color:#f92672&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
gs_clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train, Y_train)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; datetime
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; metrics


&lt;span style=&#34;color:#75715e&#34;&gt;# Predict the outcome on the testing set in a variable named y_predicted&lt;/span&gt;
Y_predicted &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gs_clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(metrics&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;classification_report(Y_test, Y_predicted))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(gs_clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;best_params_)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(gs_clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;best_score_)

&lt;span style=&#34;color:#75715e&#34;&gt;# Export some of the result cols&lt;/span&gt;
cols &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mean_test_score&amp;#39;&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mean_fit_time&amp;#39;&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;param_vect__ngram_range&amp;#39;&lt;/span&gt;,
]
df_result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(gs_clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cv_results_)
df_result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df_result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sort_values(by&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rank_test_score&amp;#39;&lt;/span&gt;)
df_result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df_result[cols]

timestamp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; datetime&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;now()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strftime(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;%Y-%m-&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%d&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;_%H-%M-%S&amp;#39;&lt;/span&gt;)
df_result&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_html(
    f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;data/results/gridcv_results_{timestamp}_linearSVC.html&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
Here we attach the top-5 performed classifiers with selected parameters.
&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;rank_test_score&lt;/th&gt;
      &lt;th&gt;mean_test_score&lt;/th&gt;
      &lt;th&gt;mean_fit_time&lt;/th&gt;
      &lt;th&gt;param_vect__max_df&lt;/th&gt;
      &lt;th&gt;param_vect__ngram_range&lt;/th&gt;
      &lt;th&gt;param_vect__use_idf&lt;/th&gt;
      &lt;th&gt;param_clf__estimator__penalty&lt;/th&gt;
      &lt;th&gt;param_clf__estimator__C&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;64&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.140811&lt;/td&gt;
      &lt;td&gt;96.127405&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;(1, 4)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;l1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;70&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.140215&lt;/td&gt;
      &lt;td&gt;103.252332&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
      &lt;td&gt;(1, 4)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;l1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;58&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.140215&lt;/td&gt;
      &lt;td&gt;98.990952&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;(1, 4)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;l1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;154&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.140215&lt;/td&gt;
      &lt;td&gt;1690.433151&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;(1, 4)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;l1&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;68&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.139618&lt;/td&gt;
      &lt;td&gt;70.778621&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
      &lt;td&gt;(1, 3)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;l1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;headline-3&#34;&gt;
Training and testing with the best parameters
&lt;/h2&gt;
&lt;p&gt;
Based on the grid search results, we found the following parameters
combined with the default parameters have the best performance. Now
let&amp;#39;s see how it will perform.
&lt;/p&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;X_train, X_test, Y_train, Y_test &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_test_split(
    df_data, Y, test_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, random_state&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;)

clf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Pipeline([
    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;vect&amp;#39;&lt;/span&gt;, TfidfVectorizer(use_idf&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True,
                             max_df&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt;, ngram_range&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;])),
    (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;clf&amp;#39;&lt;/span&gt;, OneVsRestClassifier(LinearSVC(penalty&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;l1&amp;#39;&lt;/span&gt;, C&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, dual&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False))),
])

clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(X_train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fulltext, Y_train)


Y_pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; clf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(X_test&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fulltext)

&lt;span style=&#34;color:#75715e&#34;&gt;# Inverse transform the vectors back to tags&lt;/span&gt;
pred_transformed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mlb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;inverse_transform(Y_pred)
test_transformed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mlb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;inverse_transform(Y_test)

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (title, t, p) &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(X_test&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;title, test_transformed, pred_transformed):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Article title: {title} &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;
          f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Manual tags:  {t} &lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;
          f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;predicted as: {p}&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
Here below is a fraction of the list that shows the manually input tags and the predicted tags. We can see that usually the more frequently appeared and more popular tags have better change to be correctly predicted. Personally, I would say the prediction is satisfied to me comparing when I tag the articles manually. However, there&amp;#39;s much room for improvement.
&lt;/p&gt;
&lt;div class=&#34;src src-markdown&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;Article title: Will PWAs Replace Native Mobile Apps?
Manual tags:  (&amp;#39;pwa&amp;#39;,)
predicted as: (&amp;#39;pwa&amp;#39;,)

Article title: 基于Consul的分布式信号量实现
Manual tags:  (&amp;#39;consul&amp;#39;, &amp;#39;distributed-system&amp;#39;)
predicted as: (&amp;#39;microservices&amp;#39;, &amp;#39;multithreading&amp;#39;)

Article title: commit 和 branch 理解深入
Manual tags:  (&amp;#39;git&amp;#39;,)
predicted as: (&amp;#39;git&amp;#39;,)

Article title: Existential types in Scala
Manual tags:  (&amp;#39;scala&amp;#39;,)
predicted as: (&amp;#39;scala&amp;#39;,)

Article title: Calling back into Python from llvmlite-JITed code
Manual tags:  (&amp;#39;jit&amp;#39;, &amp;#39;python&amp;#39;)
predicted as: (&amp;#39;compiler&amp;#39;, &amp;#39;python&amp;#39;)

Article title: Writing a Simple Linux Kernel Module
Manual tags:  (&amp;#39;kernel&amp;#39;, &amp;#39;linux&amp;#39;)
predicted as: (&amp;#39;linux&amp;#39;,)

Article title: Semantic segmentation with OpenCV and deep learning
Manual tags:  (&amp;#39;deep-learning&amp;#39;, &amp;#39;opencv&amp;#39;)
predicted as: (&amp;#39;deep-learning&amp;#39;, &amp;#39;image-classification&amp;#39;, &amp;#39;opencv&amp;#39;)

Article title: Transducers: Efficient Data Processing Pipelines in JavaScript
Manual tags:  (&amp;#39;javascript&amp;#39;,)
predicted as: (&amp;#39;javascript&amp;#39;,)

Article title: C++之stl::string写时拷贝导致的问题
Manual tags:  (&amp;#39;cpp&amp;#39;,)
predicted as: (&amp;#39;functional-programming&amp;#39;,)

Article title: WebSocket 浅析
Manual tags:  (&amp;#39;websocket&amp;#39;,)
predicted as: (&amp;#39;websocket&amp;#39;,)

Article title: You shouldn’t name your variables after their types for the same reason you wouldn’t name your pets “dog” or “cat”
Manual tags:  (&amp;#39;golang&amp;#39;,)
predicted as: (&amp;#39;golang&amp;#39;,)

Article title: Introduction to Data Visualization using Python
Manual tags:  (&amp;#39;data-visualization&amp;#39;, &amp;#39;python&amp;#39;)
predicted as: (&amp;#39;data-visualization&amp;#39;, &amp;#39;matplotlib&amp;#39;, &amp;#39;python&amp;#39;)

Article title: How JavaScript works: A comparison with WebAssembly + why in certain cases it’s better to use it over JavaScript
Manual tags:  (&amp;#39;javascript&amp;#39;, &amp;#39;webassembly&amp;#39;)
predicted as: (&amp;#39;javascript&amp;#39;, &amp;#39;webassembly&amp;#39;)

Article title: Parsing logs 230x faster with Rust
Manual tags:  (&amp;#39;log&amp;#39;, &amp;#39;rust&amp;#39;)
predicted as: (&amp;#39;rust&amp;#39;,)

Article title: Troubleshooting Memory Issues in Java Applications
Manual tags:  (&amp;#39;java&amp;#39;, &amp;#39;memory&amp;#39;)
predicted as: (&amp;#39;java&amp;#39;,)

Article title: How to use Docker for Node.js development
Manual tags:  (&amp;#39;docker&amp;#39;, &amp;#39;node.js&amp;#39;)
predicted as: (&amp;#39;docker&amp;#39;,)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;headline-4&#34;&gt;
A glance at the different evaluation metrics
&lt;/h2&gt;
&lt;p&gt;
Now let&amp;#39;s have a look at the evaluation metrics on the prediction performance. Evaluating multi-label classification is very different from evaluating binary classification. There&amp;#39;re quite many different evaluation methods for different situations in &lt;a href=&#34;https://scikit-learn.org/stable/modules/model_evaluation.html&#34;&gt;the model evaluation part of scikit-learn&amp;#39;s documentation&lt;/a&gt;. We will take a look at the ones that suit this problem.
&lt;/p&gt;
&lt;p&gt;
We can start with the &lt;code class=&#34;verbatim&#34;&gt;accuracy_score&lt;/code&gt; function in &lt;code class=&#34;verbatim&#34;&gt;metrics&lt;/code&gt; module. As mentioned in scikit-learn documentation, in multi-label classification, a subset accuracy is 1.0 when the entire set of predicted labels for a sample matches strictly with the true label set. The equation is simple like this:
&lt;/p&gt;
&lt;div class=&#34;export-block&#34;&gt;
$$\operatorname{accuracy}(y, \hat{y})=\frac{1}{n_{\text {samples }}} \sum_{i=0}^{n_{\text {minples }}-1} 1\left(\hat{y}_{i}=y_{i}\right)$$&lt;/div&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sklearn &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; metrics
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt

metrics&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;accuracy_score(Y_test, Y_pred)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;src src-md&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-md&#34; data-lang=&#34;md&#34;&gt;0.26356589147286824&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
The score is somehow low. But we should be noted that for this problem, an inexact match of the labels is acceptable in many cases, e.g., an article talks about the golang&amp;#39;s interface is predicted with an only label &lt;code class=&#34;verbatim&#34;&gt;golang&lt;/code&gt; while it was manually labeled with &lt;code class=&#34;verbatim&#34;&gt;golang&lt;/code&gt; and &lt;code class=&#34;verbatim&#34;&gt;interface&lt;/code&gt;. So to my opinion, this &lt;code class=&#34;verbatim&#34;&gt;accuracy_score&lt;/code&gt; is not a good evaluation metric for this problem.
&lt;/p&gt;
&lt;p&gt;
Now let&amp;#39;s see the &lt;code class=&#34;verbatim&#34;&gt;classification_report&lt;/code&gt; that presents averaged precision, recall and f1-score.
&lt;/p&gt;
&lt;div class=&#34;src src-python&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(metrics&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;classification_report(Y_test, Y_pred))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;precision&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;recall&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;f1-score&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;support&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;micro&lt;/td&gt;
&lt;td&gt;avg&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;0.74&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;0.42&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;0.54&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;1186&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;macro&lt;/td&gt;
&lt;td&gt;avg&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;0.17&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;0.13&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;0.14&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;1186&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;weighted&lt;/td&gt;
&lt;td&gt;avg&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;0.60&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;0.42&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;0.48&lt;/td&gt;
&lt;td class=&#34;align-right&#34;&gt;1186&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;
Let&amp;#39;s look at the &lt;strong&gt;micro&lt;/strong&gt; row. Why? Let me quote scikit-learn&amp;#39;s documentation:
&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;
&amp;#34;micro&amp;#34; gives each sample-class pair an equal contribution to the overall metric (except as a result of sample-weight). Rather than summing the metric per class, this sums the dividends and divisors that make up the per-class metrics to calculate an overall quotient. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;
Here we&amp;#39;re more interested in the average precision, which is 0.74. As we mentioned, for this problem and for me, it&amp;#39;s more important to not predict a label that should be negative to an article. Some of the labels for an article, e.g., the label &lt;code class=&#34;verbatim&#34;&gt;interface&lt;/code&gt; for the just mentioned article, are less important. So I&amp;#39;m OK for having a low score of recall, which measure how good the model predicts all the labels as the manually labeled.
&lt;/p&gt;
&lt;p&gt;
However, there&amp;#39;s much room for improvement.
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
Many of the labels have very few appearances or even once. These labels could be filtered out or oversampling with text augmentation to mitigate the impact to model performance.
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
The training-test set split should be controlled by methods like stratified sampling, so that all the labels would appear in both sets with similar percentages. But again this problem is unlikely to be solved by now since there isn&amp;#39;t enough samples.
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
Another problem to be though about is, the training samples are not equally labeled, i.e., for the same example all the articles talking about golang&amp;#39;s interface, some of them labeled with &lt;code class=&#34;verbatim&#34;&gt;golang&lt;/code&gt; + &lt;code class=&#34;verbatim&#34;&gt;interface&lt;/code&gt; while some of them labeled only &lt;code class=&#34;verbatim&#34;&gt;golang&lt;/code&gt;.
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Explore the house prices kaggle competition</title>
      <link>https://pcx.linkedinfo.co/post/houseprice/</link>
      <pubDate>Tue, 11 Jun 2019 17:38:33 +0200</pubDate>
      <guid>https://pcx.linkedinfo.co/post/houseprice/</guid>
      <description>

&lt;p&gt;Thanks to &lt;a href=&#34;https://www.kaggle.com/pmarcelino
 /comprehensive-data-exploration-with-python&#34; target=&#34;_blank&#34;&gt;pmarcelino&lt;/a&gt; and
 &lt;a href=&#34;https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard&#34; target=&#34;_blank&#34;&gt;serigne&lt;/a&gt; for their great work.&lt;/p&gt;

&lt;p&gt;This is my second kaggle competition to practice on the knowledge of data
 analysis and machine learning. Unlike the Titanic competition, this house
 prices is a regression problem. So there will be much difference from the
 previous binary classification. For this competition, we will have 79
 variables that describe various aspects of a house and with a price in the
 training data set. And then predict the prices of houses in the testing set
 based on the 79 variables. This will be a long journey with the 79 variables.
 So let&amp;rsquo;s start to explore the data with the data description.
 &lt;!-- * data exploration (checking missing, outliners, distribution), --&gt;
 &lt;!-- * preprocessing (imputation missing values, skew, encoding categorical), --&gt;
 &lt;!-- * base model, more models, --&gt;
 &lt;!-- * [neural_model], --&gt;
 &lt;!-- * models stats, --&gt;&lt;/p&gt;

&lt;p&gt;&lt;h2&gt;Table of Contents&lt;/h2&gt;
  HAHAHUGOSHORTCODE-TOC0-HBHB&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
# from typing import List, Union
# from pysnooper import snoop

import pandas as pd
# import matplotlib.pyplot as plt
# import numpy as np

loc = &#39;house price&#39;
if os.getcwd().split(&#39;/&#39;)[-1] != loc:
    os.chdir(loc)

df_train = pd.read_csv(f&#39;input/train.csv&#39;)
df_test = pd.read_csv(f&#39;input/test.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;data-exploration&#34;&gt;Data exploration&lt;/h1&gt;

&lt;p&gt;Let&amp;rsquo;s firstly have a look at the data we have.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(df_train.shape)
df_train.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(1460, 81)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Id&lt;/th&gt;
      &lt;th&gt;MSSubClass&lt;/th&gt;
      &lt;th&gt;MSZoning&lt;/th&gt;
      &lt;th&gt;LotFrontage&lt;/th&gt;
      &lt;th&gt;LotArea&lt;/th&gt;
      &lt;th&gt;Street&lt;/th&gt;
      &lt;th&gt;Alley&lt;/th&gt;
      &lt;th&gt;LotShape&lt;/th&gt;
      &lt;th&gt;LandContour&lt;/th&gt;
      &lt;th&gt;Utilities&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;PoolArea&lt;/th&gt;
      &lt;th&gt;PoolQC&lt;/th&gt;
      &lt;th&gt;Fence&lt;/th&gt;
      &lt;th&gt;MiscFeature&lt;/th&gt;
      &lt;th&gt;MiscVal&lt;/th&gt;
      &lt;th&gt;MoSold&lt;/th&gt;
      &lt;th&gt;YrSold&lt;/th&gt;
      &lt;th&gt;SaleType&lt;/th&gt;
      &lt;th&gt;SaleCondition&lt;/th&gt;
      &lt;th&gt;SalePrice&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;65.0&lt;/td&gt;
      &lt;td&gt;8450&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2008&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
      &lt;td&gt;208500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;80.0&lt;/td&gt;
      &lt;td&gt;9600&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;2007&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
      &lt;td&gt;181500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;68.0&lt;/td&gt;
      &lt;td&gt;11250&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;IR1&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;2008&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
      &lt;td&gt;223500&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;70&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;60.0&lt;/td&gt;
      &lt;td&gt;9550&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;IR1&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2006&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Abnorml&lt;/td&gt;
      &lt;td&gt;140000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;84.0&lt;/td&gt;
      &lt;td&gt;14260&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;IR1&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;2008&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
      &lt;td&gt;250000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 81 columns&lt;/p&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(df_test.shape)
df_test.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(1459, 80)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Id&lt;/th&gt;
      &lt;th&gt;MSSubClass&lt;/th&gt;
      &lt;th&gt;MSZoning&lt;/th&gt;
      &lt;th&gt;LotFrontage&lt;/th&gt;
      &lt;th&gt;LotArea&lt;/th&gt;
      &lt;th&gt;Street&lt;/th&gt;
      &lt;th&gt;Alley&lt;/th&gt;
      &lt;th&gt;LotShape&lt;/th&gt;
      &lt;th&gt;LandContour&lt;/th&gt;
      &lt;th&gt;Utilities&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;ScreenPorch&lt;/th&gt;
      &lt;th&gt;PoolArea&lt;/th&gt;
      &lt;th&gt;PoolQC&lt;/th&gt;
      &lt;th&gt;Fence&lt;/th&gt;
      &lt;th&gt;MiscFeature&lt;/th&gt;
      &lt;th&gt;MiscVal&lt;/th&gt;
      &lt;th&gt;MoSold&lt;/th&gt;
      &lt;th&gt;YrSold&lt;/th&gt;
      &lt;th&gt;SaleType&lt;/th&gt;
      &lt;th&gt;SaleCondition&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1461&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;RH&lt;/td&gt;
      &lt;td&gt;80.0&lt;/td&gt;
      &lt;td&gt;11622&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Reg&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;MnPrv&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;2010&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;1462&lt;/td&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;81.0&lt;/td&gt;
      &lt;td&gt;14267&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;IR1&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Gar2&lt;/td&gt;
      &lt;td&gt;12500&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;2010&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;1463&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;74.0&lt;/td&gt;
      &lt;td&gt;13830&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;IR1&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;MnPrv&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;2010&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;1464&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;78.0&lt;/td&gt;
      &lt;td&gt;9978&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;IR1&lt;/td&gt;
      &lt;td&gt;Lvl&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;2010&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;1465&lt;/td&gt;
      &lt;td&gt;120&lt;/td&gt;
      &lt;td&gt;RL&lt;/td&gt;
      &lt;td&gt;43.0&lt;/td&gt;
      &lt;td&gt;5005&lt;/td&gt;
      &lt;td&gt;Pave&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;IR1&lt;/td&gt;
      &lt;td&gt;HLS&lt;/td&gt;
      &lt;td&gt;AllPub&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;144&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2010&lt;/td&gt;
      &lt;td&gt;WD&lt;/td&gt;
      &lt;td&gt;Normal&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 80 columns&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;So we have 1460 rows in training set and 1459 rows in testing
 set. Besides the price col in the training set, both data sets have 79 cols of
 variables + 1 col of &amp;lsquo;Id&amp;rsquo;.&lt;/p&gt;

&lt;h2 id=&#34;check-missing-values&#34;&gt;Check missing values&lt;/h2&gt;

&lt;p&gt;Now let&amp;rsquo;s check if there is any missing value in the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;

def cols_missing_value(df):
    df_null_sum = df.isnull().sum()
    df_na = (df.isnull().sum() / len(df)) * 100
    missing_data = pd.concat({&#39;Missing Ratio %&#39;: df_na,
                              &#39;Total&#39;: df_null_sum}, axis=&#39;columns&#39;)
    return missing_data.drop(missing_data[missing_data[&#39;Total&#39;] == 0].index
                             ).sort_values(by=&#39;Total&#39;, ascending=False)


cols_missing_value(df_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Missing Ratio %&lt;/th&gt;
      &lt;th&gt;Total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;PoolQC&lt;/th&gt;
      &lt;td&gt;99.520548&lt;/td&gt;
      &lt;td&gt;1453&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MiscFeature&lt;/th&gt;
      &lt;td&gt;96.301370&lt;/td&gt;
      &lt;td&gt;1406&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Alley&lt;/th&gt;
      &lt;td&gt;93.767123&lt;/td&gt;
      &lt;td&gt;1369&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Fence&lt;/th&gt;
      &lt;td&gt;80.753425&lt;/td&gt;
      &lt;td&gt;1179&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;FireplaceQu&lt;/th&gt;
      &lt;td&gt;47.260274&lt;/td&gt;
      &lt;td&gt;690&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;LotFrontage&lt;/th&gt;
      &lt;td&gt;17.739726&lt;/td&gt;
      &lt;td&gt;259&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageType&lt;/th&gt;
      &lt;td&gt;5.547945&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageYrBlt&lt;/th&gt;
      &lt;td&gt;5.547945&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageFinish&lt;/th&gt;
      &lt;td&gt;5.547945&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageQual&lt;/th&gt;
      &lt;td&gt;5.547945&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageCond&lt;/th&gt;
      &lt;td&gt;5.547945&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtExposure&lt;/th&gt;
      &lt;td&gt;2.602740&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtFinType2&lt;/th&gt;
      &lt;td&gt;2.602740&lt;/td&gt;
      &lt;td&gt;38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtFinType1&lt;/th&gt;
      &lt;td&gt;2.534247&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtCond&lt;/th&gt;
      &lt;td&gt;2.534247&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtQual&lt;/th&gt;
      &lt;td&gt;2.534247&lt;/td&gt;
      &lt;td&gt;37&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MasVnrArea&lt;/th&gt;
      &lt;td&gt;0.547945&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MasVnrType&lt;/th&gt;
      &lt;td&gt;0.547945&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Electrical&lt;/th&gt;
      &lt;td&gt;0.068493&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cols_missing_value(pd.concat((df_train[df_test.columns], df_test)))
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Missing Ratio %&lt;/th&gt;
      &lt;th&gt;Total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;PoolQC&lt;/th&gt;
      &lt;td&gt;99.657417&lt;/td&gt;
      &lt;td&gt;2909&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MiscFeature&lt;/th&gt;
      &lt;td&gt;96.402878&lt;/td&gt;
      &lt;td&gt;2814&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Alley&lt;/th&gt;
      &lt;td&gt;93.216855&lt;/td&gt;
      &lt;td&gt;2721&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Fence&lt;/th&gt;
      &lt;td&gt;80.438506&lt;/td&gt;
      &lt;td&gt;2348&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;FireplaceQu&lt;/th&gt;
      &lt;td&gt;48.646797&lt;/td&gt;
      &lt;td&gt;1420&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;LotFrontage&lt;/th&gt;
      &lt;td&gt;16.649538&lt;/td&gt;
      &lt;td&gt;486&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageFinish&lt;/th&gt;
      &lt;td&gt;5.447071&lt;/td&gt;
      &lt;td&gt;159&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageQual&lt;/th&gt;
      &lt;td&gt;5.447071&lt;/td&gt;
      &lt;td&gt;159&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageCond&lt;/th&gt;
      &lt;td&gt;5.447071&lt;/td&gt;
      &lt;td&gt;159&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageYrBlt&lt;/th&gt;
      &lt;td&gt;5.447071&lt;/td&gt;
      &lt;td&gt;159&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageType&lt;/th&gt;
      &lt;td&gt;5.378554&lt;/td&gt;
      &lt;td&gt;157&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtExposure&lt;/th&gt;
      &lt;td&gt;2.809181&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtCond&lt;/th&gt;
      &lt;td&gt;2.809181&lt;/td&gt;
      &lt;td&gt;82&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtQual&lt;/th&gt;
      &lt;td&gt;2.774923&lt;/td&gt;
      &lt;td&gt;81&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtFinType2&lt;/th&gt;
      &lt;td&gt;2.740665&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtFinType1&lt;/th&gt;
      &lt;td&gt;2.706406&lt;/td&gt;
      &lt;td&gt;79&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MasVnrType&lt;/th&gt;
      &lt;td&gt;0.822199&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MasVnrArea&lt;/th&gt;
      &lt;td&gt;0.787941&lt;/td&gt;
      &lt;td&gt;23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;MSZoning&lt;/th&gt;
      &lt;td&gt;0.137033&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtFullBath&lt;/th&gt;
      &lt;td&gt;0.068517&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtHalfBath&lt;/th&gt;
      &lt;td&gt;0.068517&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Functional&lt;/th&gt;
      &lt;td&gt;0.068517&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Utilities&lt;/th&gt;
      &lt;td&gt;0.068517&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageArea&lt;/th&gt;
      &lt;td&gt;0.034258&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;GarageCars&lt;/th&gt;
      &lt;td&gt;0.034258&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Electrical&lt;/th&gt;
      &lt;td&gt;0.034258&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;KitchenQual&lt;/th&gt;
      &lt;td&gt;0.034258&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;TotalBsmtSF&lt;/th&gt;
      &lt;td&gt;0.034258&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtUnfSF&lt;/th&gt;
      &lt;td&gt;0.034258&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtFinSF2&lt;/th&gt;
      &lt;td&gt;0.034258&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;BsmtFinSF1&lt;/th&gt;
      &lt;td&gt;0.034258&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Exterior2nd&lt;/th&gt;
      &lt;td&gt;0.034258&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;Exterior1st&lt;/th&gt;
      &lt;td&gt;0.034258&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;SaleType&lt;/th&gt;
      &lt;td&gt;0.034258&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;There are quite a lot of missing values, some cols are missing almost all of the data. We need to handle the missing values by imputation or other methods later.&lt;/p&gt;

&lt;h2 id=&#34;a-look-at-distributions&#34;&gt;A look at distributions&lt;/h2&gt;

&lt;p&gt;As we&amp;rsquo;re predicting the &amp;lsquo;SalePrice&amp;rsquo;, so we should have a look at the stats of
 this col.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train[&#39;SalePrice&#39;].describe()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;count      1460.000000
mean     180921.195890
std       79442.502883
min       34900.000000
25%      129975.000000
50%      163000.000000
75%      214000.000000
max      755000.000000
Name: SalePrice, dtype: float64
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train[&#39;SalePrice&#39;].hist(bins=30)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x11414e4a8&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_12_1.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The values of &amp;lsquo;SalePrice&amp;rsquo; does fall in a normal distribution. In general, learning algorithms benefit from standardization of the data set. So we&amp;rsquo;ll transform the target values by &lt;code&gt;QuantileTransformer&lt;/code&gt; and &lt;code&gt;TransformedTargetRegressor&lt;/code&gt; later when training and testing.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s have a look at other columns&amp;rsquo; skewnesses.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.stats import skew

# Concat training and testing sets together to see the full picture
df_all = pd.concat((df_train, df_test)).reset_index(
    drop=True).drop([&#39;SalePrice&#39;], axis=&#39;columns&#39;)

numeric_cols = df_all.select_dtypes(
    exclude=[&#39;object&#39;, &#39;category&#39;]).columns

# Check the skewness of the numerical cols
skewed_cols = df_all[numeric_cols].apply(
    lambda col: skew(col)).sort_values(ascending=False)

skewness = pd.DataFrame({&#39;Skewness&#39;: skewed_cols})
skewness.head(10)

skewness = skewness[abs(skewness[&#39;Skewness&#39;]) &amp;gt; 0.75]
print(f&#39;{skewness.shape[0]} skewed numerical columns.&#39;)

df_all[skewness.index].hist(figsize=(14, 12))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;/Users/pcx/.pyenv/versions/ml/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.

  &amp;quot;&amp;quot;&amp;quot;
15 skewed numerical columns.


array([[&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1209d3550&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1041d86d8&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x104200c50&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x104233208&amp;gt;],
       [&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x120b97780&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x120bc1cf8&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x120bef2b0&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x120c17860&amp;gt;],
       [&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x120c17898&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x120c71358&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x120f2a8d0&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x120f53e48&amp;gt;],
       [&amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x120f84400&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x120fac978&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x120fd3ef0&amp;gt;,
        &amp;lt;matplotlib.axes._subplots.AxesSubplot object at 0x1210054a8&amp;gt;]],
      dtype=object)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_14_2.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We also need to handle the skewed variables later.&lt;/p&gt;

&lt;h1 id=&#34;preprocessing-data&#34;&gt;Preprocessing data&lt;/h1&gt;

&lt;h2 id=&#34;impute-missing-values&#34;&gt;Impute missing values&lt;/h2&gt;

&lt;p&gt;There are quite a lot of missing values, some cols are missing almost all of
 the data. Now look into the data description to see what the variables really
 are and how should we deal with them.
 We&amp;rsquo;re now concating the training set and testing set since we need to handle
 the missing values in both data sets. We will split them when we need.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# keep Id col for later unpack training and testing df
ids_train = df_train[&#39;Id&#39;]
ids_test = df_test[&#39;Id&#39;]
Y_train = df_train[&#39;SalePrice&#39;].values
df_all = pd.concat((df_train, df_test)).reset_index(
    drop=True).drop([&#39;SalePrice&#39;], axis=&#39;columns&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;/Users/pcx/.pyenv/versions/ml/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass &#39;sort=False&#39;.

To retain the current behavior and silence the warning, pass &#39;sort=True&#39;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;lsquo;PoolQC&amp;rsquo; (Pool quality) is the one with most missing values, and NA stands for &amp;ldquo;No Pool&amp;rdquo; (described in data_description.txt), so the missing values should be replaced by str &amp;ldquo;No Pool&amp;rdquo;. And this col should be an ordered categorical variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_all[&#39;PoolQC&#39;] = df_all[&#39;PoolQC&#39;].fillna(&amp;quot;No Pool&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The same applies to &amp;lsquo;MiscFeature&amp;rsquo;, &amp;lsquo;Alley&amp;rsquo;, &amp;lsquo;Fence&amp;rsquo;, &amp;lsquo;FireplaceQu&amp;rsquo;,
 &amp;lsquo;GarageType&amp;rsquo;, &amp;lsquo;GarageFinish&amp;rsquo;, &amp;lsquo;GarageQual&amp;rsquo;, &amp;lsquo;GarageCond&amp;rsquo;, &amp;lsquo;BsmtQual&amp;rsquo;,
 &amp;lsquo;BsmtCond&amp;rsquo;, &amp;lsquo;BsmtExposure&amp;rsquo;, &amp;lsquo;BsmtFinType1&amp;rsquo;, &amp;lsquo;BsmtFinType2&amp;rsquo;, &amp;lsquo;MasVnrType&amp;rsquo;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_all[&#39;MiscFeature&#39;] = df_all[&#39;MiscFeature&#39;].fillna(&amp;quot;None&amp;quot;)
df_all[&#39;Alley&#39;] = df_all[&#39;Alley&#39;].fillna(&amp;quot;No Alley access&amp;quot;)
df_all[&#39;Fence&#39;] = df_all[&#39;Fence&#39;].fillna(&amp;quot;No Fence&amp;quot;)
df_all[&#39;FireplaceQu&#39;] = df_all[&#39;FireplaceQu&#39;].fillna(&amp;quot;No Fireplace&amp;quot;)
df_all[&#39;GarageType&#39;] = df_all[&#39;GarageType&#39;].fillna(&amp;quot;No Garage&amp;quot;)
df_all[&#39;GarageFinish&#39;] = df_all[&#39;GarageFinish&#39;].fillna(&amp;quot;No Garage&amp;quot;)
df_all[&#39;GarageQual&#39;] = df_all[&#39;GarageQual&#39;].fillna(&amp;quot;No Garage&amp;quot;)
df_all[&#39;GarageCond&#39;] = df_all[&#39;GarageCond&#39;].fillna(&amp;quot;No Garage&amp;quot;)
df_all[&#39;BsmtCond&#39;] = df_all[&#39;BsmtCond&#39;].fillna(&amp;quot;No Basement&amp;quot;)
df_all[&#39;BsmtQual&#39;] = df_all[&#39;BsmtQual&#39;].fillna(&amp;quot;No Basement&amp;quot;)
df_all[&#39;BsmtExposure&#39;] = df_all[&#39;BsmtExposure&#39;].fillna(&amp;quot;No Basement&amp;quot;)
df_all[&#39;BsmtFinType1&#39;] = df_all[&#39;BsmtFinType1&#39;].fillna(&amp;quot;No Basement&amp;quot;)
df_all[&#39;BsmtFinType2&#39;] = df_all[&#39;BsmtFinType2&#39;].fillna(&amp;quot;No Basement&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s check &amp;lsquo;GarageYrBlt&amp;rsquo;, &amp;lsquo;GarageArea&amp;rsquo;, &amp;lsquo;GarageCars&amp;rsquo;.
 Since only 1 record of &amp;lsquo;GarageCars&amp;rsquo; is missing, and it&amp;rsquo;s &amp;lsquo;GarageType&amp;rsquo; is
 &amp;lsquo;Detchd&amp;rsquo;, so let&amp;rsquo;s make it as size of the mode/median of &amp;lsquo;GarageCars&amp;rsquo; when
 type is &amp;lsquo;Detchd&amp;rsquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_all[df_all[&#39;GarageCars&#39;].isnull()]
df_all[df_all[&#39;GarageCars&#39;].isnull()][&#39;GarageType&#39;]
df_all[&#39;GarageCars&#39;] = df_all[&#39;GarageCars&#39;].fillna(
    int(df_all[df_all[&#39;GarageType&#39;] == &#39;Detchd&#39;][&#39;GarageCars&#39;].mode()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s the same record for the missing &amp;lsquo;GarageArea&amp;rsquo; value, as we filled its
 &amp;lsquo;GarageCars&amp;rsquo; to the mode value, we will fill the area as the mean value of
 &amp;lsquo;GarageArea&amp;rsquo; where the &amp;lsquo;GarageCars&amp;rsquo; == mode value of &amp;lsquo;Detchd&amp;rsquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_all[df_all[&#39;GarageArea&#39;].isnull()]
df_all[&#39;GarageArea&#39;] = df_all[&#39;GarageArea&#39;].fillna(
    df_all[df_all[&#39;GarageType&#39;] == &#39;Detchd&#39;][&#39;GarageArea&#39;].mean())

# df_all[df_all[&#39;GarageYrBlt&#39;].isnull()][&#39;GarageType&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the records that have no garage, we set the null value of &amp;lsquo;GarageYrBlt&amp;rsquo;
 to 0, but for the records with type &amp;lsquo;Detchd&amp;rsquo;, we set the null value to the median
 value of the built year with type &amp;lsquo;Detchd&amp;rsquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;year_median = df_all[df_all[&#39;GarageType&#39;] == &#39;Detchd&#39;][&#39;GarageYrBlt&#39;].median()
df_all[&#39;GarageYrBlt&#39;] = df_all[&#39;GarageYrBlt&#39;][
    df_all[&#39;GarageType&#39;] == &#39;Detchd&#39;].fillna(year_median)

df_all[&#39;GarageYrBlt&#39;] = df_all[&#39;GarageYrBlt&#39;].fillna(0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since there are quite many missing value for &amp;lsquo;LotFrontage&amp;rsquo; (16.65%), we would drop this col.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_all = df_all.drop(&#39;LotFrontage&#39;, axis=&#39;columns&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Filling with 0 for those likely to be 0.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bsmt_zero_missing = [&#39;BsmtFinSF1&#39;, &#39;BsmtFinSF2&#39;,
                     &#39;BsmtUnfSF&#39;, &#39;TotalBsmtSF&#39;, &#39;BsmtFullBath&#39;, &#39;BsmtHalfBath&#39;]

for col in bsmt_zero_missing:
    df_all[col] = df_all[col].fillna(0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;lsquo;MasVnrArea&amp;rsquo; and &amp;lsquo;MasVnrType&amp;rsquo;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_all[df_all[&#39;MasVnrType&#39;].isnull()][&#39;MasVnrArea&#39;]
df_all[&#39;MasVnrType&#39;].astype(&#39;category&#39;).value_counts()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;None       1742
BrkFace     879
Stone       249
BrkCmn       25
Name: MasVnrType, dtype: int64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For all the records with missing values of &amp;lsquo;MasVnrType&amp;rsquo;, 1 record with
 &amp;lsquo;MasVnrArea&amp;rsquo; is not NaN, so we filling its type as &amp;lsquo;BrkFace&amp;rsquo;, which is the
 most occurred none-None type. Other missing values of &amp;lsquo;MasVnrType&amp;rsquo; we will
 fill in with the most common &lt;code&gt;None&lt;/code&gt;, so its &amp;lsquo;MasVnrArea&amp;rsquo; will be 0.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_all[&#39;MasVnrType&#39;] = df_all[&#39;MasVnrType&#39;][
    df_all[&#39;MasVnrArea&#39;].notna()].fillna(&#39;BrkFace&#39;)
df_all[&#39;MasVnrType&#39;] = df_all[&#39;MasVnrType&#39;].fillna(&#39;None&#39;)
df_all[&#39;MasVnrArea&#39;] = df_all[&#39;MasVnrArea&#39;].fillna(0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set the NaN to the mostly occurred value &amp;lsquo;RL&amp;rsquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_all[&#39;MSZoning&#39;].astype(&#39;category&#39;).value_counts()
df_all[&#39;MSZoning&#39;] = df_all[&#39;MSZoning&#39;].fillna(&#39;RL&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Set the NaN to the mostly occurred value &#39;AllPub&#39;.
df_all[&#39;Utilities&#39;].astype(&#39;category&#39;).value_counts()
df_all[&#39;Utilities&#39;] = df_all[&#39;Utilities&#39;].fillna(&#39;AllPub&#39;)

# keep or not?
df_all = df_all.drop([&#39;Utilities&#39;], axis=&#39;columns&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set NaN to mostly occurred value for the rest cols.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cols_nan_mode = [&#39;Functional&#39;, &#39;Electrical&#39;, &#39;KitchenQual&#39;,
                 &#39;Exterior1st&#39;, &#39;Exterior2nd&#39;, &#39;SaleType&#39;, &#39;MSSubClass&#39;]

for col in cols_nan_mode:
    df_all[col] = df_all[col].fillna(df_all[col].mode()[0])

cols_missing_value(df_all)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Missing Ratio %&lt;/th&gt;
      &lt;th&gt;Total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now there&amp;rsquo;s no missing values. Let&amp;rsquo;s move to the next part.&lt;/p&gt;

&lt;h2 id=&#34;transform-categorical-variables&#34;&gt;Transform categorical variables&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ll firstly transform some of the variables from numerical to categorical as
 they should be. And add one variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cols_num_cat = [&#39;MSSubClass&#39;, &#39;YrSold&#39;, &#39;MoSold&#39;]
for col in cols_num_cat:
    df_all[col] = df_all[col].astype(&#39;category&#39;)

# Adding total sqfootage feature
df_all[&#39;TotalSF&#39;] = df_all[&#39;TotalBsmtSF&#39;] + \
    df_all[&#39;1stFlrSF&#39;] + df_all[&#39;2ndFlrSF&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;check-and-handle-outliers&#34;&gt;Check and handle outliers&lt;/h2&gt;

&lt;p&gt;After handling the missing values, now we have a look at if there are outliers
 in the training set with the target variable by scatter plots.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

df_train = df_all[:len(ids_train)]
df_test = df_all[len(ids_train):]

cols = df_train.select_dtypes([&#39;int64&#39;, &#39;float64&#39;])
# cols = df_train.select_dtypes([&#39;int64&#39;, &#39;float64&#39;])
df_train = pd.concat([df_train, pd.DataFrame(
    Y_train, columns=[&#39;SalePrice&#39;])], axis=&#39;columns&#39;)

fig, axes = plt.subplots(6, 6, figsize=(30, 30))
for i, col in enumerate(cols):
    df_train.plot.scatter(x=col, y=&#39;SalePrice&#39;, ax=axes[i // 6, i % 6])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_44_0.svg&#34; alt=&#34;svg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The continuous variable &amp;lsquo;GrLivArea&amp;rsquo; seems having 2 values have very
 different &amp;ldquo;hehavior&amp;rdquo;. The 2 bottom right dots may be very inferential that
 have quite big areas but low prices. Let&amp;rsquo;s remove them to see if it&amp;rsquo;s better
 for the results. After removing these 2 rows, we would see that outliers in
 other cols such &amp;lsquo;TotalBsmtSF&amp;rsquo; and &amp;lsquo;TotalSF&amp;rsquo; are disappeared as well.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_train = df_train.drop(df_train[(df_train[&#39;GrLivArea&#39;] &amp;gt; 4000) &amp;amp;
                                  (df_train[&#39;SalePrice&#39;] &amp;lt; 250000)].index)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Packing back data sets after removing outliers in training set.
ids_train = df_train[&#39;Id&#39;]
ids_test = df_test[&#39;Id&#39;]
Y_train = df_train[&#39;SalePrice&#39;].values
df_all = pd.concat((df_train, df_test)).reset_index(
    drop=True).drop([&#39;SalePrice&#39;], axis=&#39;columns&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;transform-skewed-variables&#34;&gt;Transform skewed variables&lt;/h2&gt;

&lt;p&gt;We will transform the skewed variables into normal distributions by
 &lt;code&gt;quantile_transform&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;numeric_cols = df_all.select_dtypes(
    exclude=[&#39;object&#39;, &#39;category&#39;]).columns

# Check the skewnesses of the numerical cols
skewed_cols = df_all[numeric_cols].apply(
    lambda col: skew(col)).sort_values(ascending=False)

skewness = pd.DataFrame({&#39;Skewness&#39;: skewed_cols})

skewness = skewness[abs(skewness[&#39;Skewness&#39;]) &amp;gt; 0.75]
print(f&#39;{skewness.shape[0]} skewed numerical columns.&#39;)

from sklearn.preprocessing import quantile_transform
import numpy as np

skewed_features = skewness.index
df_all[skewed_features] = quantile_transform(
    df_all[skewed_features], output_distribution=&#39;normal&#39;, copy=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;20 skewed numerical columns.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Check again for the skewnesses of the numerical cols
skewed_cols = df_all[numeric_cols].apply(
    lambda col: skew(col)).sort_values(ascending=False)

skewness = pd.DataFrame({&#39;Skewness&#39;: skewed_cols})

skewness = skewness[abs(skewness[&#39;Skewness&#39;]) &amp;gt; 0.75]
print(f&#39;{skewness.shape[0]} skewed numerical columns.&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;11 skewed numerical columns.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;encode-categorical-valuee&#34;&gt;Encode categorical valuee&lt;/h2&gt;

&lt;p&gt;Transform categorical cols by using &lt;code&gt;pd.get_dummies()&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(df_all.shape)
# Column names in the DataFrame to be encoded. If columns is None then all the
# columns with object or category dtype will be converted.
df_all = pd.get_dummies(df_all)
print(df_all.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(2917, 79)
(2917, 330)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;training-and-testing&#34;&gt;Training and testing&lt;/h1&gt;

&lt;h2 id=&#34;base-model&#34;&gt;Base model&lt;/h2&gt;

&lt;p&gt;Now we will start to train and test with a base model with default parameters
 to see how it would perform as a base line.
 Root-Mean-Squared-Error (RMSE) as the evaluation metric for the competition, the equation is:&lt;/p&gt;

&lt;p&gt;$$\operatorname{RMSE}(y, \hat{y})=\sqrt{\frac{1}{n_{\text {samples }}} \sum_{i=0}^{n_{\text {symples }}-1}\left(y_{i}-\hat{y}_{i}\right)^{2}}$$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Unpack training and testing data sets
df_train = df_all[:len(ids_train)].drop([&#39;Id&#39;], axis=&#39;columns&#39;)
df_test = df_all[len(ids_train):].drop([&#39;Id&#39;], axis=&#39;columns&#39;)

X_train = df_train.values
X_test = df_test
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Lasso, ElasticNet, Ridge
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, make_scorer
from sklearn.compose import TransformedTargetRegressor
from sklearn.preprocessing import QuantileTransformer

Y_train_norm = np.log1p(Y_train)

# there&#39;s no implementation of RMSE in the scikit-learn library, so we have to
# define a scorer of RMSE
def rmse_cal(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))
    # return np.sqrt(np.sum(np.square(y_pred - y_true)) / len(y_pred))


# if the custom score function is a loss (greater_is_better=False), the output
# of the python function is negated by the scorer object, conforming to the
# cross validation convention that scorers return higher values for better
# models.
rmse = make_scorer(rmse_cal, greater_is_better=False)

# ridgepip = Pipeline([
    # (&#39;tran&#39;, TransformedTargetRegressor(
    #     regressor=Lasso(), func=np.log1p, inverse_func=np.expm1)),
    # (&#39;tran&#39;, TransformedTargetRegressor(
    #     regressor=Ridge(), func=np.log1p, inverse_func=np.expm1)),
# ])

models = [
    Lasso(),
    # ridgepip,
    # # ElasticNet(),
    Ridge(),
]

CV = 5
for m in models:
    scores = -cross_val_score(m, X_train, Y_train_norm,
                              scoring=rmse, cv=5, n_jobs=-1)
    print(f&#39;{type(m).__name__}\n&#39;
          f&#39;Scores: {scores}\n&#39;
          # +/-std*2 for 95% confidence interval
          f&#39;Accuracy: {scores.mean(): 0.4f} (+/-{scores.std() * 2: 0.4f})\n&#39;
          f&#39;{&amp;quot;-&amp;quot;*20}&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Lasso
Scores: [0.22425222 0.23934427 0.23998284 0.24165163 0.23227816]
Accuracy:  0.2355 (+/- 0.0129)
--------------------
Ridge
Scores: [0.11456344 0.12197379 0.13560006 0.1083432  0.1172416 ]
Accuracy:  0.1195 (+/- 0.0183)
--------------------
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;gridsearch-for-best-model-with-best-parameters&#34;&gt;GridSearch for best model with best parameters&lt;/h2&gt;

&lt;p&gt;The base models give somehow good results. The CV RMSE score of the /Ridge/
 model is around the top-1000 in the competition&amp;rsquo;s leaderboard. Now let&amp;rsquo;s try
 to find the best parameters for these and other models with &lt;code&gt;GridSearchCV&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.svm import SVR
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn import metrics


Y_train_norm = np.log1p(Y_train)

X_train_cv, X_test_cv, Y_train_cv, Y_test_cv = train_test_split(
    X_train, Y_train_norm, test_size=0.3)


param_space = {
    &#39;rob_lasso&#39;: {
        &#39;model&#39;: Pipeline([(&#39;sca&#39;, RobustScaler()), (&#39;model&#39;, Lasso())]),
        &#39;params&#39;: {
            &#39;model__alpha&#39;: [0.00005, 0.0004, 0.0005, 0.0007, 0.005, 0.05, 0.5, 0.8, 1],
        }
    },
    &#39;ridge&#39;: {
        &#39;model&#39;: Ridge(),
        &#39;params&#39;: {
            &#39;alpha&#39;: [1e-3, 1e-2, 1e-1, 1, 10],
        }
    },
    &#39;kernel_ridge&#39;: {
        &#39;model&#39;: KernelRidge(),
        &#39;params&#39;: {
            &#39;alpha&#39;: [1e-3, 1e-2, 1e-1, 1, 10],
        }
    },
    &#39;elastic_net&#39;: {
        &#39;model&#39;: Pipeline([(&#39;sca&#39;, RobustScaler()), (&#39;model&#39;, ElasticNet())]),
        &#39;params&#39;: {
            &#39;model__alpha&#39;: [0.00005, 0.0004, 0.0005, 0.0007, 0.005, 0.05, 0.5, 0.8, 1],
            # Note that a good choice of list of values for l1_ratio is often to
            # put more values close to 1 (i.e. Lasso) and less close to 0 (i.e.
            # Ridge)
            &#39;model__l1_ratio&#39;: [.1, .5, .7, .75, .8, .85, .9, .95, .97, .99, .995, 1],
        }
    },
    # &#39;gboost&#39;: {
    #     &#39;model&#39;: GradientBoostingRegressor(),
    #     &#39;params&#39;: {
    #         &#39;loss&#39;: [&#39;ls&#39;, &#39;lad&#39;, &#39;huber&#39;, &#39;quantile&#39;],
    #         &#39;learning_rate&#39;: [0.01, 0.1],
    #         &#39;n_estimators&#39;: [100, 500, 1000, 3000],
    #         &#39;max_depth&#39;: [2, 3, 4],
    #         &#39;min_samples_split&#39;: [2, 5, 10],
    #     }
    # },
    # &#39;svr&#39;: {
    #     &#39;model&#39;: SVR(),
    #     &#39;params&#39;: {
    #         &#39;kernel&#39;: [&#39;linear&#39;, &#39;rbf&#39;],
    #         &#39;C&#39;: [1, 10],
    #     }
    # },
}

gs_rec = []

# grid search parameters
for name, pair in param_space.items():
    print(f&#39;{name}---------------&#39;)
    gs_rg = GridSearchCV(pair[&#39;model&#39;], pair[&#39;params&#39;],
                         scoring=rmse, cv=CV, error_score=0, n_jobs=-1)
    gs_rg.fit(X_train, Y_train_norm)
    print(gs_rg.best_params_)
    print(gs_rg.best_score_)

    gs_rg_cv = GridSearchCV(pair[&#39;model&#39;], pair[&#39;params&#39;],
                            scoring=rmse, cv=CV, error_score=0, n_jobs=-1)
    gs_rg_cv.fit(X_train_cv, Y_train_cv)
    pred_test = gs_rg_cv.predict(X_test_cv)
    y_score = rmse_cal(Y_test_cv, pred_test)

    print(gs_rg_cv.best_params_)
    print(gs_rg_cv.best_score_)
    print(y_score)

    gs_rec.append({
        &#39;name&#39;: name,
        &#39;params&#39;: gs_rg.best_params_,
        &#39;score&#39;: -gs_rg.best_score_,
        &#39;cv_test_params&#39;: gs_rg_cv.best_params_,
        &#39;cv_test_score&#39;: y_score
    })

df_gs = pd.DataFrame(gs_rec, columns=[&#39;name&#39;, &#39;score&#39;, &#39;params&#39;,
                                      &#39;cv_test_score&#39;, &#39;cv_test_params&#39;]
                     ).sort_values(by=[&#39;score&#39;, &#39;cv_test_score&#39;])
df_gs
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;rob_lasso---------------
{&#39;model__alpha&#39;: 0.0005}
-0.1108321642082426
{&#39;model__alpha&#39;: 0.0005}
-0.11385591248537665
0.1092651116732159
ridge---------------
{&#39;alpha&#39;: 10}
-0.11417733254437629
{&#39;alpha&#39;: 10}
-0.11723423641202352
0.11022009984391984
kernel_ridge---------------
{&#39;alpha&#39;: 10}
-0.11675117173959225
{&#39;alpha&#39;: 10}
-0.1209044169077714
0.11171230919473786
elastic_net---------------
{&#39;model__alpha&#39;: 0.0005, &#39;model__l1_ratio&#39;: 0.9}
-0.11081242246612653
{&#39;model__alpha&#39;: 0.0007, &#39;model__l1_ratio&#39;: 0.8}
-0.1138195082928615
0.10934894252124043
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;score&lt;/th&gt;
      &lt;th&gt;params&lt;/th&gt;
      &lt;th&gt;cv_test_score&lt;/th&gt;
      &lt;th&gt;cv_test_params&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;elastic_net&lt;/td&gt;
      &lt;td&gt;0.110812&lt;/td&gt;
      &lt;td&gt;{&#39;model__alpha&#39;: 0.0005, &#39;model__l1_ratio&#39;: 0.9}&lt;/td&gt;
      &lt;td&gt;0.109349&lt;/td&gt;
      &lt;td&gt;{&#39;model__alpha&#39;: 0.0007, &#39;model__l1_ratio&#39;: 0.8}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;rob_lasso&lt;/td&gt;
      &lt;td&gt;0.110832&lt;/td&gt;
      &lt;td&gt;{&#39;model__alpha&#39;: 0.0005}&lt;/td&gt;
      &lt;td&gt;0.109265&lt;/td&gt;
      &lt;td&gt;{&#39;model__alpha&#39;: 0.0005}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;ridge&lt;/td&gt;
      &lt;td&gt;0.114177&lt;/td&gt;
      &lt;td&gt;{&#39;alpha&#39;: 10}&lt;/td&gt;
      &lt;td&gt;0.110220&lt;/td&gt;
      &lt;td&gt;{&#39;alpha&#39;: 10}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;kernel_ridge&lt;/td&gt;
      &lt;td&gt;0.116751&lt;/td&gt;
      &lt;td&gt;{&#39;alpha&#39;: 10}&lt;/td&gt;
      &lt;td&gt;0.111712&lt;/td&gt;
      &lt;td&gt;{&#39;alpha&#39;: 10}&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Now let&amp;rsquo;s Train with the best model so far and predict on the test data. As
 aforementioned, the values of &amp;lsquo;SalePrice&amp;rsquo; does fall in a normal distribution.
 So we&amp;rsquo;ll transform the target values by &lt;code&gt;QuantileTransformer&lt;/code&gt; and
 &lt;code&gt;TransformedTargetRegressor&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datetime import datetime


# model = Pipeline(
#     [(&#39;sca&#39;, RobustScaler()), (&#39;model&#39;, TransformedTargetRegressor(
#         regressor=ElasticNet(alpha=0.0005, l1_ratio=0.85), func=np.log1p, inverse_func=np.expm1))])
model = Pipeline(
    [(&#39;sca&#39;, RobustScaler()), (&#39;model&#39;, TransformedTargetRegressor(
        regressor=ElasticNet(alpha=0.0005, l1_ratio=0.85),
        # regressor=Lasso(alpha=0.0005),
        transformer=QuantileTransformer(output_distribution=&#39;normal&#39;)))])

model.fit(X_train, Y_train)

pred = model.predict(X_test)


def submit(ids, pred, suffix):
    sub = pd.DataFrame()
    sub[&#39;Id&#39;] = ids_test
    sub[&#39;SalePrice&#39;] = pred
    timestamp = datetime.now().strftime(&#39;%Y-%m-%d_%H-%M-%S&#39;)
    # sub.to_csv(
    # f&#39;result/kaggle1_sub_{suffix}_{score:.5f}.csv&#39;, index=False)
    sub.to_csv(
        f&#39;submissions/{suffix}_{timestamp}.csv.gz&#39;, index=False,
        compression=&#39;gzip&#39;)


submit(ids_test, pred, &#39;elastic_net&#39;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Meaningful Integration of Data from Heterogeneous Health Services and Home Environment Based on Ontology</title>
      <link>https://pcx.linkedinfo.co/publication/pcx-2019-a/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0200</pubDate>
      <guid>https://pcx.linkedinfo.co/publication/pcx-2019-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LinkedInfo.co</title>
      <link>https://pcx.linkedinfo.co/project/linkedinfo/</link>
      <pubDate>Sun, 27 Jan 2019 11:17:54 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/project/linkedinfo/</guid>
      <description>&lt;p&gt;The Web should be an open web. All the informations published on the Web are meant to be shared, share through links by search engines, rss, social networks, etc. This site is yet another method that tries to link all the informations (but starts with only technical articles on LinkedInfo) and share them.&lt;/p&gt;

&lt;p&gt;The original idea of this side project is to utilize Semantic Web technologies and Machine learning to link the informations. Noble ambition shall start from basic, it needs to be improved little by little.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What Can Teachers Do to Make the Group Work Learning Effective - a Literature Review</title>
      <link>https://pcx.linkedinfo.co/publication/peng-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/publication/peng-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Ontological Approach to Integrate Health Resources from Different Categories of Services</title>
      <link>https://pcx.linkedinfo.co/publication/pcx-2018-d/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0200</pubDate>
      <guid>https://pcx.linkedinfo.co/publication/pcx-2018-d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Web Notes</title>
      <link>https://pcx.linkedinfo.co/post/web-notes/</link>
      <pubDate>Wed, 10 Jan 2018 05:19:29 +0200</pubDate>
      <guid>https://pcx.linkedinfo.co/post/web-notes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fuzzy Matching of OpenAPI Described REST Services</title>
      <link>https://pcx.linkedinfo.co/publication/pcx-2018-c/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/publication/pcx-2018-c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Linking Health Web Services as Resource Graph by Semantic REST Resource Tagging</title>
      <link>https://pcx.linkedinfo.co/publication/pcx-2018-b/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/publication/pcx-2018-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using Tag based Semantic Annotation to Empower Client and REST Service Interaction</title>
      <link>https://pcx.linkedinfo.co/publication/pcx-2018-a/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/publication/pcx-2018-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Good Record Keeping for Conducting Research Ethically Correct</title>
      <link>https://pcx.linkedinfo.co/publication/peng-2017/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/publication/peng-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Flexible System Architecture of PHR to Support Sharing Health Data for Chronic Disease Self-Management</title>
      <link>https://pcx.linkedinfo.co/publication/pcx-2016/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/publication/pcx-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sharing health data through hybrid cloud for self-management</title>
      <link>https://pcx.linkedinfo.co/publication/huyanpcx-2015/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/publication/huyanpcx-2015/</guid>
      <description></description>
    </item>
    
    <item>
      <title>OpenStack Swift Brief Intro</title>
      <link>https://pcx.linkedinfo.co/post/swift-brief-intro/</link>
      <pubDate>Tue, 01 Jul 2014 00:00:00 +0000</pubDate>
      <guid>https://pcx.linkedinfo.co/post/swift-brief-intro/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
