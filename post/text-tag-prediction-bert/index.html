<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="PENG, Cong">

  
  
  
    
  
  <meta name="description" content="Introduction This is a follow up post of Multi-label classification to predict topic tags of technical articles from LinkedInfo.co. We will continute the same task by using BERT.
Firstly we&rsquo;ll just use the embeddings from BERT, and then feed them to the same classification model used in the last post, SVM with linear kenel. The reason of keep using SVM is that the size of the dataset is quite small.">

  
  <link rel="alternate" hreflang="en-us" href="https://pcx.linkedinfo.co/post/text-tag-prediction-bert/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.5ac49d52d669340d29cb50b00bb7867f.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-87823019-3', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="https://www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://pcx.linkedinfo.co/post/text-tag-prediction-bert/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Cong Peng">
  <meta property="og:url" content="https://pcx.linkedinfo.co/post/text-tag-prediction-bert/">
  <meta property="og:title" content="Using BERT to perform Topic Tag Prediction of Technical Articles | Cong Peng">
  <meta property="og:description" content="Introduction This is a follow up post of Multi-label classification to predict topic tags of technical articles from LinkedInfo.co. We will continute the same task by using BERT.
Firstly we&rsquo;ll just use the embeddings from BERT, and then feed them to the same classification model used in the last post, SVM with linear kenel. The reason of keep using SVM is that the size of the dataset is quite small."><meta property="og:image" content="https://pcx.linkedinfo.co/img/icon-192.png">
  <meta property="twitter:image" content="https://pcx.linkedinfo.co/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-02-13T11:20:19&#43;02:00">
    
    <meta property="article:modified_time" content="2020-03-30T15:10:14&#43;02:00">
  

  


    






  





  





  





<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://pcx.linkedinfo.co/post/text-tag-prediction-bert/"
  },
  "headline": "Using BERT to perform Topic Tag Prediction of Technical Articles",
  
  "datePublished": "2020-02-13T11:20:19+02:00",
  "dateModified": "2020-03-30T15:10:14+02:00",
  
  "author": {
    "@type": "Person",
    "name": "PENG, Cong"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Cong Peng",
    "logo": {
      "@type": "ImageObject",
      "url": "https://pcx.linkedinfo.co/img/icon-512.png"
    }
  },
  "description": "Introduction This is a follow up post of Multi-label classification to predict topic tags of technical articles from LinkedInfo.co. We will continute the same task by using BERT.\nFirstly we\u0026rsquo;ll just use the embeddings from BERT, and then feed them to the same classification model used in the last post, SVM with linear kenel. The reason of keep using SVM is that the size of the dataset is quite small."
}
</script>

  

  


  


  





  <title>Using BERT to perform Topic Tag Prediction of Technical Articles | Cong Peng</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Cong Peng</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Using BERT to perform Topic Tag Prediction of Technical Articles</h1>

  

  
    



<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Mar 30, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  
  

  
  

  
    

  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      

<h1 id="introduction">Introduction</h1>

<p>This is a follow up post of <a href="https://pcx.linkedinfo.co/post/text-tag-prediction/" target="_blank">Multi-label classification to predict topic tags of technical articles from LinkedInfo.co</a>. We will continute the same task by using BERT.</p>

<p>Firstly we&rsquo;ll just use the embeddings from BERT, and then feed them to the same classification model used in the last post, SVM with linear kenel. The reason of keep using SVM is that the size of the dataset is quite small.</p>

<h1 id="experiments">Experiments</h1>

<h2 id="classify-by-using-bert-mini-and-svm-with-linear-kernel">Classify by using BERT-Mini and SVM with Linear Kernel</h2>

<p>Due to the limited computation capacity, we&rsquo;ll use a smaller BERT model - BERT-Mini. The first experiment we&rsquo;ll try to train on only the titles of the articles.</p>

<p>Now we firstly load the dataset. And then load the pretrained BERT tokenizer and model. Note that we only load the article samples that are in English since the BERT-Mini model here were pretrained in English.</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import torch
from transformers import DistilBertModel, DistilBertTokenizer, AutoTokenizer, AutoModel

import dataset
from mltb.model_utils import download_once_pretrained_transformers

</code></pre>

<pre><code>ds = dataset.ds_info_tags(from_batch_cache='fulltext',
                          aug_level=0, lan='en',
                          concate_title=True,
                          filter_tags_threshold=0, partial_len=3000)
</code></pre>

<pre><code>from transformers import  AutoTokenizer, AutoModel

PRETRAINED_BERT_WEIGHTS = download_once_pretrained_transformers(
    &quot;google/bert_uncased_L-4_H-256_A-4&quot;)
tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BERT_WEIGHTS)
model = AutoModel.from_pretrained(PRETRAINED_BERT_WEIGHTS)
</code></pre>

<p>Now we encode all the titles by the BERT-Mini model. We&rsquo;ll use only the 1st output vector from the model as it&rsquo;s used for classification task.</p>

<pre><code>import nltk


nltk.download('punkt')

col_text = 'title'
max_length = ds.data[col_text].apply(lambda x: len(nltk.word_tokenize(x))).max()

encoded = ds.data[col_text].apply(
    (lambda x: tokenizer.encode_plus(x, add_special_tokens=True,
                                     pad_to_max_length=True,
                                     return_attention_mask=True,
                                     max_length=max_length,
                                     return_tensors='pt')))

input_ids = torch.cat(tuple(encoded.apply(lambda x:x['input_ids'])))
attention_mask = torch.cat(tuple(encoded.apply(lambda x:x['attention_mask'])))

features = []
with torch.no_grad():
    last_hidden_states = model(input_ids, attention_mask=attention_mask)
    features = last_hidden_states[0][:, 0, :].numpy()
</code></pre>

<p>As the features are changed from Tf-idf transformed to BERT transformed, so we&rsquo;ll re-search for the hyper-parameters for the LinearSVC to use.</p>

<p>The scorer we used in grid search is f-0.5 score since we want to weight higher precision over recall.</p>

<pre><code>from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.svm import LinearSVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn import metrics


clf = OneVsRestClassifier(LinearSVC())

C_OPTIONS = [0.01, 0.1, 0.5, 1, 10]

parameters = {
    'estimator__penalty': ['l1', 'l2'],
    'estimator__dual': [True, False],
    'estimator__C': C_OPTIONS,
}

micro_f05_sco = metrics.make_scorer(
    metrics.fbeta_score, beta=0.5, average='micro')

gs_clf = GridSearchCV(clf, parameters,
                      scoring=micro_f05_sco,
                      cv=3, n_jobs=-1)

gs_clf.fit(train_features, train_labels)

Y_predicted = gs_clf.predict(test_features)

print(metrics.classification_report(test_labels, Y_predicted))

print(gs_clf.best_params_)
print(gs_clf.best_score_)

</code></pre>

<pre><code>              precision    recall  f1-score   support
   micro avg       0.70      0.36      0.47      1152
   macro avg       0.32      0.19      0.22      1152
weighted avg       0.58      0.36      0.43      1152
 samples avg       0.46      0.38      0.40      1152

{'estimator__C': 0.1, 'estimator__dual': True, 'estimator__penalty': 'l2'}
0.5195981110298558
</code></pre>

<p>Though it&rsquo;s not comparable, the result metrics are somehow similar to the Tf-idf one when we use only the English samples with their titles here. The micro averages are about the same, the macro averages are slightly better.</p>

<p>Now let&rsquo;s combine the titles and short descriptions to see if there&rsquo;s any improment.</p>

<pre><code>import nltk

nltk.download('punkt')

col_text = 'description'
max_length = ds.data[col_text].apply(lambda x: len(nltk.word_tokenize(x))).max()
encoded = ds.data[col_text].apply(
    (lambda x: tokenizer.encode_plus(x, add_special_tokens=True,
                                     pad_to_max_length=True,
                                     return_attention_mask=True,
                                     max_length=max_length,
                                     return_tensors='pt')))

input_ids = torch.cat(tuple(encoded.apply(lambda x:x['input_ids'])))
attention_mask = torch.cat(tuple(encoded.apply(lambda x:x['attention_mask'])))

features = []
with torch.no_grad():
    last_hidden_states = model(input_ids, attention_mask=attention_mask)
    features = last_hidden_states[0][:, 0, :].numpy()
</code></pre>

<pre><code>RAND_STATE = 20200122

train_features, test_features, train_labels, test_labels = train_test_split(
    features, ds.target, test_size=0.3, random_state=RAND_STATE)
</code></pre>

<pre><code>from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.svm import SVC, LinearSVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn import metrics


clf = OneVsRestClassifier(LinearSVC())

C_OPTIONS = [0.1, 1]

parameters = {
    'estimator__penalty': ['l2'],
    'estimator__dual': [True],
    'estimator__C': C_OPTIONS,
}

micro_f05_sco = metrics.make_scorer(
    metrics.fbeta_score, beta=0.5, average='micro')

gs_clf = GridSearchCV(clf, parameters,
                      scoring=micro_f05_sco,
                      cv=3, n_jobs=-1)

gs_clf.fit(train_features, train_labels)

# Predict the outcome on the testing set in a variable named y_predicted
Y_predicted = gs_clf.predict(test_features)

print(metrics.classification_report(test_labels, Y_predicted))

print(gs_clf.best_params_)
print(gs_clf.best_score_)

</code></pre>

<pre><code>              precision    recall  f1-score   support

   micro avg       0.68      0.29      0.41      1238
   macro avg       0.18      0.11      0.13      1238
weighted avg       0.52      0.29      0.36      1238
 samples avg       0.40      0.31      0.34      1238

{'estimator__C': 1, 'estimator__dual': True, 'estimator__penalty': 'l2'}
0.49852671465233817
</code></pre>

<p>There is no improvement, the macro averages even got a little bit worse. Let&rsquo;s try to explore further.</p>

<h2 id="iterative-stratified-multilabel-data-sampling">Iterative stratified multilabel data sampling</h2>

<p>It would be a good idea to perform stratified sampling for spliting training and test sets since there&rsquo;s a big imbalancement in the dataset for the labels. The problem is that the size of dataset is very small, which causes it that using normal stratified sampling method would fail since it&rsquo;s likely that some labels may not appear in both training and testing sets. That&rsquo;s why we have to use iterative stratified multilabel sampling. The explanation of this method can refer to <a href="http://scikit.ml/stratification.html" target="_blank">document of scikit-multilearn</a>.</p>

<p>In the code below we have wrapped the split method to make the process smoother.</p>

<pre><code>import nltk

from mltb.experiment import multilearn_iterative_train_test_split
from mltb.transformers import bert_tokenize, bert_transform


nltk.download('punkt')
COL_TEXT = 'description'

train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split(
    ds.data, ds.target, test_size=0.3, cols=ds.data.columns)

batch_size = 128
model_name = &quot;google/bert_uncased_L-4_H-256_A-4&quot;

train_features, test_features = bert_transform(
    train_features, test_features, COL_TEXT, model_name, batch_size)


from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.svm import SVC, LinearSVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn import metrics


clf = OneVsRestClassifier(LinearSVC())

C_OPTIONS = [0.1, 1]

parameters = {
    'estimator__penalty': ['l2'],
    'estimator__dual': [True],
    'estimator__C': C_OPTIONS,
}

micro_f05_sco = metrics.make_scorer(
    metrics.fbeta_score, beta=0.5, average='micro')

gs_clf = GridSearchCV(clf, parameters,
                      scoring=micro_f05_sco,
                      cv=3, n_jobs=-1)

gs_clf.fit(train_features, train_labels)

# Predict the outcome on the testing set in a variable named y_predicted
Y_predicted = gs_clf.predict(test_features)

print(metrics.classification_report(test_labels, Y_predicted))

print(gs_clf.best_params_)
print(gs_clf.best_score_)
</code></pre>

<pre><code>              precision    recall  f1-score   support

   micro avg       0.69      0.36      0.47      1193
   macro avg       0.24      0.16      0.19      1193
weighted avg       0.57      0.36      0.43      1193
 samples avg       0.47      0.38      0.40      1193

{'estimator__C': 0.1, 'estimator__dual': True, 'estimator__penalty': 'l2'}
0.33116317929125705
</code></pre>

<p>There seems no improvement. But the cross validation F-0.5 score is lower than the testing score. It might be a sign that it&rsquo;s under-fitting.</p>

<h2 id="training-set-augmentation">Training set augmentation</h2>

<p>As the dataset is quite small, now we&rsquo;ll try to augment the trainig set to see if there&rsquo;s any improvement.</p>

<p>Here we set the augmentation level to 2, which means the dataset are concatenated by 2 times of the samples. And the added samples&rsquo; content will be randomly chopped out as <sup>9</sup>&frasl;<sub>10</sub> of its original content. Of course, both the actions only apply to the training set. The 30% test set is kept aside.</p>

<pre><code>COL_TEXT = 'description'

train_features, test_features, train_labels, test_labels = multilearn_iterative_train_test_split(
    ds.data, ds.target, test_size=0.3, cols=ds.data.columns)

train_features, train_labels = dataset.augmented_samples(
    train_features, train_labels, level=2, crop_ratio=0.1)

batch_size = 128
model_name = &quot;google/bert_uncased_L-4_H-256_A-4&quot;

train_features, test_features = bert_transform(
    train_features, test_features, COL_TEXT, model_name, batch_size)

clf = OneVsRestClassifier(LinearSVC())

C_OPTIONS = [0.1, 1]

parameters = {
    'estimator__penalty': ['l2'],
    'estimator__dual': [True],
    'estimator__C': C_OPTIONS,
}

micro_f05_sco = metrics.make_scorer(
    metrics.fbeta_score, beta=0.5, average='micro')

gs_clf = GridSearchCV(clf, parameters,
                      scoring=micro_f05_sco,
                      cv=3, n_jobs=-1)

gs_clf.fit(train_features, train_labels)

# Predict the outcome on the testing set in a variable named y_predicted
Y_predicted = gs_clf.predict(test_features)

print(metrics.classification_report(test_labels, Y_predicted))

print(gs_clf.best_params_)
print(gs_clf.best_score_)
</code></pre>

<pre><code>              precision    recall  f1-score   support

   micro avg       0.64      0.33      0.44      1186
   macro avg       0.22      0.15      0.17      1186
weighted avg       0.53      0.33      0.40      1186
 samples avg       0.44      0.36      0.38      1186

{'estimator__C': 0.1, 'estimator__dual': True, 'estimator__penalty': 'l2'}
0.6115829006361205
</code></pre>

<p>We can see that there&rsquo;s still no improvement. It seems that we should change direction.</p>

<h2 id="fine-tuning-bert-model">Fine-tuning BERT model</h2>

<p>The next step is to see if we can make some progress by fine-tuning the BERT-Mini model.</p>

<p>To be continuted&hellip;</p>

    </div>

    


    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/bert/">BERT</a>
  
  <a class="badge badge-light" href="/tags/machine-learning/">Machine Learning</a>
  
  <a class="badge badge-light" href="/tags/multi-label-classification/">Multi-label classification</a>
  
  <a class="badge badge-light" href="/tags/text-classification/">Text classification</a>
  
  <a class="badge badge-light" href="/tags/linkedinfo.co/">LinkedInfo.co</a>
  
</div>



    
      








  






  
  
  
    
  
  
  <div class="media author-card">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_huf79fbb756ad93d32a2794d68e8fd33ec_47015_250x250_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://pcx.linkedinfo.co/">PENG, Cong</a></h5>
      <h6 class="card-subtitle">Ph.D. Student</h6>
      
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
          <li>
            <a href="mailto:cong.peng@bth.se" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a href="https://scholar.google.com/citations?user=87g_0KgAAAAJ" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a href="https://www.researchgate.net/profile/Cong_Peng5" target="_blank" rel="noopener">
              <i class="fab fa-researchgate"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a href="https://github.com/ddxgz" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
      <div class="article-widget">
        <div class="hr-light"></div>
        <h3>Related</h3>
        <ul>
          
          <li><a href="/post/text-tag-prediction/">Multi-label classification to predict topic tags of technical articles from LinkedInfo.co</a></li>
          
          <li><a href="/project/topic-tag-predictor/">Topic Tag Predictor</a></li>
          
          <li><a href="/post/fraud-detection/">A Walk Through of the IEEE-CIS Fraud Detection Challenge</a></li>
          
          <li><a href="/project/skin-lesion-classifier/">Skin Lesion Classifier</a></li>
          
          <li><a href="/post/skin-lesion-cls/">Skin Lesion Image Classification with Deep Convolutional Neural Networks</a></li>
          
        </ul>
      </div>
      
    

    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.130521ecfc6f534c52c158217bbff718.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    ©2014–2019 Cong Peng &middot; 

    Powered by
    <a href="https://orgmode.org/" target="_blank" rel="noopener">Org-mode</a> and the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
