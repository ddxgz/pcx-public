<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Cong Peng</title>
    <link>https://pcx.linkedinfo.co/tags/machine-learning/</link>
      <atom:link href="https://pcx.linkedinfo.co/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©2014–2019 Cong Peng</copyright><lastBuildDate>Wed, 11 Sep 2019 13:36:43 +0200</lastBuildDate>
    <image>
      <url>https://pcx.linkedinfo.co/img/icon-192.png</url>
      <title>Machine Learning</title>
      <link>https://pcx.linkedinfo.co/tags/machine-learning/</link>
    </image>
    
    <item>
      <title>Multi-label classification to predict topic tags of technical articles from LinkedInfo.co</title>
      <link>https://pcx.linkedinfo.co/post/texttagspred/</link>
      <pubDate>Wed, 11 Sep 2019 13:36:43 +0200</pubDate>
      <guid>https://pcx.linkedinfo.co/post/texttagspred/</guid>
      <description>

&lt;!-- Go back to [pcx.linkedinfo.co](https://pcx.linkedinfo.co) --&gt;

&lt;p&gt;This code snippet is to predict topic tags based on the text of an article. Each article could have 1 or more tags (usually have at least 1 tag), and the tags are not mutually exclusive. So this is a multi-label classification problem. It&amp;rsquo;s different from multi-class classification, the classes in multi-class classification are mutually exclusive, i.e., each item belongs to 1 and  only 1 class.&lt;/p&gt;

&lt;p&gt;In this snippet, we will use &lt;code&gt;OneVsRestClassifier&lt;/code&gt; (the One-Vs-the-Rest) in scikit-learn to process the multi-label classification. The article data will be retrieved from &lt;a href=&#34;https://linkedinfo.co&#34; target=&#34;_blank&#34;&gt;LinkedInfo.co&lt;/a&gt; via Web API.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Preprocessing data and explore the method&lt;/li&gt;
&lt;li&gt;Search for best model include SVM and Neural Networks (Will be updated)&lt;/li&gt;
&lt;li&gt;Training and testing with the best parameters&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The methods in this snippet should give credits to &lt;a href=&#34;https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html&#34; target=&#34;_blank&#34;&gt;Working With Text Data - scikit-learn&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;preprocessing-data-and-explore-the-method&#34;&gt;Preprocessing data and explore the method&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;dataset.df_tags&lt;/code&gt; fetches the data set from &lt;a href=&#34;https://linkedinfo.co&#34; target=&#34;_blank&#34;&gt;LinkedInfo.co&lt;/a&gt;. It calls Web API of LinkedInfo.co to retrieve the article list, and then download and extract the full text of each article based on an article&amp;rsquo;s url. The tags of each article are encoded using &lt;code&gt;MultiLabelBinarizer&lt;/code&gt; in scikit-learn. The implementation of the code could be found in &lt;a href=&#34;https://github.com/ddxgz/linkedinfo-ml-models/blob/master/dataset.py&#34; target=&#34;_blank&#34;&gt;dataset.py&lt;/a&gt;. We&amp;rsquo;ve set the parameter of &lt;code&gt;content_length_threshold&lt;/code&gt; to 100 to screen out the articles with less than 100 for the description or full text.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import dataset

ds = dataset.df_tags(content_length_threshold=100)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The dataset contains 3353 articles by the time retrieved the data.
The dataset re returned as an object with the following attribute:&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;ds.data: pandas.DataFrame with cols of title, description, fulltext&lt;/li&gt;
&lt;li&gt;ds.target: encoding of tagsID&lt;/li&gt;
&lt;li&gt;ds.target_names: tagsID&lt;/li&gt;
&lt;li&gt;ds.target_decoded: the list of lists contains tagsID for each info&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;ds.data.head()
ds.target[:5]
ds.target_names[:5]
ds.target_decoded[:5]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following snippet is the actual process of getting the above dataset, by
reading from file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import json
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer

infos_file = &#39;data/infos/infos_0_3353_fulltext.json&#39;
with open(infos_file, &#39;r&#39;) as f:
    infos = json.load(f)

content_length_threshold = 100

data_lst = []
tags_lst = []
for info in infos[&#39;content&#39;]:
    if len(info[&#39;fulltext&#39;]) &amp;lt; content_length_threshold:
        continue
    if len(info[&#39;description&#39;]) &amp;lt; content_length_threshold:
        continue
    data_lst.append({&#39;title&#39;: info[&#39;title&#39;],
                     &#39;description&#39;: info[&#39;description&#39;],
                     &#39;fulltext&#39;: info[&#39;fulltext&#39;]})
    tags_lst.append([tag[&#39;tagID&#39;] for tag in info[&#39;tags&#39;]])

df_data = pd.DataFrame(data_lst)
df_tags = pd.DataFrame(tags_lst)

# fit and transform the binarizer
mlb = MultiLabelBinarizer()
Y = mlb.fit_transform(tags_lst)
Y.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;ve transformed the target (tags) but we cannot directly perform the algorithms on the text data, so we have to process and transform them into vectors. In order to do this, we will use &lt;code&gt;TfidfVectorizer&lt;/code&gt; to preprocess, tokenize, filter stop words and transform the text data. The &lt;code&gt;TfidfVectorizer&lt;/code&gt; implements the &lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;tf-idf&lt;/em&gt;&lt;/a&gt; (Term Frequency-Inverse Deocument Frequency) to reflect how important a word is to to a docuemnt in a collection of documents.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.feature_extraction.text import TfidfVectorizer

# Use the default parameters for now, use_idf=True in default
vectorizer = TfidfVectorizer()
# Use the short descriptions for now for faster processing
X = vectorizer.fit_transform(df_data.description)
X.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As mentioned in the beginning, this is a multi-label classification problem, we will use &lt;code&gt;OneVsRestClassifier&lt;/code&gt; to tackle our problem. And firstly we will use the SVM (Support Vector Machines) with linear kernel, implemented as &lt;code&gt;LinearSVC&lt;/code&gt; in scikit-learn, to do the classification.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split

# Use default parameters, and train and test with small set of samples.
clf = OneVsRestClassifier(LinearSVC())

from sklearn.utils import resample

X_sample, Y_sample = resample(
    X, Y, n_samples=1000, replace=False, random_state=7)

# X_sample_test, Y_sample_test = resample(
#     X, Y, n_samples=10, replace=False, random_state=1)

X_sample_train, X_sample_test, Y_sample_train, Y_sample_test = train_test_split(
    X_sample, Y_sample, test_size=0.01, random_state=42)

clf.fit(X_sample, Y_sample)
Y_sample_pred = clf.predict(X_sample_test)

# Inverse transform the vectors back to tags
pred_transformed = mlb.inverse_transform(Y_sample_pred)
test_transformed = mlb.inverse_transform(Y_sample_test)

for (t, p) in zip(test_transformed, pred_transformed):
    print(f&#39;tags: {t} predicted as: {p}&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;tags: (&#39;javascript&#39;,) predicted as: (&#39;javascript&#39;,)
tags: (&#39;erasure-code&#39;, &#39;storage&#39;) predicted as: ()
tags: (&#39;mysql&#39;, &#39;network&#39;) predicted as: ()
tags: (&#39;token&#39;,) predicted as: ()
tags: (&#39;flask&#39;, &#39;python&#39;, &#39;web&#39;) predicted as: ()
tags: (&#39;refactoring&#39;,) predicted as: ()
tags: (&#39;emacs&#39;,) predicted as: ()
tags: (&#39;async&#39;, &#39;javascript&#39;, &#39;promises&#39;) predicted as: (&#39;async&#39;, &#39;javascript&#39;)
tags: (&#39;neural-networks&#39;,) predicted as: ()
tags: (&#39;kubernetes&#39;,) predicted as: (&#39;kubernetes&#39;,)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Though not very satisfied, this classifier predicted right a few tags. Next we&amp;rsquo;ll try to search for the best parameters for the classifier and train with fulltext of articles.&lt;/p&gt;

&lt;h2 id=&#34;search-for-best-model-parameters-for-svm-with-linear-kernel&#34;&gt;Search for best model parameters for SVM with linear kernel&lt;/h2&gt;

&lt;p&gt;For the estimators &lt;code&gt;TfidfVectorizer&lt;/code&gt; and &lt;code&gt;LinearSVC&lt;/code&gt;, they both have many parameters could be tuned for better performance. We&amp;rsquo;ll the &lt;code&gt;GridSearchCV&lt;/code&gt; to search for the best parameters with the help of &lt;code&gt;Pipeline&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV


# Split the dataset into training and test set, and use fulltext of articles:
X_train, X_test, Y_train, Y_test = train_test_split(
    df_data.fulltext, Y, test_size=0.5, random_state=42)

# Build vectorizer classifier pipeline
clf = Pipeline([
    (&#39;vect&#39;, TfidfVectorizer()),
    (&#39;clf&#39;, OneVsRestClassifier(LinearSVC())),
])

# Grid search parameters, I minimized the parameter set based on previous
# experience to accelerate the processing speed.
# And the combination of penalty=&#39;l1&#39; and loss=&#39;squared_hinge&#39; are not supported when dual=True
parameters = {
    &#39;vect__ngram_range&#39;: [(1, 2), (1, 3), (1, 4)],
    &#39;vect__max_df&#39;: [1, 0.9, 0.8, 0.7],
    &#39;vect__min_df&#39;: [1, 0.9, 0.8, 0.7, 0],
    &#39;vect__use_idf&#39;: [True, False],
    &#39;clf__estimator__penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;],
    &#39;clf__estimator__C&#39;: [1, 10, 100, 1000],
    &#39;clf__estimator__dual&#39;: [False],
}

gs_clf = GridSearchCV(clf, parameters, cv=5, n_jobs=-1)
gs_clf.fit(X_train, Y_train)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import datetime
from sklearn import metrics


# Predict the outcome on the testing set in a variable named y_predicted
Y_predicted = gs_clf.predict(X_test)

print(metrics.classification_report(Y_test, Y_predicted))
print(gs_clf.best_params_)
print(gs_clf.best_score_)

# Export some of the result cols
cols = [
    &#39;mean_test_score&#39;,
    &#39;mean_fit_time&#39;,
    &#39;param_vect__ngram_range&#39;,
]
df_result = pd.DataFrame(gs_clf.cv_results_)
df_result = df_result.sort_values(by=&#39;rank_test_score&#39;)
df_result = df_result[cols]

timestamp = datetime.now().strftime(&#39;%Y-%m-%d_%H-%M-%S&#39;)
df_result.to_html(
    f&#39;data/results/gridcv_results_{timestamp}_linearSVC.html&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we attach the top-5 performed classifiers with selected parameters.&lt;/p&gt;

&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;rank_test_score&lt;/th&gt;
      &lt;th&gt;mean_test_score&lt;/th&gt;
      &lt;th&gt;mean_fit_time&lt;/th&gt;
      &lt;th&gt;param_vect__max_df&lt;/th&gt;
      &lt;th&gt;param_vect__ngram_range&lt;/th&gt;
      &lt;th&gt;param_vect__use_idf&lt;/th&gt;
      &lt;th&gt;param_clf__estimator__penalty&lt;/th&gt;
      &lt;th&gt;param_clf__estimator__C&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;64&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.140811&lt;/td&gt;
      &lt;td&gt;96.127405&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;(1, 4)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;l1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;70&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.140215&lt;/td&gt;
      &lt;td&gt;103.252332&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
      &lt;td&gt;(1, 4)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;l1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;58&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.140215&lt;/td&gt;
      &lt;td&gt;98.990952&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;(1, 4)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;l1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;154&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.140215&lt;/td&gt;
      &lt;td&gt;1690.433151&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;(1, 4)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;l1&lt;/td&gt;
      &lt;td&gt;1000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;68&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.139618&lt;/td&gt;
      &lt;td&gt;70.778621&lt;/td&gt;
      &lt;td&gt;0.7&lt;/td&gt;
      &lt;td&gt;(1, 3)&lt;/td&gt;
      &lt;td&gt;True&lt;/td&gt;
      &lt;td&gt;l1&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Based on the grid search results, we found the following parameters combined with the default parameters have the best performance. Now let&amp;rsquo;s see how it will perform.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train, X_test, Y_train, Y_test = train_test_split(
    df_data, Y, test_size=0.2, random_state=42)

clf = Pipeline([
    (&#39;vect&#39;, TfidfVectorizer(use_idf=True,
                             max_df=0.8, ngram_range=[1, 4])),
    (&#39;clf&#39;, OneVsRestClassifier(LinearSVC(penalty=&#39;l1&#39;, C=10, dual=False))),
])

clf.fit(X_train.fulltext, Y_train)


Y_pred = clf.predict(X_test.fulltext)

# Inverse transform the vectors back to tags
pred_transformed = mlb.inverse_transform(Y_pred)
test_transformed = mlb.inverse_transform(Y_test)

for (title, t, p) in zip(X_test.title, test_transformed, pred_transformed):
    print(f&#39;info title: {title} \ntags: {t} \npredicted as: {p}&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;info title: Designing resilient systems: Circuit Breakers or Retries? (Part 1) 
tags: (&#39;circuit-breaker&#39;, &#39;system-architecture&#39;) 
predicted as: (&#39;golang&#39;, &#39;system-architecture&#39;)
info title: Zuul 2 : The Netflix Journey to Asynchronous, Non-Blocking Systems 
tags: (&#39;asynchronous&#39;, &#39;system-architecture&#39;) 
predicted as: ()
info title: 对称加密算法与非对称加密算法的优缺点  
tags: (&#39;crytography&#39;,) 
predicted as: ()
info title: Go coding in go way 
tags: (&#39;golang&#39;,) 
predicted as: (&#39;golang&#39;,)
info title: Emacs Doom for Newbies 
tags: (&#39;doom-emacs&#39;, &#39;emacs&#39;) 
predicted as: (&#39;emacs&#39;,)
info title: Basic testing patterns in Go 
tags: (&#39;golang&#39;, &#39;testing&#39;) 
predicted as: (&#39;golang&#39;, &#39;testing&#39;)
info title: Dead Simple Python: Virtual Environments and pip 
tags: (&#39;python&#39;, &#39;virtualenv&#39;) 
predicted as: (&#39;python&#39;,)
info title:  Overcoming RESTlessness 
tags: (&#39;api&#39;, &#39;restful&#39;, &#39;web&#39;) 
predicted as: (&#39;api&#39;,)
info title: Let’s Build A Simple Interpreter. Part 7. 
tags: (&#39;compiler&#39;, &#39;interpreter&#39;, &#39;pascal&#39;, &#39;python&#39;) 
predicted as: (&#39;compiler&#39;, &#39;interpreter&#39;, &#39;pascal&#39;, &#39;python&#39;)
info title: Spring RestTemplate详解 
tags: (&#39;html&#39;, &#39;java&#39;, &#39;spring&#39;) 
predicted as: (&#39;java&#39;,)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here above is a fraction of the list that shows the manually input tags and the predicted tags. We can see that usually the more frequently appeared and more popular tags have better change to be correctly predicted.&lt;/p&gt;

&lt;!-- However, the manual tags, as the training and prediction comparing group, suffer from problems like  --&gt;
</description>
    </item>
    
    <item>
      <title>LinkedInfo.co</title>
      <link>https://pcx.linkedinfo.co/project/linkedinfo/</link>
      <pubDate>Sun, 27 Jan 2019 11:17:54 +0100</pubDate>
      <guid>https://pcx.linkedinfo.co/project/linkedinfo/</guid>
      <description>&lt;p&gt;The Web should be an open web. All the informations published on the Web are meant to be shared, share through links by search engines, rss, social networks, etc. This site is yet another method that tries to link all the informations (but starts with only technical articles on LinkedInfo) and share them.&lt;/p&gt;

&lt;p&gt;The original idea of this side project is to utilize Semantic Web technologies and Machine learning to link the informations. Noble ambition shall start from basic, it needs to be improved little by little.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
